{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Dataset Stats\n",
    "\n",
    "**Author:** Prisca Dotti\n",
    "\n",
    "**Last modified:** 30.03.2024\n",
    "\n",
    "Used this script for the following purposes:\n",
    "- Compute number of individual instances per type\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload is used to reload modules automatically before entering the\n",
    "# execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# To import modules from parent directory in Jupyter Notebook\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from config import TrainingConfig, config\n",
    "from data.data_processing_tools import masks_to_instances_dict\n",
    "from utils.training_script_utils import init_dataset\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "config.verbosity = 3  # To get debug messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:07:37] [WARNING ] [   config   ] <313 > -- No config file found at C:\\Users\\dotti\\Code\\sparks_project, trying to use fallback values.\n"
     ]
    }
   ],
   "source": [
    "############################ Get default parameters ############################\n",
    "\n",
    "test_ids = [\n",
    "    \"05\",\n",
    "    \"10\",\n",
    "    \"15\",\n",
    "    \"20\",\n",
    "    \"25\",\n",
    "    \"32\",\n",
    "    \"34\",\n",
    "    \"40\",\n",
    "    \"45\",\n",
    "]\n",
    "train_ids = [\n",
    "    \"01\",\n",
    "    \"02\",\n",
    "    \"03\",\n",
    "    \"04\",\n",
    "    \"06\",\n",
    "    \"07\",\n",
    "    \"08\",\n",
    "    \"09\",\n",
    "    \"11\",\n",
    "    \"12\",\n",
    "    \"13\",\n",
    "    \"14\",\n",
    "    \"16\",\n",
    "    \"17\",\n",
    "    \"18\",\n",
    "    \"19\",\n",
    "    \"21\",\n",
    "    \"22\",\n",
    "    \"23\",\n",
    "    \"24\",\n",
    "    \"27\",\n",
    "    \"28\",\n",
    "    \"29\",\n",
    "    \"30\",\n",
    "    \"33\",\n",
    "    \"35\",\n",
    "    \"36\",\n",
    "    \"38\",\n",
    "    \"39\",\n",
    "    \"41\",\n",
    "    \"42\",\n",
    "    \"43\",\n",
    "    \"44\",\n",
    "    \"46\",\n",
    "]\n",
    "sample_ids = test_ids + train_ids\n",
    "\n",
    "# Initialize general parameters with default values\n",
    "params = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:08:32] [  INFO  ] [  __main__  ] < 3  > -- Loading samples ['05', '10', '15', '20', '25', '32', '34', '40', '45', '01', '02', '03', '04', '06', '07', '08', '09', '11', '12', '13', '14', '16', '17', '18', '19', '21', '22', '23', '24', '27', '28', '29', '30', '33', '35', '36', '38', '39', '41', '42', '43', '44', '46'].\n",
      "[12:08:32] [  INFO  ] [  __main__  ] < 4  > -- Using C:\\Users\\dotti\\Code\\sparks_project\\data\\sparks_dataset as dataset root path.\n",
      "[12:13:12] [  INFO  ] [utils.training_script_utils] <137 > -- Samples in dataset: 830\n"
     ]
    }
   ],
   "source": [
    "############################## Configure dataset ###############################\n",
    "\n",
    "logger.info(f\"Loading samples {sample_ids}.\")\n",
    "logger.info(f\"Using {params.dataset_dir} as dataset root path.\")\n",
    "\n",
    "# Create dataset\n",
    "dataset = init_dataset(\n",
    "    params=params,\n",
    "    sample_ids=sample_ids,\n",
    "    apply_data_augmentation=False,\n",
    "    print_dataset_info=True,\n",
    "    load_instances=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = dataset.get_instances()\n",
    "labels = dataset.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of instances per class per movie\n",
    "n_instances_per_movie = {movie_id: {} for movie_id in sample_ids}\n",
    "\n",
    "for movie_id, i, l in zip(sample_ids, instances.values(), labels.values()):\n",
    "    movie_instances_dict = masks_to_instances_dict(i, l)\n",
    "\n",
    "    n_instances_per_type = {\n",
    "        c: len(np.unique(i)) - 1 for c, i in movie_instances_dict.items()\n",
    "    }\n",
    "    n_instances_per_movie[movie_id] = n_instances_per_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_sparks': 1380,\n",
       " 'total_waves': 37,\n",
       " 'total_puffs': 299,\n",
       " 'average_sparks': 32.093023255813954,\n",
       " 'average_waves': 0.8604651162790697,\n",
       " 'average_puffs': 6.953488372093023,\n",
       " 'movie_most_sparks': ('23', 188),\n",
       " 'movie_most_waves': ('38', 8),\n",
       " 'movie_most_puffs': ('05', 26),\n",
       " 'total_instances': 1716}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert nested dictionary into a DataFrame for easier analysis\n",
    "df = pd.DataFrame.from_dict(n_instances_per_movie, orient=\"index\")\n",
    "\n",
    "# Calculate total number of each event type across all movies\n",
    "total_events = df.sum()\n",
    "\n",
    "# Calculate average number of each event type per movie\n",
    "average_events = df.mean()\n",
    "\n",
    "# Find the movie with the maximum number of each event type\n",
    "max_events = df.idxmax()\n",
    "\n",
    "# Gather statistics in a dictionary for easier presentation\n",
    "stats = {\n",
    "    \"total_sparks\": total_events[\"sparks\"],\n",
    "    \"total_waves\": total_events[\"waves\"],\n",
    "    \"total_puffs\": total_events[\"puffs\"],\n",
    "    \"average_sparks\": average_events[\"sparks\"],\n",
    "    \"average_waves\": average_events[\"waves\"],\n",
    "    \"average_puffs\": average_events[\"puffs\"],\n",
    "    \"movie_most_sparks\": (max_events[\"sparks\"], df.loc[max_events[\"sparks\"]][\"sparks\"]),\n",
    "    \"movie_most_waves\": (max_events[\"waves\"], df.loc[max_events[\"waves\"]][\"waves\"]),\n",
    "    \"movie_most_puffs\": (max_events[\"puffs\"], df.loc[max_events[\"puffs\"]][\"puffs\"]),\n",
    "    \"total_instances\": df.sum(axis=1).sum(),  # Total of all instances\n",
    "}\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies with sparks: 40\n",
      "Number of movies with waves: 16\n",
      "Number of movies with puffs: 37\n"
     ]
    }
   ],
   "source": [
    "# Convert the boolean DataFrame to int (True to 1, False to 0)\n",
    "movies_with_events_int = df.map(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Now, sum each column to get the number of movies that contain each event type\n",
    "num_movies_with_sparks = movies_with_events_int[\"sparks\"].sum()\n",
    "num_movies_with_waves = movies_with_events_int[\"waves\"].sum()\n",
    "num_movies_with_puffs = movies_with_events_int[\"puffs\"].sum()\n",
    "\n",
    "# Output the results\n",
    "print(f\"Number of movies with sparks: {num_movies_with_sparks}\")\n",
    "print(f\"Number of movies with waves: {num_movies_with_waves}\")\n",
    "print(f\"Number of movies with puffs: {num_movies_with_puffs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'sparks': 1115, 'waves': 30, 'puffs': 225},\n",
       " {'sparks': 265, 'waves': 7, 'puffs': 74})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize dictionaries to count the number of each event type in the test and train datasets\n",
    "train_event_counts = {\"sparks\": 0, \"waves\": 0, \"puffs\": 0}\n",
    "test_event_counts = {\"sparks\": 0, \"waves\": 0, \"puffs\": 0}\n",
    "\n",
    "# Iterate over the nested dictionary to sum the events for each dataset\n",
    "for movie_id, events in n_instances_per_movie.items():\n",
    "    if movie_id in train_ids:\n",
    "        for event, count in events.items():\n",
    "            train_event_counts[event] += count\n",
    "    elif movie_id in test_ids:\n",
    "        for event, count in events.items():\n",
    "            test_event_counts[event] += count\n",
    "\n",
    "train_event_counts, test_event_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sparks': 19.202898550724637,\n",
       " 'waves': 18.91891891891892,\n",
       " 'puffs': 24.74916387959866}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have the total counts for each event type in the train and test datasets\n",
    "# Calculate the ratios of each event type in the train dataset vs. the test dataset\n",
    "ratios = {}\n",
    "for event_type in train_event_counts.keys():\n",
    "    if test_event_counts[event_type] > 0:  # To avoid division by zero\n",
    "        ratios[event_type] = (\n",
    "            train_event_counts[event_type] / test_event_counts[event_type]\n",
    "        )\n",
    "    else:\n",
    "        ratios[event_type] = float(\n",
    "            \"inf\"\n",
    "        )  # If the event type is not present in the test dataset at all\n",
    "\n",
    "# Given the ratios, convert them to percentages\n",
    "percentages = {}\n",
    "for event_type in train_event_counts.keys():\n",
    "    percentages[event_type] = (\n",
    "        100\n",
    "        * test_event_counts[event_type]\n",
    "        / (train_event_counts[event_type] + test_event_counts[event_type])\n",
    "    )\n",
    "\n",
    "percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
