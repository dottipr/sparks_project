{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Model Training Script\n",
    "\n",
    "**Author:** Prisca Dotti\n",
    "\n",
    "**Last Edit:** 24.10.2023\n",
    "\n",
    "This Jupyter Notebook contains the code for training a U-Net model on a dataset of sparks videos. The dataset is split into training and testing sets, and the model is trained using the training set. The testing set is used to evaluate the performance of the trained model.\n",
    "\n",
    "To run the notebook, simply execute each cell in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload is used to reload modules automatically before entering the\n",
    "# execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# To import modules from parent directory in Jupyter Notebook\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "import wandb\n",
    "from config import TrainingConfig, config\n",
    "from models.UNet import unet\n",
    "from utils.training_inference_tools import (\n",
    "    MyTrainingManager,\n",
    "    sampler,\n",
    "    test_function,\n",
    "    training_step,\n",
    "    weights_init,\n",
    ")\n",
    "from utils.training_script_utils import (\n",
    "    get_sample_ids,\n",
    "    init_criterion,\n",
    "    init_dataset,\n",
    "    init_model,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger __main__ (WARNING)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print logger configuation\n",
    "logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important notes for thesis:\n",
    "- after trying with and without weight init, I think I will keep U-Net weight init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:42] [  INFO  ] [   config   ] <316 > -- Loading C:\\Users\\dotti\\Code\\sparks_project\\config_files\\config_final_model.ini\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --     training_config_file: C:\\Users\\dotti\\Code\\sparks_project\\config_files\\config_final_model.ini\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --              dataset_dir: C:\\Users\\dotti\\Code\\sparks_project\\data\\sparks_dataset\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --                        c: <configparser.ConfigParser object at 0x000001EE4EB63C40>\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --                 run_name: final_model\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --            load_run_name: \n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --               load_epoch: 100000\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --             train_epochs: 100000\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --                criterion: lovasz_softmax\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --                 lr_start: 0.0001\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --       ignore_frames_loss: 6\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --                     cuda: True\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --                scheduler: None\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --                optimizer: adam\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --             dataset_size: full\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --               batch_size: 4\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --              num_workers: 0\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --            data_duration: 256\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --              data_stride: 32\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --           data_smoothing: no\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --               norm_video: abs_max\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --        remove_background: no\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --  noise_data_augmentation: False\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --              sparks_type: raw\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --                  new_fps: 0\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --             sin_channels: False\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --           n_sin_channels: [0, 0, 0]\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --  inference_data_duration: 256\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --    inference_data_stride: 32\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --                inference: overlap\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --     inference_load_epoch: 100000\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --     inference_batch_size: 2\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --   inference_dataset_size: full\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --          nn_architecture: pablos_unet\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --               unet_steps: 6\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --     first_layer_channels: 8\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --             num_channels: 1\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --                 dilation: 1\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --              border_mode: same\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --      batch_normalization: none\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --       temporal_reduction: False\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --       initialize_weights: False\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --                wandb_log: False\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --                   device: cuda\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --               pin_memory: True\n",
      "[18:41:42] [  INFO  ] [   config   ] <562 > --                   n_gpus: 1\n"
     ]
    }
   ],
   "source": [
    "##################### Get training-specific parameters #####################\n",
    "\n",
    "# Initialize training-specific parameters\n",
    "# (get the configuration file path from ArgParse)\n",
    "config_filename = os.path.join(\"config_files\", \"config_final_model.ini\")\n",
    "# config_filename = os.path.join(\"config_files\", \"config_final_model_w_init.ini\")\n",
    "# config_filename = os.path.join(\"config_files\", \"config_final_model_TEST.ini\")\n",
    "# config_filename = os.path.join(\"config_files\", \"config_sin_channels.ini\")\n",
    "params = TrainingConfig(training_config_file=config_filename)\n",
    "\n",
    "# Print parameters to console if needed\n",
    "params.print_params()\n",
    "\n",
    "######################### Initialize random seeds ##########################\n",
    "\n",
    "# We used these random seeds to ensure reproducibility of the results\n",
    "\n",
    "torch.manual_seed(0)  # <--------------------------------------------------!\n",
    "random.seed(0)  # <--------------------------------------------------------!\n",
    "np.random.seed(0)  # <-----------------------------------------------------!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params.set_device(\"cpu\")\n",
    "# params.display_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:57] [  INFO  ] [utils.training_script_utils] <149 > -- Samples in dataset: 672\n",
      "[18:42:38] [  INFO  ] [utils.training_script_utils] <149 > -- Samples in dataset: 158\n"
     ]
    }
   ],
   "source": [
    "############################ Configure datasets ############################\n",
    "\n",
    "# Select samples for training and testing based on dataset size\n",
    "train_sample_ids = get_sample_ids(\n",
    "    train_data=True,\n",
    "    dataset_size=params.dataset_size,\n",
    ")\n",
    "test_sample_ids = get_sample_ids(\n",
    "    train_data=False,\n",
    "    dataset_size=params.dataset_size,\n",
    ")\n",
    "\n",
    "# Initialize training dataset\n",
    "dataset = init_dataset(\n",
    "    params=params,\n",
    "    sample_ids=train_sample_ids,\n",
    "    apply_data_augmentation=True,\n",
    "    load_instances=False,\n",
    ")\n",
    "\n",
    "# Initialize testing datasets\n",
    "testing_dataset = init_dataset(\n",
    "    params=params,\n",
    "    sample_ids=test_sample_ids,\n",
    "    apply_data_augmentation=False,\n",
    "    load_instances=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # traing only with the central chunk for each movie in the dataset\n",
    "# dataset.source_dataset.set_debug_dataset()\n",
    "# testing_dataset.set_debug_dataset()\n",
    "\n",
    "# print(\"Dataset size: \", len(dataset))\n",
    "# print(\"Testing dataset size: \", len(testing_dataset))\n",
    "\n",
    "# # train with only one batch\n",
    "# import numpy as np\n",
    "# from torch.utils.data import Subset\n",
    "# ids = list(np.arange(3, 3+params.batch_size, 1, dtype=np.int64))\n",
    "# dataset = Subset(dataset, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loaders\n",
    "dataset_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=params.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=params.num_workers,\n",
    "    pin_memory=params.pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Configure UNet ##############################\n",
    "\n",
    "# Initialize the UNet model\n",
    "network = init_model(params=params)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "if params.device.type != \"cpu\":\n",
    "    network = nn.DataParallel(network).to(params.device, non_blocking=True)\n",
    "    # cudnn.benchmark = True\n",
    "\n",
    "# Watch the model with wandb for logging if enabled\n",
    "if params.wandb_log:\n",
    "    wandb.watch(network)\n",
    "\n",
    "# Initialize UNet weights if required\n",
    "if params.initialize_weights:\n",
    "    logger.info(\"Initializing UNet weights...\")\n",
    "    network.apply(weights_init)\n",
    "\n",
    "# The following line is commented as it does not work on Windows\n",
    "# torch.compile(network, mode=\"default\", backend=\"inductor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(network.parameters())).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:42:39] [  INFO  ] [  __main__  ] < 26 > -- Output directory: C:\\Users\\dotti\\Code\\sparks_project\\models\\saved_models\\final_model\n",
      "[18:42:39] [  INFO  ] [utils.training_inference_tools] <1360> -- Loading 'C:\\Users\\dotti\\Code\\sparks_project\\models\\saved_models\\final_model\\network_100000.pth'...\n",
      "[18:42:39] [  INFO  ] [utils.training_inference_tools] <1360> -- Loading 'C:\\Users\\dotti\\Code\\sparks_project\\models\\saved_models\\final_model\\optimizer_100000.pth'...\n"
     ]
    }
   ],
   "source": [
    "########################### Initialize training ############################\n",
    "\n",
    "# Initialize the optimizer based on the specified type\n",
    "if params.optimizer == \"adam\":\n",
    "    optimizer = optim.Adam(network.parameters(), lr=params.lr_start)\n",
    "elif params.optimizer == \"adadelta\":\n",
    "    optimizer = optim.Adadelta(network.parameters(), lr=params.lr_start)\n",
    "elif params.optimizer == \"sgd\":\n",
    "    optimizer = optim.SGD(network.parameters(), lr=params.lr_start)\n",
    "else:\n",
    "    logger.error(f\"{params.optimizer} is not a valid optimizer.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the learning rate scheduler if specified\n",
    "if params.scheduler == \"step\":\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=params.scheduler_step_size,\n",
    "        gamma=params.scheduler_gamma,\n",
    "    )\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "# Define the output directory path\n",
    "output_path = os.path.join(config.output_dir, params.run_name)\n",
    "logger.info(f\"Output directory: {os.path.realpath(output_path)}\")\n",
    "\n",
    "# Initialize the summary writer for TensorBoard logging\n",
    "summary_writer = SummaryWriter(os.path.join(output_path, \"summary\"), purge_step=0)\n",
    "\n",
    "# Check if a pre-trained model should be loaded\n",
    "if params.load_run_name != \"\":\n",
    "    load_path = os.path.join(config.output_dir, params.load_run_name)\n",
    "    logger.info(f\"Model loaded from directory: {os.path.realpath(load_path)}\")\n",
    "else:\n",
    "    load_path = None\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = init_criterion(params=params, dataset=dataset)\n",
    "\n",
    "# Create a directory to save predicted class movies\n",
    "preds_output_dir = os.path.join(output_path, \"predictions\")\n",
    "os.makedirs(preds_output_dir, exist_ok=True)\n",
    "\n",
    "# Create a dictionary of managed objects\n",
    "managed_objects = {\"network\": network, \"optimizer\": optimizer}\n",
    "if scheduler is not None:\n",
    "    managed_objects[\"scheduler\"] = scheduler\n",
    "\n",
    "# Create a training manager with the specified training and testing functions\n",
    "trainer = MyTrainingManager(\n",
    "    # Training parameters\n",
    "    training_step=lambda _: training_step(\n",
    "        dataset_loader=dataset_loader,\n",
    "        params=params,\n",
    "        sampler=sampler,\n",
    "        network=network,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        scheduler=scheduler,\n",
    "        scaler=GradScaler(),\n",
    "    ),\n",
    "    save_every=params.c.getint(\"training\", \"save_every\", fallback=5000),\n",
    "    load_path=load_path,\n",
    "    save_path=output_path,\n",
    "    managed_objects=unet.managed_objects(managed_objects),\n",
    "    # Testing parameters\n",
    "    test_function=lambda _: test_function(\n",
    "        network=network,\n",
    "        device=params.device,\n",
    "        criterion=criterion,\n",
    "        params=params,\n",
    "        testing_dataset=testing_dataset,\n",
    "        training_name=params.run_name,\n",
    "        output_dir=preds_output_dir,\n",
    "        training_mode=True,\n",
    "    ),\n",
    "    test_every=params.c.getint(\"training\", \"test_every\", fallback=1000),\n",
    "    plot_every=params.c.getint(\"training\", \"test_every\", fallback=1000),\n",
    "    summary_writer=summary_writer,\n",
    ")\n",
    "\n",
    "# Load the model if a specific epoch is provided\n",
    "if params.load_epoch != 0:\n",
    "    trainer.load(params.load_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Start training ##############################\n",
    "\n",
    "# Resume the W&B run if needed (commented out for now)\n",
    "# if wandb.run.resumed:\n",
    "#     checkpoint = torch.load(wandb.restore(checkpoint_path))\n",
    "#     network.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     epoch = checkpoint['epoch']\n",
    "#     loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:49:13] [  INFO  ] [  __main__  ] < 3  > -- Validate network before training\n",
      "[18:49:13] [  INFO  ] [utils.training_inference_tools] <1207> -- Validating network at iteration 100000...\n",
      "[18:49:13] [ DEBUG  ] [utils.training_inference_tools] <923 > -- Test function: running samples in UNet\n",
      "[18:57:58] [ DEBUG  ] [utils.training_inference_tools] <951 > -- Time to run samples ['05', '10', '15', '20', '25', '32', '34', '40', '45'] in UNet: 524.97 s\n",
      "[18:57:58] [ DEBUG  ] [utils.training_inference_tools] <955 > -- Test function: computing loss\n",
      "[18:58:20] [ DEBUG  ] [utils.training_inference_tools] <975 > -- Processing sample 05\n",
      "[18:58:20] [ DEBUG  ] [utils.training_inference_tools] <984 > -- Test function: saving raw predictions on disk\n",
      "[18:58:20] [ DEBUG  ] [utils.training_inference_tools] <998 > -- Test function: re-organising annotations\n",
      "[18:58:22] [ DEBUG  ] [utils.training_inference_tools] <1008> -- Time to re-organise annotations: 1.52 s\n",
      "[18:58:22] [ DEBUG  ] [utils.training_inference_tools] <1013> -- Test function: getting processed output (segmentation and instances)\n",
      "[18:58:22] [ DEBUG  ] [data.data_processing_tools] <547 > -- Events detection threshold: 0.498\n",
      "[18:58:23] [ DEBUG  ] [data.data_processing_tools] <1052> -- Separating events in predictions...\n",
      "[18:58:23] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of sparks...\n",
      "[18:58:27] [ DEBUG  ] [data.data_processing_tools] <637 > -- Number of sparks detected by nonmaxima suppression: 130\n",
      "[18:58:28] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of waves...\n",
      "[18:58:28] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of puffs...\n",
      "[18:58:29] [ DEBUG  ] [data.data_processing_tools] <1061> -- Removing small events and merging events...\n",
      "[18:58:36] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of sparks instances: 130\n",
      "[18:58:36] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of waves instances: 0\n",
      "[18:58:36] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of puffs instances: 62\n",
      "[18:58:38] [ DEBUG  ] [data.data_processing_tools] <1100> -- Time for removing small events: 9.24 s\n",
      "[18:58:38] [ DEBUG  ] [utils.training_inference_tools] <1030> -- Time to process predictions: 16.21 s\n",
      "[18:58:38] [ DEBUG  ] [utils.training_inference_tools] <1050> -- Test function: computing pairwise scores between 97 annotated events and 159 predicted events\n",
      "[18:58:53] [ DEBUG  ] [utils.training_inference_tools] <1063> -- Time to compute pairwise scores: 15.09 s\n",
      "[18:58:53] [ DEBUG  ] [utils.training_inference_tools] <1070> -- Test function: getting matches summary\n",
      "[18:58:55] [ DEBUG  ] [utils.training_inference_tools] <1092> -- Time to get matches summary: 1.85 s\n",
      "[18:58:55] [ DEBUG  ] [utils.training_inference_tools] <975 > -- Processing sample 10\n",
      "[18:58:55] [ DEBUG  ] [utils.training_inference_tools] <984 > -- Test function: saving raw predictions on disk\n",
      "[18:58:56] [ DEBUG  ] [utils.training_inference_tools] <998 > -- Test function: re-organising annotations\n",
      "[18:58:56] [ DEBUG  ] [utils.training_inference_tools] <1008> -- Time to re-organise annotations: 0.87 s\n",
      "[18:58:56] [ DEBUG  ] [utils.training_inference_tools] <1013> -- Test function: getting processed output (segmentation and instances)\n",
      "[18:58:57] [ DEBUG  ] [data.data_processing_tools] <547 > -- Events detection threshold: 0.494\n",
      "[18:58:57] [ DEBUG  ] [data.data_processing_tools] <1052> -- Separating events in predictions...\n",
      "[18:58:57] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of sparks...\n",
      "[18:59:02] [ DEBUG  ] [data.data_processing_tools] <637 > -- Number of sparks detected by nonmaxima suppression: 34\n",
      "[18:59:02] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of waves...\n",
      "[18:59:03] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of puffs...\n",
      "[18:59:03] [ DEBUG  ] [data.data_processing_tools] <1061> -- Removing small events and merging events...\n",
      "[18:59:06] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of sparks instances: 34\n",
      "[18:59:06] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of waves instances: 0\n",
      "[18:59:06] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of puffs instances: 27\n",
      "[18:59:07] [ DEBUG  ] [data.data_processing_tools] <1100> -- Time for removing small events: 3.71 s\n",
      "[18:59:07] [ DEBUG  ] [utils.training_inference_tools] <1030> -- Time to process predictions: 10.20 s\n",
      "[18:59:07] [ DEBUG  ] [utils.training_inference_tools] <1050> -- Test function: computing pairwise scores between 29 annotated events and 52 predicted events\n",
      "[18:59:11] [ DEBUG  ] [utils.training_inference_tools] <1063> -- Time to compute pairwise scores: 4.41 s\n",
      "[18:59:11] [ DEBUG  ] [utils.training_inference_tools] <1070> -- Test function: getting matches summary\n",
      "[18:59:12] [ DEBUG  ] [utils.training_inference_tools] <1092> -- Time to get matches summary: 1.09 s\n",
      "[18:59:12] [ DEBUG  ] [utils.training_inference_tools] <975 > -- Processing sample 15\n",
      "[18:59:12] [ DEBUG  ] [utils.training_inference_tools] <984 > -- Test function: saving raw predictions on disk\n",
      "[18:59:13] [ DEBUG  ] [utils.training_inference_tools] <998 > -- Test function: re-organising annotations\n",
      "[18:59:14] [ DEBUG  ] [utils.training_inference_tools] <1008> -- Time to re-organise annotations: 1.07 s\n",
      "[18:59:14] [ DEBUG  ] [utils.training_inference_tools] <1013> -- Test function: getting processed output (segmentation and instances)\n",
      "[18:59:14] [ DEBUG  ] [data.data_processing_tools] <547 > -- Events detection threshold: 0.494\n",
      "[18:59:15] [ DEBUG  ] [data.data_processing_tools] <1052> -- Separating events in predictions...\n",
      "[18:59:15] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of sparks...\n",
      "[18:59:20] [ DEBUG  ] [data.data_processing_tools] <637 > -- Number of sparks detected by nonmaxima suppression: 30\n",
      "[18:59:21] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of waves...\n",
      "[18:59:21] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of puffs...\n",
      "[18:59:21] [ DEBUG  ] [data.data_processing_tools] <1061> -- Removing small events and merging events...\n",
      "[18:59:24] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of sparks instances: 30\n",
      "[18:59:24] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of waves instances: 0\n",
      "[18:59:24] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of puffs instances: 26\n",
      "[18:59:25] [ DEBUG  ] [data.data_processing_tools] <1100> -- Time for removing small events: 3.96 s\n",
      "[18:59:25] [ DEBUG  ] [utils.training_inference_tools] <1030> -- Time to process predictions: 11.49 s\n",
      "[18:59:25] [ DEBUG  ] [utils.training_inference_tools] <1050> -- Test function: computing pairwise scores between 54 annotated events and 47 predicted events\n",
      "[18:59:34] [ DEBUG  ] [utils.training_inference_tools] <1063> -- Time to compute pairwise scores: 8.32 s\n",
      "[18:59:34] [ DEBUG  ] [utils.training_inference_tools] <1070> -- Test function: getting matches summary\n",
      "[18:59:35] [ DEBUG  ] [utils.training_inference_tools] <1092> -- Time to get matches summary: 0.99 s\n",
      "[18:59:35] [ DEBUG  ] [utils.training_inference_tools] <975 > -- Processing sample 20\n",
      "[18:59:35] [ DEBUG  ] [utils.training_inference_tools] <984 > -- Test function: saving raw predictions on disk\n",
      "[18:59:35] [ DEBUG  ] [utils.training_inference_tools] <998 > -- Test function: re-organising annotations\n",
      "[18:59:36] [ DEBUG  ] [utils.training_inference_tools] <1008> -- Time to re-organise annotations: 1.22 s\n",
      "[18:59:36] [ DEBUG  ] [utils.training_inference_tools] <1013> -- Test function: getting processed output (segmentation and instances)\n",
      "[18:59:37] [ DEBUG  ] [data.data_processing_tools] <547 > -- Events detection threshold: 0.498\n",
      "[18:59:37] [ DEBUG  ] [data.data_processing_tools] <1052> -- Separating events in predictions...\n",
      "[18:59:37] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of sparks...\n",
      "[18:59:42] [ DEBUG  ] [data.data_processing_tools] <637 > -- Number of sparks detected by nonmaxima suppression: 87\n",
      "[18:59:43] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of waves...\n",
      "[18:59:43] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of puffs...\n",
      "[18:59:43] [ DEBUG  ] [data.data_processing_tools] <1061> -- Removing small events and merging events...\n",
      "[18:59:48] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of sparks instances: 87\n",
      "[18:59:48] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of waves instances: 0\n",
      "[18:59:48] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of puffs instances: 20\n",
      "[18:59:49] [ DEBUG  ] [data.data_processing_tools] <1100> -- Time for removing small events: 6.07 s\n",
      "[18:59:49] [ DEBUG  ] [utils.training_inference_tools] <1030> -- Time to process predictions: 12.77 s\n",
      "[18:59:49] [ DEBUG  ] [utils.training_inference_tools] <1050> -- Test function: computing pairwise scores between 87 annotated events and 97 predicted events\n",
      "[19:00:02] [ DEBUG  ] [utils.training_inference_tools] <1063> -- Time to compute pairwise scores: 13.16 s\n",
      "[19:00:02] [ DEBUG  ] [utils.training_inference_tools] <1070> -- Test function: getting matches summary\n",
      "[19:00:03] [ DEBUG  ] [utils.training_inference_tools] <1092> -- Time to get matches summary: 1.03 s\n",
      "[19:00:04] [ DEBUG  ] [utils.training_inference_tools] <975 > -- Processing sample 25\n",
      "[19:00:04] [ DEBUG  ] [utils.training_inference_tools] <984 > -- Test function: saving raw predictions on disk\n",
      "[19:00:05] [ DEBUG  ] [utils.training_inference_tools] <998 > -- Test function: re-organising annotations\n",
      "[19:00:07] [ DEBUG  ] [utils.training_inference_tools] <1008> -- Time to re-organise annotations: 1.69 s\n",
      "[19:00:07] [ DEBUG  ] [utils.training_inference_tools] <1013> -- Test function: getting processed output (segmentation and instances)\n",
      "[19:00:07] [ DEBUG  ] [data.data_processing_tools] <547 > -- Events detection threshold: 0.494\n",
      "[19:00:08] [ DEBUG  ] [data.data_processing_tools] <1052> -- Separating events in predictions...\n",
      "[19:00:08] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of sparks...\n",
      "[19:00:17] [ DEBUG  ] [data.data_processing_tools] <637 > -- Number of sparks detected by nonmaxima suppression: 67\n",
      "[19:00:20] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of waves...\n",
      "[19:00:20] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of puffs...\n",
      "[19:00:20] [ DEBUG  ] [data.data_processing_tools] <1061> -- Removing small events and merging events...\n",
      "[19:00:27] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of sparks instances: 67\n",
      "[19:00:27] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of waves instances: 0\n",
      "[19:00:27] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of puffs instances: 2\n",
      "[19:00:30] [ DEBUG  ] [data.data_processing_tools] <1100> -- Time for removing small events: 9.19 s\n",
      "[19:00:30] [ DEBUG  ] [utils.training_inference_tools] <1030> -- Time to process predictions: 23.07 s\n",
      "[19:00:30] [ DEBUG  ] [utils.training_inference_tools] <1050> -- Test function: computing pairwise scores between 32 annotated events and 65 predicted events\n",
      "[19:00:40] [ DEBUG  ] [utils.training_inference_tools] <1063> -- Time to compute pairwise scores: 10.42 s\n",
      "[19:00:40] [ DEBUG  ] [utils.training_inference_tools] <1070> -- Test function: getting matches summary\n",
      "[19:00:43] [ DEBUG  ] [utils.training_inference_tools] <1092> -- Time to get matches summary: 2.59 s\n",
      "[19:00:43] [ DEBUG  ] [utils.training_inference_tools] <975 > -- Processing sample 32\n",
      "[19:00:43] [ DEBUG  ] [utils.training_inference_tools] <984 > -- Test function: saving raw predictions on disk\n",
      "[19:00:44] [ DEBUG  ] [utils.training_inference_tools] <998 > -- Test function: re-organising annotations\n",
      "[19:00:45] [ DEBUG  ] [utils.training_inference_tools] <1008> -- Time to re-organise annotations: 1.38 s\n",
      "[19:00:45] [ DEBUG  ] [utils.training_inference_tools] <1013> -- Test function: getting processed output (segmentation and instances)\n",
      "[19:00:46] [ DEBUG  ] [data.data_processing_tools] <547 > -- Events detection threshold: 0.498\n",
      "[19:00:47] [ DEBUG  ] [data.data_processing_tools] <1052> -- Separating events in predictions...\n",
      "[19:00:47] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of sparks...\n",
      "[19:00:56] [ DEBUG  ] [data.data_processing_tools] <637 > -- Number of sparks detected by nonmaxima suppression: 9\n",
      "[19:00:57] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of waves...\n",
      "[19:00:58] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of puffs...\n",
      "[19:00:58] [ DEBUG  ] [data.data_processing_tools] <1061> -- Removing small events and merging events...\n",
      "[19:01:00] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of sparks instances: 9\n",
      "[19:01:00] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of waves instances: 0\n",
      "[19:01:00] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of puffs instances: 1\n",
      "[19:01:02] [ DEBUG  ] [data.data_processing_tools] <1100> -- Time for removing small events: 3.55 s\n",
      "[19:01:02] [ DEBUG  ] [utils.training_inference_tools] <1030> -- Time to process predictions: 16.77 s\n",
      "[19:01:02] [ DEBUG  ] [utils.training_inference_tools] <1050> -- Test function: computing pairwise scores between 9 annotated events and 7 predicted events\n",
      "[19:01:05] [ DEBUG  ] [utils.training_inference_tools] <1063> -- Time to compute pairwise scores: 3.47 s\n",
      "[19:01:05] [ DEBUG  ] [utils.training_inference_tools] <1070> -- Test function: getting matches summary\n",
      "[19:01:07] [ DEBUG  ] [utils.training_inference_tools] <1092> -- Time to get matches summary: 1.75 s\n",
      "[19:01:07] [ DEBUG  ] [utils.training_inference_tools] <975 > -- Processing sample 34\n",
      "[19:01:07] [ DEBUG  ] [utils.training_inference_tools] <984 > -- Test function: saving raw predictions on disk\n",
      "[19:01:08] [ DEBUG  ] [utils.training_inference_tools] <998 > -- Test function: re-organising annotations\n",
      "[19:01:10] [ DEBUG  ] [utils.training_inference_tools] <1008> -- Time to re-organise annotations: 2.00 s\n",
      "[19:01:10] [ DEBUG  ] [utils.training_inference_tools] <1013> -- Test function: getting processed output (segmentation and instances)\n",
      "[19:01:10] [ DEBUG  ] [data.data_processing_tools] <547 > -- Events detection threshold: 0.498\n",
      "[19:01:11] [ DEBUG  ] [data.data_processing_tools] <1052> -- Separating events in predictions...\n",
      "[19:01:11] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of sparks...\n",
      "[19:01:20] [ DEBUG  ] [data.data_processing_tools] <637 > -- Number of sparks detected by nonmaxima suppression: 61\n",
      "[19:01:22] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of waves...\n",
      "[19:01:22] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of puffs...\n",
      "[19:01:23] [ DEBUG  ] [data.data_processing_tools] <1061> -- Removing small events and merging events...\n",
      "[19:01:31] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of sparks instances: 60\n",
      "[19:01:31] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of waves instances: 7\n",
      "[19:01:31] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of puffs instances: 33\n",
      "[19:01:34] [ DEBUG  ] [data.data_processing_tools] <1100> -- Time for removing small events: 11.75 s\n",
      "[19:01:35] [ DEBUG  ] [utils.training_inference_tools] <1030> -- Time to process predictions: 24.59 s\n",
      "[19:01:35] [ DEBUG  ] [utils.training_inference_tools] <1050> -- Test function: computing pairwise scores between 24 annotated events and 79 predicted events\n",
      "[19:01:43] [ DEBUG  ] [utils.training_inference_tools] <1063> -- Time to compute pairwise scores: 8.14 s\n",
      "[19:01:43] [ DEBUG  ] [utils.training_inference_tools] <1070> -- Test function: getting matches summary\n",
      "[19:01:45] [ DEBUG  ] [utils.training_inference_tools] <1092> -- Time to get matches summary: 2.27 s\n",
      "[19:01:45] [ DEBUG  ] [utils.training_inference_tools] <975 > -- Processing sample 40\n",
      "[19:01:45] [ DEBUG  ] [utils.training_inference_tools] <984 > -- Test function: saving raw predictions on disk\n",
      "[19:01:46] [ DEBUG  ] [utils.training_inference_tools] <998 > -- Test function: re-organising annotations\n",
      "[19:01:48] [ DEBUG  ] [utils.training_inference_tools] <1008> -- Time to re-organise annotations: 2.12 s\n",
      "[19:01:48] [ DEBUG  ] [utils.training_inference_tools] <1013> -- Test function: getting processed output (segmentation and instances)\n",
      "[19:01:49] [ DEBUG  ] [data.data_processing_tools] <547 > -- Events detection threshold: 0.498\n",
      "[19:01:50] [ DEBUG  ] [data.data_processing_tools] <1052> -- Separating events in predictions...\n",
      "[19:01:50] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of sparks...\n",
      "[19:01:59] [ DEBUG  ] [data.data_processing_tools] <637 > -- Number of sparks detected by nonmaxima suppression: 10\n",
      "[19:02:01] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of waves...\n",
      "[19:02:01] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of puffs...\n",
      "[19:02:02] [ DEBUG  ] [data.data_processing_tools] <1061> -- Removing small events and merging events...\n",
      "[19:02:05] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of sparks instances: 10\n",
      "[19:02:05] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of waves instances: 1\n",
      "[19:02:05] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of puffs instances: 10\n",
      "[19:02:08] [ DEBUG  ] [data.data_processing_tools] <1100> -- Time for removing small events: 5.73 s\n",
      "[19:02:08] [ DEBUG  ] [utils.training_inference_tools] <1030> -- Time to process predictions: 19.39 s\n",
      "[19:02:08] [ DEBUG  ] [utils.training_inference_tools] <1050> -- Test function: computing pairwise scores between 13 annotated events and 18 predicted events\n",
      "[19:02:12] [ DEBUG  ] [utils.training_inference_tools] <1063> -- Time to compute pairwise scores: 4.54 s\n",
      "[19:02:12] [ DEBUG  ] [utils.training_inference_tools] <1070> -- Test function: getting matches summary\n",
      "[19:02:14] [ DEBUG  ] [utils.training_inference_tools] <1092> -- Time to get matches summary: 1.86 s\n",
      "[19:02:14] [ DEBUG  ] [utils.training_inference_tools] <975 > -- Processing sample 45\n",
      "[19:02:14] [ DEBUG  ] [utils.training_inference_tools] <984 > -- Test function: saving raw predictions on disk\n",
      "[19:02:15] [ DEBUG  ] [utils.training_inference_tools] <998 > -- Test function: re-organising annotations\n",
      "[19:02:16] [ DEBUG  ] [utils.training_inference_tools] <1008> -- Time to re-organise annotations: 0.75 s\n",
      "[19:02:16] [ DEBUG  ] [utils.training_inference_tools] <1013> -- Test function: getting processed output (segmentation and instances)\n",
      "[19:02:16] [ DEBUG  ] [data.data_processing_tools] <547 > -- Events detection threshold: 0.494\n",
      "[19:02:17] [ DEBUG  ] [data.data_processing_tools] <1052> -- Separating events in predictions...\n",
      "[19:02:17] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of sparks...\n",
      "[19:02:27] [ DEBUG  ] [data.data_processing_tools] <637 > -- Number of sparks detected by nonmaxima suppression: 19\n",
      "[19:02:28] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of waves...\n",
      "[19:02:29] [ DEBUG  ] [data.data_processing_tools] <604 > -- Separating instances of puffs...\n",
      "[19:02:29] [ DEBUG  ] [data.data_processing_tools] <1061> -- Removing small events and merging events...\n",
      "[19:02:34] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of sparks instances: 19\n",
      "[19:02:34] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of waves instances: 1\n",
      "[19:02:34] [ DEBUG  ] [data.data_processing_tools] <1076> -- Number of puffs instances: 7\n",
      "[19:02:36] [ DEBUG  ] [data.data_processing_tools] <1100> -- Time for removing small events: 6.58 s\n",
      "[19:02:36] [ DEBUG  ] [utils.training_inference_tools] <1030> -- Time to process predictions: 20.23 s\n",
      "[19:02:36] [ DEBUG  ] [utils.training_inference_tools] <1050> -- Test function: computing pairwise scores between 1 annotated events and 21 predicted events\n",
      "[19:02:37] [ DEBUG  ] [utils.training_inference_tools] <1063> -- Time to compute pairwise scores: 1.13 s\n",
      "[19:02:37] [ DEBUG  ] [utils.training_inference_tools] <1070> -- Test function: getting matches summary\n",
      "[19:02:39] [ DEBUG  ] [utils.training_inference_tools] <1092> -- Time to get matches summary: 2.13 s\n",
      "[19:02:39] [ DEBUG  ] [utils.training_inference_tools] <1112> -- Test function: reducing metrics\n",
      "[19:02:50] [ DEBUG  ] [utils.training_inference_tools] <1183> -- Time to reduce metrics: 10.55 s\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1310> -- Metrics:\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tvalidation_loss: 0.5296\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tsparks/precision: 0.3912\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tsparks/recall: 0.5774\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tsparks/correctly_classified: 0.5945\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tsparks/detected: 0.6566\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tsparks/f1-score: 0.4664\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tsparks/labeled: 0.658\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \twaves/precision: 1\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \twaves/recall: 0.5714\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \twaves/correctly_classified: 1\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \twaves/detected: 0.8571\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \twaves/f1-score: 0.7273\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \twaves/labeled: 1\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tpuffs/precision: 0.374\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tpuffs/recall: 0.5541\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tpuffs/correctly_classified: 0.6622\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tpuffs/detected: 0.7432\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tpuffs/f1-score: 0.4466\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tpuffs/labeled: 0.5649\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \ttotal/precision: 0.3916\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \ttotal/recall: 0.5723\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \ttotal/correctly_classified: 0.6145\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \ttotal/detected: 0.6792\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \ttotal/f1-score: 0.465\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \ttotal/labeled: 0.6372\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \taverage/precision: 0.5884\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \taverage/recall: 0.5676\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \taverage/correctly_classified: 0.7522\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \taverage/detected: 0.7523\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \taverage/f1-score: 0.5467\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \taverage/labeled: 0.741\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tsegmentation/sparks_IoU: 0.1972\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tsegmentation/waves_IoU: 0.2804\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tsegmentation/puffs_IoU: 0.1786\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1316> -- \tsegmentation/average_IoU: 0.2187\n",
      "[19:02:50] [  INFO  ] [utils.training_inference_tools] <1314> -- \tsegmentation_confusion_matrix:\n",
      "[[214692203     88162    782659    317985]\n",
      " [   111747     75850         0     27829]\n",
      " [  2313480     26353   1464843    637354]\n",
      " [  1080134     54738         0    460456]]\n"
     ]
    }
   ],
   "source": [
    "# Validate the network before training if resuming from a checkpoint\n",
    "if True:  # params.load_epoch > 0:\n",
    "    logger.info(\"Validate network before training\")\n",
    "    trainer.run_validation(wandb_log=params.wandb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the network in training mode\n",
    "network.train()\n",
    "\n",
    "# Train the model for the specified number of epochs\n",
    "logger.info(\"Starting training\")\n",
    "trainer.train(\n",
    "    params.train_epochs,\n",
    "    print_every=params.c.getint(\"training\", \"print_every\", fallback=100),\n",
    "    wandb_log=params.wandb_log,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:31:51] [  INFO  ] [  __main__  ] < 1  > -- Starting final validation\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting final validation\")\n",
    "# Run the final validation/testing procedure\n",
    "# trainer.run_validation(wandb_log=params.wandb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">final_model_w_init</strong> at: <a href='https://wandb.ai/dottip/sparks_thesis/runs/final_model_w_init' target=\"_blank\">https://wandb.ai/dottip/sparks_thesis/runs/final_model_w_init</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240428_184254-final_model_w_init\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Close the summary writer\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the wandb run\n",
    "if params.wandb_log:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging purposes\n",
    "# model_parameters = filter(lambda p: p.requires_grad, network.parameters())\n",
    "# model_parameters = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# logger.debug(f\"Number of trainable parameters: {model_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for load_epoch in [10000,20000,30000,40000,50000,60000,70000,80000,90000,100000]:\n",
    "# for load_epoch in [100000]:\n",
    "#     trainer.load(load_epoch)\n",
    "#     logger.info(\"Starting final validation\")\n",
    "#     trainer.run_validation(wandb_log=wandb_log)\n",
    "# if wandb_log:\n",
    "#     wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize UNet architecture (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get number of trainable parameters\n",
    "# num_params = sum(p.numel() for p in network.parameters() if p.requires_grad)\n",
    "# logger.debug(f\"Number of trainable parameters: {num_params}\")\n",
    "# # get dummy unet input\n",
    "# batch = next(iter(dataset_loader))\n",
    "# x = batch[0].to(device)\n",
    "# yhat = network(x[:,None]) # Give dummy batch to forward()\n",
    "# from torchviz import make_dot\n",
    "# make_dot(yhat, params=dict(list(network.named_parameters()))).render(\"unet_model\", format=\"png\")\n",
    "# a = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "# len(a[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d33eb8e81965b779f2871c6ab1ae98a760df4ff814358c9a5efa0a44482010f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
