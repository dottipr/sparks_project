{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference script\n",
    "\n",
    "Da usare per:\n",
    "- creare gli output video\n",
    "- testare nuovi metodi di inferenza\n",
    "- calcolare metrics di un modello salvato\n",
    "- testare nuovi metodi per valutare le preds\n",
    "\n",
    "Nel config file, modificare i parametri nella \"testing\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload is used to reload modules automatically before entering the\n",
    "# execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To import modules from parent directory in Jupyter Notebook\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import TrainingConfig, config\n",
    "from data.data_processing_tools import masks_to_instances_dict, process_raw_predictions\n",
    "from utils.in_out_tools import write_videos_on_disk\n",
    "\n",
    "# from torch.cuda.amp import GradScaler\n",
    "from utils.training_inference_tools import do_inference\n",
    "from utils.training_script_utils import init_dataset, init_model, get_sample_ids\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:39:44] [  INFO  ] [   config   ] <288 > -- Loading ..\\config_files\\config_final_model.ini\n"
     ]
    }
   ],
   "source": [
    "config.verbosity = 3  # To get debug messages\n",
    "\n",
    "##################### Get training-specific parameters #####################\n",
    "\n",
    "# run_name = \"final_model\"\n",
    "run_name = \"TEMP_new_annotated_peaks_physio\"  # TEMP local (run on laptop)\n",
    "config_filename = \"config_final_model.ini\"\n",
    "load_epoch = 100000\n",
    "\n",
    "use_train_data = False\n",
    "get_final_pred = True  # set to False to only compute raw predictions\n",
    "testing = False  # set to False to only generate unet predictions\n",
    "# set to True to also compute processed outputs and metrics\n",
    "# inference_types = ['overlap', 'average', 'gaussian', 'max']\n",
    "inference_types = None  # set to None to use the default inference type from\n",
    "# the config file\n",
    "\n",
    "# Initialize general parameters\n",
    "params = TrainingConfig(\n",
    "    training_config_file=os.path.join(\"..\", \"config_files\", config_filename)  # notebook\n",
    "    # training_config_file=os.path.join(\"config_files\", config_filename)\n",
    ")\n",
    "if run_name:\n",
    "    params.run_name = run_name\n",
    "model_filename = f\"network_{load_epoch:06d}.pth\"\n",
    "\n",
    "# Print parameters to console if needed\n",
    "# params.print_params()\n",
    "\n",
    "if testing:\n",
    "    get_final_pred = True\n",
    "\n",
    "debug = True if config.verbosity == 3 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:39:46] [  INFO  ] [  __main__  ] < 16 > -- Annotations and predictions will be saved on 'evaluation\\inference_script\\final_model'\n"
     ]
    }
   ],
   "source": [
    "######################### Configure output folder ##########################\n",
    "\n",
    "output_folder = os.path.join(\n",
    "    \"evaluation\", \"inference_script\"\n",
    ")  # Same folder for train and test preds\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Subdirectory of output_folder where predictions are saved.\n",
    "# Change this to save results for same model with different inference\n",
    "# approaches.\n",
    "# output_name = training_name + \"_step=2\"\n",
    "output_name = params.run_name\n",
    "\n",
    "save_folder = os.path.join(output_folder, output_name)\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "logger.info(f\"Annotations and predictions will be saved on '{save_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:39:47] [  INFO  ] [   config   ] <520 > -- Using cuda\n"
     ]
    }
   ],
   "source": [
    "########################### Detect GPU, if available ###########################\n",
    "\n",
    "params.set_device(device=\"auto\")\n",
    "params.display_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:40:14] [  INFO  ] [  __main__  ] < 3  > -- Processing training 'final_model'...\n",
      "[15:40:14] [  INFO  ] [  __main__  ] < 9  > -- Predicting outputs for samples ['34']\n",
      "[15:40:14] [  INFO  ] [  __main__  ] < 11 > -- Using C:\\Users\\prisc\\Code\\sparks_project\\data\\sparks_dataset as dataset root path\n",
      "[15:40:14] [WARNING ] [tifffile.tifffile] <16549> -- TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n",
      "[15:40:14] [WARNING ] [tifffile.tifffile] <16549> -- TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n",
      "[15:40:14] [  INFO  ] [utils.training_script_utils] <136 > -- Samples in training dataset: 22\n",
      "[15:40:15] [  INFO  ] [  __main__  ] < 44 > -- Loading trained model 'final_model' at epoch 100000...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models\\\\saved_models\\\\final_model\\\\network_100000.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prisc\\Code\\sparks_project\\notebooks\\DEBUG inference.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prisc/Code/sparks_project/notebooks/DEBUG%20inference.ipynb#X63sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Load the model state dictionary\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prisc/Code/sparks_project/notebooks/DEBUG%20inference.ipynb#X63sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading trained model \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrun_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m at epoch \u001b[39m\u001b[39m{\u001b[39;00mload_epoch\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prisc/Code/sparks_project/notebooks/DEBUG%20inference.ipynb#X63sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m network\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_path, map_location\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mdevice))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prisc/Code/sparks_project/notebooks/DEBUG%20inference.ipynb#X63sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m network\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models\\\\saved_models\\\\final_model\\\\network_100000.pth'"
     ]
    }
   ],
   "source": [
    "###################### Config dataset and UNet model #######################\n",
    "\n",
    "logger.info(f\"Processing training '{params.run_name}'...\")\n",
    "\n",
    "# Define the sample IDs based on dataset size and usage\n",
    "sample_ids = get_sample_ids(\n",
    "    train_data=use_train_data, dataset_size=params.dataset_size, custom_ids=[]\n",
    ")\n",
    "logger.info(f\"Predicting outputs for samples {sample_ids}\")\n",
    "\n",
    "logger.info(f\"Using {params.dataset_dir} as dataset root path\")\n",
    "\n",
    "# Create dataset\n",
    "dataset = init_dataset(\n",
    "    params=params,\n",
    "    sample_ids=sample_ids,\n",
    "    inference_dataset=True,\n",
    "    print_dataset_info=True,\n",
    ")\n",
    "\n",
    "# Create a dataloader\n",
    "dataset_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=params.inference_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=params.num_workers,\n",
    "    pin_memory=params.pin_memory,\n",
    ")\n",
    "\n",
    "### Configure UNet ###\n",
    "\n",
    "network = init_model(params=params)\n",
    "network = nn.DataParallel(network).to(params.device)\n",
    "\n",
    "### Load UNet model ###\n",
    "\n",
    "# Path to the saved model checkpoint\n",
    "models_relative_path = os.path.join(\n",
    "    \"models\", \"saved_models\", params.run_name, model_filename\n",
    ")\n",
    "model_dir = os.path.realpath(os.path.join(config.basedir, models_relative_path))\n",
    "\n",
    "# Load the model state dictionary\n",
    "logger.info(f\"Loading trained model '{run_name}' at epoch {load_epoch}...\")\n",
    "network.load_state_dict(torch.load(model_dir, map_location=params.device))\n",
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Run samples in UNet ############################\n",
    "\n",
    "if inference_types is None:\n",
    "    inference_types = [params.inference]\n",
    "\n",
    "# get U-Net's raw predictions\n",
    "raw_preds = do_inference(\n",
    "    network=network,\n",
    "    params=params,\n",
    "    dataloader=dataset_loader,\n",
    "    device=params.device,\n",
    "    compute_loss=False,\n",
    "    inference_types=inference_types,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Get movies and labels (and instances if testing) #############\n",
    "\n",
    "xs = dataset.get_movies()\n",
    "ys = dataset.get_labels()\n",
    "\n",
    "if testing:\n",
    "    ys_instances = dataset.get_instances()\n",
    "\n",
    "    # convert instance masks to dictionaries\n",
    "    ys_instances = {\n",
    "        i: masks_to_instances_dict(\n",
    "            instances_mask=instances_mask,\n",
    "            labels_mask=ys[i],\n",
    "            shift_ids=True,\n",
    "        )\n",
    "        for i, instances_mask in ys_instances.items()\n",
    "    }\n",
    "\n",
    "    # remove ignored events entry from ys_instances\n",
    "    for inference in ys_instances:\n",
    "    ys_instances[inference].pop(\"ignore\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Get processed output (if required) ####################\n",
    "\n",
    "if get_final_pred:\n",
    "    logger.debug(\"Getting processed output (segmentation and instances)\")\n",
    "\n",
    "    final_segmentation_dict = {}\n",
    "    final_instances_dict = {}\n",
    "    for i in range(len(sample_ids)):\n",
    "        movie_segmentation = {}\n",
    "        movie_instances = {}\n",
    "\n",
    "        for inference in inference_types:\n",
    "            # transform raw predictions into a dictionary\n",
    "            raw_preds_dict = {\n",
    "                event_type: raw_preds[i][inference][event_label]\n",
    "                for event_type, event_label in config.classes_dict\n",
    "            }\n",
    "\n",
    "            preds_instances, preds_segmentation, _ = process_raw_predictions(\n",
    "                raw_preds_dict=raw_preds_dict,\n",
    "                input_movie=xs[i],\n",
    "                training_mode=False,\n",
    "                debug=debug,\n",
    "            )\n",
    "\n",
    "            movie_segmentation[inference] = preds_segmentation\n",
    "            movie_instances[inference] = preds_instances\n",
    "\n",
    "        final_segmentation_dict[sample_ids[i]] = movie_segmentation\n",
    "        final_instances_dict[sample_ids[i]] = movie_instances\n",
    "\n",
    "else:\n",
    "    final_segmentation_dict = {}\n",
    "    final_instances_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Save preds on disk ############################\n",
    "\n",
    "logger.info(f\"\\tSaving annotations and predictions...\")\n",
    "\n",
    "for i, sample_id in enumerate(sample_ids):\n",
    "    for inference in inference_types:\n",
    "        video_name = f\"{str(params.load_epoch)}_{sample_id}_{inference}\"\n",
    "\n",
    "        raw_preds_movie = raw_preds[i][inference]\n",
    "        if get_final_pred:\n",
    "            segmented_preds_movie = final_segmentation_dict[sample_id][inference]\n",
    "            instances_preds_movie = final_instances_dict[sample_id][inference]\n",
    "        else:\n",
    "            segmented_preds_movie = None\n",
    "            instances_preds_movie = None\n",
    "\n",
    "        write_videos_on_disk(\n",
    "            training_name=output_name,\n",
    "            video_name=video_name,\n",
    "            path=os.path.join(save_folder, \"inference_\" + inference),\n",
    "            xs=xs[i],\n",
    "            ys=ys[i],\n",
    "            raw_preds=raw_preds_movie,\n",
    "            segmented_preds=segmented_preds_movie,\n",
    "            instances_preds=instances_preds_movie,\n",
    "        )\n",
    "\n",
    "logger.info(f\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize preds with Napari\n",
    "\n",
    "- load given training at given iteration (testing section of config file) and visualize predictions using Napari\n",
    "- (idea is to check if it makes sense to stop the training earlier)\n",
    "- adapt dataset size (& other params) in config file prior to execute code above if necessary\n",
    "\n",
    "Trainings already checked with this method:\n",
    "- final_model (minimal dataset, only movie 34)\n",
    "    - 10K; 20K; 30K; 40K: too many sparks, does not make sense (>2K after correction)\n",
    "    - 50K: too many sparks, does not make sense (>1K after correction)\n",
    "    - 60K: 72 sparks before correction/82 sparks after correction\n",
    "    - 70K: 71/78 -> does not change much from 60K aesthetically\n",
    "    - 80K: 53/55 -> same; in the end what matters is to chose the best metrics (I believe)\n",
    "    - 90K: 51/55 -> same\n",
    "    - 100K: 61/63 -> same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = get_discrete_cmap(name=\"gray\", lut=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_classes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = sample_ids[0]\n",
    "\n",
    "# get contours of annotations, for visualization\n",
    "ys_contours = get_annotations_contour(annotations=ys[sample_id], contour_val=2)\n",
    "\n",
    "# get predicted segmentation and event instances\n",
    "# _, raw_sparks, raw_waves, raw_puffs = preds[sample_id]\n",
    "# preds_events = preds_instances[sample_id]\n",
    "\n",
    "preds_classes[params[\"load_epoch\"]] = (\n",
    "    preds_segmentation[sample_id][\"sparks\"]\n",
    "    + 3 * preds_segmentation[sample_id][\"puffs\"]\n",
    "    + 2 * preds_segmentation[sample_id][\"waves\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(input_movies[sample_id], name=\"raw\", colormap=(\"colors\", cmap))\n",
    "viewer.add_labels(ys_contours, name=\"gt\", opacity=0.5, color=get_labels_cmap())\n",
    "\n",
    "# viewer.add_image(raw_sparks, name='raw_sparks')\n",
    "# viewer.add_image(raw_waves, name='raw_waves')\n",
    "# viewer.add_image(raw_puffs, name='raw_puffs')\n",
    "for epoch in preds_classes.keys():\n",
    "    viewer.add_labels(\n",
    "        preds_classes[epoch],\n",
    "        name=f\"preds_{epoch}\",\n",
    "        opacity=0.3,\n",
    "        color=get_labels_cmap(),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics\n",
    "\n",
    "- if considering more than one inference type:\n",
    "    - `preds_instances` is a nested dict indexed first by movie id, then by inference type, and finally by class. E.g., `preds_instances['05']['overlap']['sparks']` is a numpy array of shape (500, 64, 512) with integer values denoting the events' IDs.\n",
    "    - `preds_segmentation` is a nested dict indexed first by movie id, then by inference type, and finally by class e.g., `preds_segmentation['05']['overlap']['sparks']` is a numpy array of shape (500, 64, 512) with boolean values denoting the events' presence.\n",
    "- if considering only one inference type: `preds_instances` and `preds_segmentation`\n",
    "are the same as above, buth without the inference type index \n",
    "- `ys_instances` is a nested dict indexed by movie id and class of events containing arrays with integer values denoting the events' IDs.\n",
    "- `ys` is a dict indexed by movie id with integers between 0 and 4 to denote the class of the annotated events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not testing:\n",
    "    print(\"!!!!!!!!! THE FOLLOWING CODE WON'T WORK !!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loss on all samples\n",
    "sum_loss = 0.0  # SERVE???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to unify the results for any number of inferences, transform 'preds_instances'\n",
    "# and 'preds_segmentation' adding a new \"nested\" key 'inference_type'\n",
    "\n",
    "if len(inference_types) == 1:\n",
    "    temp_instances = {\n",
    "        movie_id: {params.inference: preds_instances[movie_id]}\n",
    "        for movie_id in sample_ids\n",
    "    }\n",
    "    temp_segmentation = {\n",
    "        movie_id: {params.inference: preds_segmentation[movie_id]}\n",
    "        for movie_id in sample_ids\n",
    "    }\n",
    "\n",
    "    preds_instances = temp_instances\n",
    "    preds_segmentation = temp_segmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation-based metrics (i.e., IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the columns of the dataframe\n",
    "segmentation_cols = [\"inference_type\", \"event_type\", \"iou\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate annotations and preds to compute segmentation-based metrics\n",
    "ys_concat = []\n",
    "preds_concat = {i: [] for i in inference_types}\n",
    "\n",
    "for sample_id in sample_ids:\n",
    "    ys_concat.append(ys[sample_id])\n",
    "\n",
    "    for i in inference_types:\n",
    "        # get preds segmentation as integer array with values in [0, 1, 2, 3]\n",
    "        temp_preds = dict_to_int_mask(preds_segmentation[sample_id][i])\n",
    "        preds_concat[i].append(temp_preds)\n",
    "\n",
    "ys_concat = np.concatenate(ys_concat)\n",
    "preds_concat = {i: np.concatenate(preds_concat[i]) for i in inference_types}\n",
    "\n",
    "# get masks for pixels labelled with 4\n",
    "ignore_concat = ys_concat == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute intersection over union for each inference type\n",
    "# (per class, average, and as binary classification)\n",
    "\n",
    "iou_dict = {i: {} for i in inference_types}\n",
    "\n",
    "for i in inference_types:\n",
    "    for event_type, event_label in config.classes_dict.items():\n",
    "        if event_type in [\"ignore\", \"background\"]:\n",
    "            continue\n",
    "        class_ys = ys_concat == event_label\n",
    "        class_preds = preds_concat[i] == event_label\n",
    "\n",
    "        iou_dict[i][event_type] = compute_iou(\n",
    "            ys_roi=class_ys, preds_roi=class_preds, ignore_mask=ignore_concat\n",
    "        )\n",
    "\n",
    "    # compute average iou\n",
    "    iou_dict[i][\"average\"] = np.mean(list(iou_dict[i].values()))\n",
    "\n",
    "    # compute binary classification iou\n",
    "    iou_dict[i][\"binary\"] = compute_iou(\n",
    "        ys_roi=ys_concat != 0, preds_roi=preds_concat[i] != 0, ignore_mask=ignore_concat\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 3)\n",
    "\n",
    "# create dataframe where index is event type and columns are inference types\n",
    "df_barplot = pd.DataFrame(iou_dict).T.T\n",
    "df_barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_barplot.plot.bar(rot=0, figsize=(10, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance-based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_release_events = [\n",
    "    event_type\n",
    "    for event_type in config.classes_dict.keys()\n",
    "    if event_type not in [\"ignore\", \"background\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum events over all samples\n",
    "preds_cat = [\"tot\", \"tp\", \"ignored\", \"unlabeled\"] + ca_release_events\n",
    "ys_cat = [\"tot\", \"tp\", \"undetected\"] + ca_release_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dicts that will contain the results\n",
    "matched_preds_ids = {i: {} for i in inference_types}\n",
    "matched_ys_ids = {i: {} for i in inference_types}\n",
    "\n",
    "for i in inference_types:\n",
    "    matched_preds_ids[i][\"sum\"] = {}\n",
    "    matched_ys_ids[i][\"sum\"] = {}\n",
    "    for ca_event in ca_release_events:\n",
    "        matched_preds_ids[i][\"sum\"][ca_event] = {\n",
    "            cat: 0 for cat in preds_cat if cat != ca_event\n",
    "        }\n",
    "        matched_ys_ids[i][\"sum\"][ca_event] = {\n",
    "            cat: 0 for cat in ys_cat if cat != ca_event\n",
    "        }\n",
    "\n",
    "for sample_id in sample_ids:\n",
    "    logger.info(f\"Processing sample {sample_id}...\")\n",
    "\n",
    "    # get ignore mask for this sample\n",
    "    ignore_mask = ys[sample_id] == 4\n",
    "\n",
    "    # compute pairwise scores between annotated and predicted ROIs\n",
    "    # (for each inference type)\n",
    "    for i in inference_types:\n",
    "        logger.info(f\"\\tInference type {i}...\")\n",
    "        iomin_scores = get_score_matrix(\n",
    "            ys_instances=ys_instances[sample_id],\n",
    "            preds_instances=preds_instances[sample_id][i],\n",
    "            ignore_mask=None,\n",
    "            score=\"iomin\",\n",
    "        )\n",
    "\n",
    "        # get ids of matched ROIs\n",
    "        (\n",
    "            matched_ys_ids[i][sample_id],\n",
    "            matched_preds_ids[i][sample_id],\n",
    "        ) = get_matches_summary(\n",
    "            ys_instances=ys_instances[sample_id],\n",
    "            preds_instances=preds_instances[sample_id][i],\n",
    "            scores=iomin_scores,\n",
    "            ignore_mask=ignore_mask,\n",
    "        )\n",
    "\n",
    "        # count number of categorized events that are necessary for the metrics\n",
    "        for ca_event in ca_release_events:\n",
    "            for cat in matched_ys_ids[i][sample_id][ca_event].keys():\n",
    "                matched_ys_ids[i][\"sum\"][ca_event][cat] += len(\n",
    "                    matched_ys_ids[i][sample_id][ca_event][cat]\n",
    "                )\n",
    "\n",
    "            for cat in matched_preds_ids[i][sample_id][ca_event].keys():\n",
    "                matched_preds_ids[i][\"sum\"][ca_event][cat] += len(\n",
    "                    matched_preds_ids[i][sample_id][ca_event][cat]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dict that only contains sum over all samples\n",
    "tot_preds = {\n",
    "    i: {\n",
    "        ca_class: matched_preds_ids[i][\"sum\"][ca_class][\"tot\"]\n",
    "        for ca_class in ca_release_events\n",
    "    }\n",
    "    for i in inference_types\n",
    "}\n",
    "tp_preds = {\n",
    "    i: {\n",
    "        ca_class: matched_preds_ids[i][\"sum\"][ca_class][\"tp\"]\n",
    "        for ca_class in ca_release_events\n",
    "    }\n",
    "    for i in inference_types\n",
    "}\n",
    "ignored_preds = {\n",
    "    i: {\n",
    "        ca_class: matched_preds_ids[i][\"sum\"][ca_class][\"ignored\"]\n",
    "        for ca_class in ca_release_events\n",
    "    }\n",
    "    for i in inference_types\n",
    "}\n",
    "unlabeled_preds = {\n",
    "    i: {\n",
    "        ca_class: matched_preds_ids[i][\"sum\"][ca_class][\"unlabeled\"]\n",
    "        for ca_class in ca_release_events\n",
    "    }\n",
    "    for i in inference_types\n",
    "}\n",
    "tot_ys = {\n",
    "    i: {\n",
    "        ca_class: matched_ys_ids[i][\"sum\"][ca_class][\"tot\"]\n",
    "        for ca_class in ca_release_events\n",
    "    }\n",
    "    for i in inference_types\n",
    "}\n",
    "tp_ys = {\n",
    "    i: {\n",
    "        ca_class: matched_ys_ids[i][\"sum\"][ca_class][\"tp\"]\n",
    "        for ca_class in ca_release_events\n",
    "    }\n",
    "    for i in inference_types\n",
    "}\n",
    "undetected_ys = {\n",
    "    i: {\n",
    "        ca_class: matched_ys_ids[i][\"sum\"][ca_class][\"undetected\"]\n",
    "        for ca_class in ca_release_events\n",
    "    }\n",
    "    for i in inference_types\n",
    "}\n",
    "\n",
    "\n",
    "metrics_all = {i: {} for i in inference_types}\n",
    "\n",
    "# get other metrics (precision, recall, % correctly classified, % detected)\n",
    "for i in inference_types:\n",
    "    metrics_all[i] = get_metrics_from_summary(\n",
    "        tot_preds=tot_preds[i],\n",
    "        tp_preds=tp_preds[i],\n",
    "        ignored_preds=ignored_preds[i],\n",
    "        unlabeled_preds=unlabeled_preds[i],\n",
    "        tot_ys=tot_ys[i],\n",
    "        tp_ys=tp_ys[i],\n",
    "        undetected_ys=undetected_ys[i],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute same metrics but as % instad of absolute numbers\n",
    "matched_preds_percent = {\n",
    "    i: {ca_event: {} for ca_event in ca_release_events} for i in inference_types\n",
    "}\n",
    "matched_ys_percent = {\n",
    "    i: {ca_event: {} for ca_event in ca_release_events} for i in inference_types\n",
    "}\n",
    "\n",
    "for i in inference_types:\n",
    "    for ca_event in ca_release_events:\n",
    "        for cat in matched_preds_ids[i][\"sum\"][ca_event].keys():\n",
    "            if cat != \"tot\":\n",
    "                matched_preds_percent[i][ca_event][cat] = (\n",
    "                    matched_preds_ids[i][\"sum\"][ca_event][cat]\n",
    "                    / matched_preds_ids[i][\"sum\"][ca_event][\"tot\"]\n",
    "                    * 100\n",
    "                )\n",
    "\n",
    "        for cat in matched_ys_ids[i][\"sum\"][ca_event].keys():\n",
    "            if cat != \"tot\":\n",
    "                matched_ys_percent[i][ca_event][cat] = (\n",
    "                    matched_ys_ids[i][\"sum\"][ca_event][cat]\n",
    "                    / matched_ys_ids[i][\"sum\"][ca_event][\"tot\"]\n",
    "                    * 100\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary of detected events\")\n",
    "for i in inference_types:\n",
    "    df = get_df_summary_events(\n",
    "        inference_type=i,\n",
    "        matched_ids=matched_preds_ids,\n",
    "        matched_percent=matched_preds_percent,\n",
    "        is_detected=True,\n",
    "    )\n",
    "    # Format the DataFrame\n",
    "    styled_df = df.style.format(precision=2, na_rep=\"N/A\")\n",
    "    print(f\"{i} inference\")\n",
    "    display(styled_df)\n",
    "\n",
    "print(\"Summary of labeled events\")\n",
    "for i in inference_types:\n",
    "    df = get_df_summary_events(\n",
    "        inference_type=i,\n",
    "        matched_ids=matched_ys_ids,\n",
    "        matched_percent=matched_ys_percent,\n",
    "        is_detected=False,\n",
    "    )\n",
    "    # Format the DataFrame\n",
    "    styled_df = df.style.format(precision=2, na_rep=\"N/A\")\n",
    "    print(f\"{i} inference\")\n",
    "    display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inference_types:\n",
    "    df = get_df_metrics(inference_type=i, metrics_all=metrics_all)\n",
    "    # Set the display precision to 2 decimal places\n",
    "    pd.set_option(\"display.precision\", 2)\n",
    "    # Format the DataFrame\n",
    "    styled_df = df.style.format(precision=2)\n",
    "    print(f\"Metrics using {i} inference\")\n",
    "    display(styled_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize other properties of data..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histograms of raw predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(5, 5))\n",
    "\n",
    "for i, ax in zip(inference_types, axs.flatten()):\n",
    "    # concatenate raw predictions\n",
    "    raw_preds_concat = [preds_dict[sample_id][i] for sample_id in sample_ids]\n",
    "    raw_preds_concat = np.concatenate(raw_preds_concat, axis=1)\n",
    "\n",
    "    ax.hist(raw_preds_concat.flatten(), bins=10)\n",
    "    ax.set_title(\n",
    "        f\"Histogram of raw predictions \\n(inference type '{i}')\", fontsize=10.0\n",
    "    )\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code to try and visualize different types of inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DEBUG ###\n",
    "\n",
    "# test_dataset = SparkDataset(\n",
    "#         base_path=dataset_path,\n",
    "#         sample_ids=['34'],\n",
    "#         testing=testing,\n",
    "#         smoothing=c.get(\"dataset\", \"data_smoothing\"),\n",
    "#         step=c.getint(\"testing\", \"data_stride\"),\n",
    "#         #step=2,\n",
    "#         duration=c.getint(\"testing\", \"data_duration\"),\n",
    "#         remove_background=c.get(\"dataset\", \"remove_background\"),\n",
    "#         temporal_reduction=c.getboolean(\n",
    "#             \"network\", \"temporal_reduction\", fallback=False\n",
    "#         ),\n",
    "#         num_channels=c.getint(\"network\", \"num_channels\", fallback=1),,\n",
    "#         normalize_video=c.get(\"dataset\", \"norm_video\"),\n",
    "#         only_sparks=c.getboolean(\"dataset\", \"only_sparks\", fallback=False),\n",
    "#         sparks_type=c.get(\"dataset\", \"sparks_type\"),\n",
    "#         ignore_frames=c.getint(\"training\", \"ignore_frames_loss\"),\n",
    "#         ignore_index=4,\n",
    "#         gt_available=True,\n",
    "#         inference=inference,\n",
    "#     )\n",
    "\n",
    "# testing_dataloader = torch.utils.data.DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "\n",
    "# from training_inference_tools import do_inference\n",
    "\n",
    "# test_dataset.inference = 'overlap'\n",
    "# pred_overlap = do_inference(network=network,\n",
    "#                             test_dataset=test_dataset,\n",
    "#                             test_dataloader=testing_dataloader,\n",
    "#                             device=device,\n",
    "#                             )\n",
    "\n",
    "\n",
    "# test_dataset.inference = 'average'\n",
    "# pred_average = do_inference(network=network,\n",
    "#                             test_dataset=test_dataset,\n",
    "#                             test_dataloader=testing_dataloader,\n",
    "#                             device=device,\n",
    "#                             )\n",
    "\n",
    "# test_dataset.inference = 'max'\n",
    "# pred_max = do_inference(network=network,\n",
    "#                             test_dataset=test_dataset,\n",
    "#                             test_dataloader=testing_dataloader,\n",
    "#                             device=device,\n",
    "#                             )\n",
    "\n",
    "# test_dataset.inference = 'gaussian'\n",
    "# pred_gaussian = do_inference(network=network,\n",
    "#                             test_dataset=test_dataset,\n",
    "#                             test_dataloader=testing_dataloader,\n",
    "#                             device=device,\n",
    "#                             )\n",
    "\n",
    "# pred_overlap.shape, pred_average.shape, pred_max.shape, pred_gaussian.shape\n",
    "# empty_vertical = np.ones((pred_overlap.shape[0],\n",
    "#                           pred_overlap.shape[1],\n",
    "#                           pred_overlap.shape[2],\n",
    "#                           10))*0.5\n",
    "\n",
    "# empty_horizontal = np.ones((pred_overlap.shape[0],\n",
    "#                             pred_overlap.shape[1],\n",
    "#                             10,\n",
    "#                             2*pred_overlap.shape[3]+10))*0.5\n",
    "# # stack the four predictions togheter in a squared grid\n",
    "\n",
    "# stack1 = np.concatenate((pred_overlap, empty_vertical, pred_average), axis=3)\n",
    "# stack2 = np.concatenate((pred_max, empty_vertical, pred_gaussian), axis=3)\n",
    "\n",
    "# stack_all = np.concatenate((stack1, empty_horizontal, stack2), axis=2)\n",
    "# import napari\n",
    "# viewer = napari.Viewer()\n",
    "# viewer.theme = 'dark'\n",
    "\n",
    "# viewer.add_image(stack_all[0],\n",
    "#                  name='background',\n",
    "#                     #colormap='white',\n",
    "#                     blending='additive',\n",
    "#                     opacity=0.5,\n",
    "#                     #visible=False,\n",
    "#                 )\n",
    "\n",
    "# viewer.add_image(stack_all[1],\n",
    "#                     name='sparks',\n",
    "#                     colormap='green',\n",
    "#                     blending='additive',\n",
    "#                     opacity=0.5,\n",
    "#                     #visible=False,\n",
    "#                 )\n",
    "\n",
    "# viewer.add_image(stack_all[2],\n",
    "#                     name='waves',\n",
    "#                     colormap='yellow',\n",
    "#                     blending='additive',\n",
    "#                     opacity=0.5,\n",
    "#                     #visible=False,\n",
    "#                 )\n",
    "\n",
    "# viewer.add_image(stack_all[3],\n",
    "#                     name='puffs',\n",
    "#                     colormap='red',\n",
    "#                     blending='additive',\n",
    "#                     opacity=0.5,\n",
    "#                     #visible=False,\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code to visualize a confusion matrix (OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams.update({\"font.size\": 24})\n",
    "\n",
    "# n_rows = 1\n",
    "# n_cols = 1\n",
    "# num_plots = n_rows * n_cols\n",
    "\n",
    "# pad = 5  # in points\n",
    "\n",
    "# fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 100))\n",
    "\n",
    "# cols = [\"IoU\"]\n",
    "# tick_labels = [\"Background\", \"Sparks\", \"Waves\", \"Puffs\"]\n",
    "\n",
    "# for ax, col in zip(axs[0], cols):\n",
    "#     ax.annotate(\n",
    "#         col,\n",
    "#         xy=(0.5, 1.2),\n",
    "#         xytext=(0, pad),\n",
    "#         xycoords=\"axes fraction\",\n",
    "#         textcoords=\"offset points\",\n",
    "#         size=\"large\",\n",
    "#         ha=\"center\",\n",
    "#         va=\"baseline\",\n",
    "#     )\n",
    "\n",
    "\n",
    "# for ax, row in zip(axs[:, 0], [\"TOT\"] + movie_ids):\n",
    "#     ax.annotate(\n",
    "#         row,\n",
    "#         xy=(0, 0.5),\n",
    "#         xytext=(-ax.yaxis.labelpad - pad, 0),\n",
    "#         xycoords=ax.yaxis.label,\n",
    "#         textcoords=\"offset points\",\n",
    "#         size=\"large\",\n",
    "#         ha=\"right\",\n",
    "#         va=\"center\",\n",
    "#     )\n",
    "\n",
    "# fig.suptitle(\"Confusion matrices\", fontsize=36, y=1)\n",
    "\n",
    "\n",
    "# # configure heatmap background\n",
    "# colors = sns.color_palette(\n",
    "#     [\"white\", \"lightcoral\", \"paleturquoise\", \"lemonchiffon\", \"lightgreen\"], as_cmap=True\n",
    "# )\n",
    "# colored_bg = [[0, 2, 2, 2], [1, 4, 3, 3], [1, 3, 4, 3], [1, 3, 3, 4]]\n",
    "\n",
    "# # Get array with confusion matrices to be plotted\n",
    "# cm_array = np.concatenate(\n",
    "#     (\n",
    "#         [[iou_confusion_matrix_tot, iomin_confusion_matrix_tot]],\n",
    "#         [\n",
    "#             [iou_confusion_matrix[sample_id], iomin_confusion_matrix[sample_id]]\n",
    "#             for sample_id in movie_ids\n",
    "#         ],\n",
    "#     ),\n",
    "#     axis=0,\n",
    "# )\n",
    "\n",
    "# for row_id in range(n_rows):\n",
    "#     for col_id in range(n_cols):\n",
    "#         cm = cm_array[row_id, col_id].astype(int).astype(str)\n",
    "\n",
    "#         ax = axs[row_id, col_id]\n",
    "#         sns.heatmap(\n",
    "#             data=colored_bg,\n",
    "#             cmap=colors,\n",
    "#             annot=cm,\n",
    "#             fmt=\"\",\n",
    "#             annot_kws={\"fontsize\": 36},\n",
    "#             cbar=False,\n",
    "#             square=True,\n",
    "#             ax=ax,\n",
    "#         )\n",
    "\n",
    "#         ax.tick_params(length=0, labeltop=True, labelbottom=False)\n",
    "#         ax.tick_params(axis=\"both\", which=\"major\", pad=16)\n",
    "\n",
    "#         ax.set_xlabel(\"Predicted\", labelpad=32)\n",
    "#         ax.xaxis.set_label_position(\"top\")\n",
    "#         ax.set_xticklabels(tick_labels)\n",
    "#         ax.add_patch(\n",
    "#             plt.Rectangle(\n",
    "#                 (-0.01, 1),\n",
    "#                 1.01,\n",
    "#                 0.1,\n",
    "#                 color=\"yellow\",\n",
    "#                 clip_on=False,\n",
    "#                 zorder=0,\n",
    "#                 transform=ax.transAxes,\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#         ax.set_ylabel(\"Actual Values\", labelpad=32)\n",
    "#         ax.set_yticklabels(tick_labels, rotation=90, va=\"center\")\n",
    "#         ax.add_patch(\n",
    "#             plt.Rectangle(\n",
    "#                 (0, 0),\n",
    "#                 -0.1,\n",
    "#                 1,\n",
    "#                 color=\"yellow\",\n",
    "#                 clip_on=False,\n",
    "#                 zorder=0,\n",
    "#                 transform=ax.transAxes,\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "# # plt.subplots_adjust(hspace=0.005, wspace=0.)\n",
    "# # plt.subplots_adjust(hspace=0.1, wspace=0.1, top=0.9, left=0.05, right=0.95)\n",
    "# # fig.subplots_adjust(left=0.15, top=0.95)\n",
    "# fig.subplots_adjust(wspace=1.5)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(os.path.join(out_dir, \"all_confusion_matrices.png\"))\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
