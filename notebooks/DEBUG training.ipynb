{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Model Training Script\n",
    "\n",
    "**Author:** Prisca Dotti\n",
    "\n",
    "**Last Edit:** 23.10.2023\n",
    "\n",
    "This Jupyter Notebook contains the code for training a U-Net model on a dataset of sparks videos. The dataset is split into training and testing sets, and the model is trained using the training set. The testing set is used to evaluate the performance of the trained model.\n",
    "\n",
    "To run the notebook, simply execute each cell in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload is used to reload modules automatically before entering the\n",
    "# execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# To import modules from parent directory in Jupyter Notebook\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.backends import cudnn\n",
    "\n",
    "# from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "import wandb\n",
    "from config import TrainingConfig, config\n",
    "from models.UNet import unet\n",
    "from utils.training_inference_tools import (\n",
    "    MyTrainingManager,\n",
    "    sampler,\n",
    "    test_function,\n",
    "    training_step,\n",
    "    weights_init,\n",
    ")\n",
    "from utils.training_script_utils import (\n",
    "    get_sample_ids,\n",
    "    init_config_file_path,\n",
    "    init_criterion,\n",
    "    init_dataset,\n",
    "    init_model,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:31:20] [  INFO  ] [   config   ] <291 > -- Loading C:\\Users\\dotti\\sparks_project\\config_files\\config_final_model.ini\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --     training_config_file: C:\\Users\\dotti\\sparks_project\\config_files\\config_final_model.ini\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --              dataset_dir: C:\\Users\\dotti\\sparks_project\\data\\sparks_dataset\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --                        c: <configparser.ConfigParser object at 0x0000027893A46AD0>\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --                 run_name: final_model\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --            load_run_name: \n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --               load_epoch: 0\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --             train_epochs: 90000\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --                criterion: lovasz_softmax\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --                 lr_start: 0.0001\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --       ignore_frames_loss: 6\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --                     cuda: True\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --                scheduler: None\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --                optimizer: adam\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --             dataset_size: minimal\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --               batch_size: 4\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --              num_workers: 0\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --            data_duration: 256\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --              data_stride: 32\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --           data_smoothing: no\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --               norm_video: abs_max\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --        remove_background: no\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --  noise_data_augmentation: False\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --              sparks_type: raw\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --                  new_fps: 0\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --  inference_data_duration: 256\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --    inference_data_stride: 32\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --                inference: overlap\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --     inference_load_epoch: 100000\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --     inference_batch_size: 4\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --   inference_dataset_size: full\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --          nn_architecture: pablos_unet\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --               unet_steps: 6\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --     first_layer_channels: 8\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --             num_channels: 1\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --                 dilation: 1\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --              border_mode: same\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --      batch_normalization: none\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --       temporal_reduction: False\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --       initialize_weights: True\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --                wandb_log: False\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --                   device: cuda\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --               pin_memory: True\n",
      "[12:31:20] [  INFO  ] [   config   ] <533 > --                   n_gpus: 1\n"
     ]
    }
   ],
   "source": [
    "##################### Get training-specific parameters #####################\n",
    "\n",
    "# Initialize training-specific parameters\n",
    "# (get the configuration file path from ArgParse)\n",
    "config_filename = os.path.join(\"config_files\", \"config_final_model.ini\")\n",
    "params = TrainingConfig(training_config_file=config_filename)\n",
    "\n",
    "# Print parameters to console if needed\n",
    "params.print_params()\n",
    "\n",
    "######################### Initialize random seeds ##########################\n",
    "\n",
    "# We used these random seeds to ensure reproducibility of the results\n",
    "\n",
    "# torch.manual_seed(0) <--------------------------------------------------!\n",
    "# random.seed(0) <--------------------------------------------------------!\n",
    "# np.random.seed(0) <-----------------------------------------------------!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:31:20] [  INFO  ] [utils.training_script_utils] <137 > -- Samples in training dataset: 9\n",
      "[12:31:24] [  INFO  ] [utils.training_script_utils] <137 > -- Samples in training dataset: 22\n"
     ]
    }
   ],
   "source": [
    "############################ Configure datasets ############################\n",
    "\n",
    "# Select samples for training and testing based on dataset size\n",
    "train_sample_ids = get_sample_ids(\n",
    "    train_data=True,\n",
    "    dataset_size=params.dataset_size,\n",
    ")\n",
    "test_sample_ids = get_sample_ids(\n",
    "    train_data=False,\n",
    "    dataset_size=params.dataset_size,\n",
    ")\n",
    "\n",
    "# Initialize training dataset\n",
    "dataset = init_dataset(\n",
    "    params=params,\n",
    "    sample_ids=train_sample_ids,\n",
    "    apply_data_augmentation=True,\n",
    "    load_instances=False,\n",
    ")\n",
    "\n",
    "# Initialize testing datasets\n",
    "testing_dataset = init_dataset(\n",
    "    params=params,\n",
    "    sample_ids=test_sample_ids,\n",
    "    apply_data_augmentation=False,\n",
    "    load_instances=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with only one batch\n",
    "# ids = np.arange(0, params.batch_size, 1, dtype=np.int64)\n",
    "# dataset = torch.utils.data.Subset(dataset, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loaders\n",
    "dataset_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=params.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=params.num_workers,\n",
    "    pin_memory=params.pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:31:24] [  INFO  ] [  __main__  ] < 17 > -- Initializing UNet weights...\n"
     ]
    }
   ],
   "source": [
    "############################## Configure UNet ##############################\n",
    "\n",
    "# Initialize the UNet model\n",
    "network = init_model(params=params)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "if params.device != \"cpu\":\n",
    "    network = nn.DataParallel(network).to(params.device, non_blocking=True)\n",
    "    # cudnn.benchmark = True\n",
    "\n",
    "# Watch the model with wandb for logging if enabled\n",
    "if params.wandb_log:\n",
    "    wandb.watch(network)\n",
    "\n",
    "# Initialize UNet weights if required\n",
    "if params.initialize_weights:\n",
    "    logger.info(\"Initializing UNet weights...\")\n",
    "    network.apply(weights_init)\n",
    "\n",
    "# The following line is commented as it does not work on Windows\n",
    "# torch.compile(network, mode=\"default\", backend=\"inductor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:31:24] [  INFO  ] [  __main__  ] < 24 > -- Output directory: C:\\Users\\dotti\\sparks_project\\models\\saved_models\\final_model\n"
     ]
    }
   ],
   "source": [
    "########################### Initialize training ############################\n",
    "\n",
    "# Initialize the optimizer based on the specified type\n",
    "if params.optimizer == \"adam\":\n",
    "    optimizer = optim.Adam(network.parameters(), lr=params.lr_start)\n",
    "elif params.optimizer == \"adadelta\":\n",
    "    optimizer = optim.Adadelta(network.parameters(), lr=params.lr_start)\n",
    "else:\n",
    "    logger.error(f\"{params.optimizer} is not a valid optimizer.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the learning rate scheduler if specified\n",
    "if params.scheduler == \"step\":\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=params.scheduler_step_size,\n",
    "        gamma=params.scheduler_gamma,\n",
    "    )\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "# Define the output directory path\n",
    "output_path = os.path.join(config.output_dir, params.run_name)\n",
    "logger.info(f\"Output directory: {os.path.realpath(output_path)}\")\n",
    "\n",
    "# Initialize the summary writer for TensorBoard logging\n",
    "summary_writer = SummaryWriter(os.path.join(\n",
    "    output_path, \"summary\"), purge_step=0)\n",
    "\n",
    "# Check if a pre-trained model should be loaded\n",
    "if params.load_run_name != \"\":\n",
    "    load_path = os.path.join(config.output_dir, params.load_run_name)\n",
    "    logger.info(f\"Model loaded from directory: {os.path.realpath(load_path)}\")\n",
    "else:\n",
    "    load_path = None\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = init_criterion(params=params, dataset=dataset)\n",
    "\n",
    "# Create a directory to save predicted class movies\n",
    "preds_output_dir = os.path.join(output_path, \"predictions\")\n",
    "os.makedirs(preds_output_dir, exist_ok=True)\n",
    "\n",
    "# Create a dictionary of managed objects\n",
    "managed_objects = {\"network\": network, \"optimizer\": optimizer}\n",
    "if scheduler is not None:\n",
    "    managed_objects[\"scheduler\"] = scheduler\n",
    "\n",
    "# Create a training manager with the specified training and testing functions\n",
    "trainer = MyTrainingManager(\n",
    "    # Training parameters\n",
    "    training_step=lambda _: training_step(\n",
    "        dataset_loader=dataset_loader,\n",
    "        params=params,\n",
    "        sampler=sampler,\n",
    "        network=network,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        scheduler=scheduler,\n",
    "        # scaler=GradScaler(),\n",
    "    ),\n",
    "    save_every=params.c.getint(\"training\", \"save_every\", fallback=5000),\n",
    "    load_path=load_path,\n",
    "    save_path=output_path,\n",
    "    managed_objects=unet.managed_objects(managed_objects),\n",
    "    # Testing parameters\n",
    "    test_function=lambda _: test_function(\n",
    "        network=network,\n",
    "        device=params.device,\n",
    "        criterion=criterion,\n",
    "        params=params,\n",
    "        testing_dataset=testing_dataset,\n",
    "        training_name=params.run_name,\n",
    "        output_dir=preds_output_dir,\n",
    "        training_mode=True,\n",
    "        debug=config.debug_mode,\n",
    "    ),\n",
    "    test_every=params.c.getint(\"training\", \"test_every\", fallback=1000),\n",
    "    plot_every=params.c.getint(\"training\", \"test_every\", fallback=1000),\n",
    "    summary_writer=summary_writer,\n",
    ")\n",
    "\n",
    "# Load the model if a specific epoch is provided\n",
    "if params.load_epoch != 0:\n",
    "    trainer.load(params.load_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Start training ##############################\n",
    "\n",
    "# Set the network in training mode\n",
    "network.train()\n",
    "\n",
    "\n",
    "# Resume the W&B run if needed (commented out for now)\n",
    "# if wandb.run.resumed:\n",
    "#     checkpoint = torch.load(wandb.restore(checkpoint_path))\n",
    "#     network.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     epoch = checkpoint['epoch']\n",
    "#     loss = checkpoint['loss']\n",
    "\n",
    "# Check if training is enabled in the configuration\n",
    "if params.c.getboolean(\"general\", \"training\", fallback=False):\n",
    "    # Validate the network before training if resuming from a checkpoint\n",
    "    if params.load_epoch > 0:\n",
    "        logger.info(\"Validate network before training\")\n",
    "        trainer.run_validation(wandb_log=params.wandb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:31:25] [  INFO  ] [  __main__  ] < 3  > -- Starting training\n",
      "[12:31:48] [WARNING ] [utils.training_inference_tools] <136 > -- loss: 0.7503355741500854, loss is tensor: True\n",
      "[12:31:48] [WARNING ] [utils.training_inference_tools] <139 > -- gradients zeroed\n",
      "[12:31:57] [WARNING ] [utils.training_inference_tools] <142 > -- loss backward\n",
      "[12:31:57] [WARNING ] [utils.training_inference_tools] <145 > -- optimizer step done\n",
      "[12:31:57] [WARNING ] [utils.training_inference_tools] <159 > -- End of training step\n",
      "[12:32:34] [  INFO  ] [utils.training_inference_tools] <1314> -- Iteration 0...\n",
      "[12:32:34] [  INFO  ] [utils.training_inference_tools] <1315> -- \tTraining loss: 0.7503\n",
      "[12:32:34] [  INFO  ] [utils.training_inference_tools] <1316> -- \tTime elapsed: 70.12s\n",
      "[12:32:46] [WARNING ] [utils.training_inference_tools] <136 > -- loss: 0.7502722144126892, loss is tensor: True\n",
      "[12:32:46] [WARNING ] [utils.training_inference_tools] <139 > -- gradients zeroed\n",
      "[12:32:46] [WARNING ] [utils.training_inference_tools] <142 > -- loss backward\n",
      "[12:32:46] [WARNING ] [utils.training_inference_tools] <145 > -- optimizer step done\n",
      "[12:32:46] [WARNING ] [utils.training_inference_tools] <159 > -- End of training step\n",
      "[12:34:46] [WARNING ] [utils.training_inference_tools] <136 > -- loss: 0.7502129077911377, loss is tensor: True\n",
      "[12:34:46] [WARNING ] [utils.training_inference_tools] <139 > -- gradients zeroed\n",
      "[12:34:46] [WARNING ] [utils.training_inference_tools] <142 > -- loss backward\n",
      "[12:34:46] [WARNING ] [utils.training_inference_tools] <145 > -- optimizer step done\n",
      "[12:34:46] [WARNING ] [utils.training_inference_tools] <159 > -- End of training step\n",
      "[12:36:32] [WARNING ] [utils.training_inference_tools] <136 > -- loss: 0.7501578330993652, loss is tensor: True\n",
      "[12:36:32] [WARNING ] [utils.training_inference_tools] <139 > -- gradients zeroed\n",
      "[12:36:32] [WARNING ] [utils.training_inference_tools] <142 > -- loss backward\n",
      "[12:36:32] [WARNING ] [utils.training_inference_tools] <145 > -- optimizer step done\n",
      "[12:36:32] [WARNING ] [utils.training_inference_tools] <159 > -- End of training step\n"
     ]
    }
   ],
   "source": [
    "# Check if training is enabled in the configuration\n",
    "if params.c.getboolean(\"general\", \"training\", fallback=False):\n",
    "    logger.info(\"Starting training\")\n",
    "    # Train the model for the specified number of epochs\n",
    "    trainer.train(\n",
    "        params.train_epochs,\n",
    "        print_every=params.c.getint(\"training\", \"print_every\", fallback=100),\n",
    "        wandb_log=params.wandb_log,\n",
    "    )\n",
    "\n",
    "# Check if final testing/validation is enabled in the configuration\n",
    "if params.c.getboolean(\"general\", \"testing\", fallback=False):\n",
    "    logger.info(\"Starting final validation\")\n",
    "    # Run the final validation/testing procedure\n",
    "    trainer.run_validation(wandb_log=params.wandb_log)\n",
    "\n",
    "# Close the summary writer\n",
    "summary_writer.close()\n",
    "\n",
    "# Close the wandb run\n",
    "if params.wandb_log:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging purposes\n",
    "# model_parameters = filter(lambda p: p.requires_grad, network.parameters())\n",
    "# model_parameters = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# logger.debug(f\"Number of trainable parameters: {model_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for load_epoch in [10000,20000,30000,40000,50000,60000,70000,80000,90000,100000]:\n",
    "# for load_epoch in [100000]:\n",
    "#     trainer.load(load_epoch)\n",
    "#     logger.info(\"Starting final validation\")\n",
    "#     trainer.run_validation(wandb_log=wandb_log)\n",
    "# if wandb_log:\n",
    "#     wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize UNet architecture (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get number of trainable parameters\n",
    "# num_params = sum(p.numel() for p in network.parameters() if p.requires_grad)\n",
    "# logger.debug(f\"Number of trainable parameters: {num_params}\")\n",
    "# # get dummy unet input\n",
    "# batch = next(iter(dataset_loader))\n",
    "# x = batch[0].to(device)\n",
    "# yhat = network(x[:,None]) # Give dummy batch to forward()\n",
    "# from torchviz import make_dot\n",
    "# make_dot(yhat, params=dict(list(network.named_parameters()))).render(\"unet_model\", format=\"png\")\n",
    "# a = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "# len(a[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d33eb8e81965b779f2871c6ab1ae98a760df4ff814358c9a5efa0a44482010f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
