{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import SparkDatasetPath\n",
    "from torch import nn\n",
    "from training_inference_tools import get_preds, myTrainingManager\n",
    "from data_processing_tools import get_processed_result, preds_dict_to_mask\n",
    "from in_out_tools import write_videos_on_disk\n",
    "from training_script_utils import init_model\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that are necessary to configure the dataset and the UNet model (can be eventually hard-coded in the function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "### Set parameters ###\n",
    "\n",
    "training_name = \"final_model\"\n",
    "config_file = os.path.join(\"config_files\", \"config_final_model.ini\")\n",
    "\n",
    "c = configparser.ConfigParser()\n",
    "c.read(config_file)\n",
    "    \n",
    "params = {}\n",
    "\n",
    "# training params\n",
    "\n",
    "params['load_epoch'] = c.getint(\"testing\", \"load_epoch\")\n",
    "# params['load_epoch'] = 100000\n",
    "params['batch_size'] = c.getint(\"testing\", \"batch_size\", fallback=\"1\")\n",
    "params[\"ignore_frames_loss\"] = c.getint(\n",
    "    \"training\", \"ignore_frames_loss\", fallback=0)\n",
    "\n",
    "# dataset params\n",
    "\n",
    "params[\"data_duration\"] = c.getint(\"dataset\", \"data_duration\")\n",
    "params[\"data_step\"] = c.getint(\"dataset\", \"data_step\", fallback=1)\n",
    "params[\"data_smoothing\"] = c.get(\"dataset\", \"data_smoothing\", fallback=\"2d\")\n",
    "params[\"norm_video\"] = c.get(\"dataset\", \"norm_video\", fallback=\"chunk\")\n",
    "params[\"remove_background\"] = c.get(\"dataset\", \"remove_background\", fallback=\"average\")\n",
    "params[\"only_sparks\"] = c.getboolean(\"dataset\", \"only_sparks\", fallback=False)\n",
    "params[\"sparks_type\"] = c.get(\"dataset\", \"sparks_type\", fallback=\"peaks\")\n",
    "\n",
    "# UNet params\n",
    "\n",
    "params[\"nn_architecture\"] = c.get(\"network\", \"nn_architecture\", fallback=\"pablos_unet\")\n",
    "if params[\"nn_architecture\"] == \"unet_lstm\":\n",
    "    params[\"bidirectional\"] = c.getboolean(\"network\", \"bidirectional\")\n",
    "params[\"unet_steps\"] = c.getint(\"network\", \"unet_steps\")\n",
    "params[\"first_layer_channels\"] = c.getint(\"network\", \"first_layer_channels\")\n",
    "params[\"num_channels\"] = c.getint(\"network\", \"num_channels\", fallback=1)\n",
    "params[\"dilation\"] = c.getboolean(\"network\", \"dilation\", fallback=1)\n",
    "params[\"border_mode\"] = c.get(\"network\", \"border_mode\")\n",
    "params[\"batch_normalization\"] = c.get(\"network\", \"batch_normalization\", fallback=\"none\")\n",
    "params[\"temporal_reduction\"] = c.getboolean(\"network\", \"temporal_reduction\", fallback=False)\n",
    "params[\"initialize_weights\"] = c.getboolean(\"network\", \"initialize_weights\", fallback=False)\n",
    "if params[\"nn_architecture\"] == \"github_unet\":\n",
    "    params[\"attention\"] = c.getboolean(\"network\", \"attention\")\n",
    "    params[\"up_mode\"] = c.get(\"network\", \"up_mode\")\n",
    "if params[\"nn_architecture\"] == \"openai_unet\":\n",
    "    params[\"num_res_blocks\"] = c.getint(\"network\", \"num_res_blocks\")\n",
    "\n",
    "assert params['nn_architecture'] in ['pablos_unet', 'github_unet', 'openai_unet'], \\\n",
    "    f\"nn_architecture must be one of 'pablos_unet', 'github_unet', 'openai_unet'\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configure UNet ###\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "network = init_model(\n",
    "    params=params,\n",
    "    num_classes=4,\n",
    "    ndims=3\n",
    "    )\n",
    "network = nn.DataParallel(network).to(device)\n",
    "network.eval()\n",
    "\n",
    "### Load UNet model ###\n",
    "models_relative_path = \"runs/\"\n",
    "model_path = os.path.join(models_relative_path, training_name)\n",
    "summary_writer = SummaryWriter(\n",
    "    os.path.join(model_path, \"summary\"), purge_step=0\n",
    "    )\n",
    "\n",
    "trainer = myTrainingManager(\n",
    "    # training items\n",
    "    training_step=None,\n",
    "    save_path=model_path,\n",
    "    managed_objects=unet.managed_objects({\"network\": network}),\n",
    "    summary_writer=summary_writer,\n",
    ")\n",
    "\n",
    "trainer.load(params['load_epoch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define movie path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_path = os.path.join(\n",
    "#     r\"C:\\Users\\dotti\\sparks_project\\data\\sparks_dataset\",\n",
    "#     \"34_video.tif\"\n",
    "# )\n",
    "movie_path = r\"C:\\Users\\dotti\\Desktop\\cropped 34_video.tif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_preds_from_path(\n",
    "    model,\n",
    "    params,\n",
    "    movie_path,\n",
    "    return_dict=False,\n",
    "    # output_dir=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to get predictions from a movie path\n",
    "    :param model: model to use for prediction\n",
    "    :param params: parameters for prediction\n",
    "    :param movie_path: path to movie\n",
    "    :param return_dict: if True, return dictionary else return tuple of numpy arrays\n",
    "    :param output_dir: if not None, save raw predictions on disk\n",
    "    :return: if return_dict is True, return dictionary with keys 'sparks',\n",
    "     'puffs', 'waves' else return tuple of numpy arrays with integral values\n",
    "     for classes and instances\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get sample as dataset ###\n",
    "    sample_dataset = SparkDatasetPath(\n",
    "        sample_path=movie_path,\n",
    "        step=params[\"data_step\"],\n",
    "        duration=params[\"data_duration\"],\n",
    "        smoothing=params[\"data_smoothing\"],\n",
    "        remove_background=params[\"remove_background\"],\n",
    "        temporal_reduction=params[\"temporal_reduction\"],\n",
    "        num_channels=params[\"num_channels\"],\n",
    "        normalize_video=params[\"norm_video\"],\n",
    "        only_sparks=params[\"only_sparks\"],\n",
    "        sparks_type=params[\"sparks_type\"],\n",
    "        ignore_index=4,\n",
    "        ignore_frames=params[\"ignore_frames_loss\"],\n",
    "        # resampling=False, # could be implemented later\n",
    "        # resampling_rate=150,\n",
    "    )\n",
    "\n",
    "    ### Set physiological parameters ###\n",
    "\n",
    "    # min distance in space between two sparks\n",
    "    min_dist_xy = sample_dataset.min_dist_xy # = 9 pixels\n",
    "    # min distance in time between two sparks\n",
    "    min_dist_t = sample_dataset.min_dist_t  # = 3 frames\n",
    "\n",
    "    # spark instances detection parameters\n",
    "    radius = math.ceil(min_dist_xy / 2)\n",
    "    y, x = np.ogrid[-radius: radius + 1, -radius: radius + 1]\n",
    "    disk = x**2 + y**2 <= radius**2\n",
    "    conn_mask = np.stack([disk] * (min_dist_t), axis=0)\n",
    "\n",
    "    # parameters for removing small events\n",
    "    # TODO: use better parameters !!!\n",
    "    spark_min_width = 3\n",
    "    spark_min_t = 3\n",
    "    puff_min_t = 5\n",
    "    wave_min_width = round(15 / sample_dataset.pixel_size)\n",
    "\n",
    "    # connectivity for event instances detection\n",
    "    connectivity = 26\n",
    "\n",
    "    # maximal gap between two predicted puffs or waves that belong together\n",
    "    max_gap = 2  # i.e., 2 empty frames\n",
    "\n",
    "    sigma = 3 # for gaussian smoothing\n",
    "\n",
    "    ### Run samples in UNet ###\n",
    "    xs, preds = get_preds(\n",
    "        network=model, \n",
    "        test_dataset=sample_dataset, \n",
    "        compute_loss=False, \n",
    "        device=next(model.parameters()).device, # same device as model\n",
    "        batch_size=1,\n",
    "        inference_types=None,\n",
    "    ) # ys and preds are numpy arrays\n",
    "    \n",
    "    ### Get processed output ###\n",
    "\n",
    "    # get predicted segmentation and event instances\n",
    "    preds_instances, preds_segmentation, _ = get_processed_result(\n",
    "        sparks=preds[1],\n",
    "        puffs=preds[3],\n",
    "        waves=preds[2],\n",
    "        xs=xs,\n",
    "        conn_mask=conn_mask,\n",
    "        connectivity=connectivity,\n",
    "        max_gap=max_gap,\n",
    "        sigma=sigma,\n",
    "        wave_min_width=wave_min_width,\n",
    "        puff_min_t=puff_min_t,\n",
    "        spark_min_t=spark_min_t,\n",
    "        spark_min_width=spark_min_width,\n",
    "        training_mode=False,\n",
    "        debug=False\n",
    "    )\n",
    "    # preds_instances and preds_segmentations are dictionaries \n",
    "    # with keys 'sparks', 'puffs', 'waves'\n",
    "\n",
    "    ### Save raw preds on disk ### I don't know if this is necessary\n",
    "    # if output_dir is not None:\n",
    "    # # create output directory if it does not exist\n",
    "    # os.makedirs(output_dir, exist_ok=True)\n",
    "    # write_videos_on_disk(\n",
    "    #     training_name=None,\n",
    "    #     video_name=sample_dataset.video_name,\n",
    "    #     path=output_dir,\n",
    "    #     preds=preds,\n",
    "    #     ys=None,\n",
    "    # ) \n",
    "\n",
    "    if return_dict:\n",
    "        return preds_segmentation, preds_instances\n",
    "    \n",
    "    else:\n",
    "        # get integral values for classes and instances\n",
    "        preds_segmentation = preds_dict_to_mask(preds_segmentation)\n",
    "        preds_instances = sum(preds_instances.values())\n",
    "        # instances already have different ids\n",
    "\n",
    "        return preds_segmentation, preds_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation, instances = get_preds_from_path(\n",
    "    model=network,\n",
    "    params=params,\n",
    "    movie_path=movie_path,\n",
    "    return_dict=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize preds with Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from visualization_tools import get_annotations_contour, get_discrete_cmap, get_labels_cmap\n",
    "import napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open original movie\n",
    "sample = np.asarray(imageio.volread(movie_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up napari parameters\n",
    "cmap = get_discrete_cmap(name='gray', lut=16)\n",
    "labels_cmap = get_labels_cmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize only border of classes (segmentation array)\n",
    "segmentation_border = get_annotations_contour(segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'instances' at 0x21662e24400>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(\n",
    "    sample,\n",
    "    name='input movie',\n",
    "    # colormap=('colors',cmap)\n",
    ")\n",
    "\n",
    "viewer.add_labels(\n",
    "    segmentation_border,\n",
    "    name='segmentation',\n",
    "    opacity=0.9,\n",
    "    color=labels_cmap,\n",
    ") # only visualize border\n",
    "\n",
    "viewer.add_labels(\n",
    "  segmentation,\n",
    "  name='segmentation',\n",
    "  opacity=0.5,\n",
    "  color=labels_cmap,\n",
    "  visible=False,\n",
    ") # to visualize whole roi instead\n",
    "\n",
    "viewer.add_labels(\n",
    "    instances,\n",
    "    name='instances',\n",
    "    opacity=0.5, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
