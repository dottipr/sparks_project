{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795642ee-d368-4bf8-8cbf-2a0f9e9fc61d",
   "metadata": {},
   "source": [
    "### Add colored sparks on video\n",
    "\n",
    "28.02.2022: questo codice viene da un vecchio file che avevo creato per aggiungere delle preds colorate sui video originali\n",
    "\n",
    "TODO: adattarlo al codice corrente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a30f08-f7ee-44a5-bb93-42b4567b6c4d",
   "metadata": {},
   "source": [
    "Add colored annotations and predictions (with transparency) on top of sample video\n",
    "\n",
    "Created the 29th November 2019\n",
    "\n",
    "For a given sample video needs:\n",
    "- sample video\n",
    "- mask with sparks annotations\n",
    "- mask with network predicitions for sparks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea84405-7f9b-417b-b8b1-a57fe34f6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82ab986-05c1-4bce-afad-28a352726650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import imageio\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from scipy import spatial, optimize\n",
    "\n",
    "from metrics_tools import nonmaxima_suppression, Metrics, correspondences_precision_recall, reduce_metrics\n",
    "#from save_predictions_as_videos import find_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b681a06-fcde-475a-a2f3-8e6f17c6d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paste_annotations_on_video(video, colored_mask):\n",
    "    # video is a RGB video, list of PIL images\n",
    "    # colored_mask is a RGBA video, list of PIL images\n",
    "    for frame,ann in zip(video, colored_mask):\n",
    "        frame.paste(ann, mask = ann.split()[3])\n",
    "\n",
    "def add_colored_annotations_to_video(annotations,video,color,transparency=50,radius=4):\n",
    "    # annotations is a list of t,x,y coordinates\n",
    "    # video is a RGB video, list of PIL images\n",
    "    # color is a list of 3 RGB elements\n",
    "    mask_shape = (len(video), video[0].size[1], video[0].size[0], 4)\n",
    "    colored_mask = np.zeros(mask_shape, dtype=np.uint8)\n",
    "    for pt in annotations:\n",
    "        colored_mask = color_ball(colored_mask,pt,radius,color,transparency)\n",
    "    colored_mask = [Image.fromarray(frame).convert('RGBA') for frame in colored_mask]\n",
    "\n",
    "    paste_annotations_on_video(video, colored_mask)\n",
    "    return video\n",
    "\n",
    "def l2_dist(p1,p2):\n",
    "    # p1 = (t1,y1,x1)\n",
    "    # p2 = (t2,y2,x2)\n",
    "    t1,y1,x1 = p1\n",
    "    t2,y2,x2 = p2\n",
    "    return math.sqrt(math.pow((t1-t2),2)+math.pow((y1-y2),2)+math.pow((x1-x2),2))\n",
    "\n",
    "def ball(c,r):\n",
    "    # r scalar\n",
    "    # c = (t,y,x)\n",
    "    # returns coordinates c' around c st dist(c,c') <= r\n",
    "    t,y,x = c\n",
    "    t_vect = np.linspace(t-r,t+r, 2*r+1, dtype = int)\n",
    "    y_vect = np.linspace(y-r,y+r, 2*r+1, dtype = int)\n",
    "    x_vect = np.linspace(x-r,x+r, 2*r+1, dtype = int)\n",
    "\n",
    "    cube_idxs = list(itertools.product(t_vect,y_vect,x_vect))\n",
    "    ball_idxs = [pt for pt in cube_idxs if l2_dist(c, pt) <= r]\n",
    "\n",
    "    return ball_idxs\n",
    "\n",
    "def color_ball(mask,c,r,color,transparency=50):\n",
    "    color_idx = ball(c,r)\n",
    "    # mask boundaries\n",
    "    duration, height, width, _ = np.shape(mask)\n",
    "\n",
    "    for t,y,x in color_idx:\n",
    "        if 0 <= t and t < duration and 0 <= y and y < height and 0 <= x and x < width:\n",
    "            mask[t,y,x] = [*color, transparency]\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b52ff-59e0-4cd6-a817-0ea04caf4a9f",
   "metadata": {},
   "source": [
    "### Load movies, annotations & training predictions (only sparks needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbdb01ca-b244-43c2-91de-038b0e35671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# physiological params\n",
    "PIXEL_SIZE = 0.2 # 1 pixel = 0.2 um x 0.2 um\n",
    "global MIN_DIST_XY\n",
    "MIN_DIST_XY = round(1.8 / PIXEL_SIZE) # min distance in space between sparks\n",
    "TIME_FRAME = 6.8 # 1 frame = 6.8 ms\n",
    "global MIN_DIST_T\n",
    "MIN_DIST_T = round(20 / TIME_FRAME) # min distance in time between sparks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9299ecf3-bd7e-4a24-87c4-7ea3855ec791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "ignore_frames = 6\n",
    "\n",
    "t_detection_sparks = 0.7\n",
    "min_radius_sparks = 2\n",
    "\n",
    "transparency = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9a79728-6ef7-4b12-a305-7a165a2cf5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_tools import load_movies_ids, load_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7db26e80-9a94-49f0-a563-c03b0543bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample training name\n",
    "training_name = \"focal_loss_gamma_5_ubelix\"\n",
    "epoch = 100000\n",
    "metrics_folder = os.path.join(\"trainings_validation\",training_name,\"colored_sparks\")\n",
    "\n",
    "os.makedirs(metrics_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84dd2ad0-9fb7-44e6-8c7c-012f93eae4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotations and sparks\n",
    "ys, sparks, puffs, _ = load_predictions(training_name, epoch, metrics_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0669cfca-8fc1-4afc-b49a-14fb55774851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('05', '10', '15', '20', '25', '32', '34', '40', '45')\n"
     ]
    }
   ],
   "source": [
    "# get movies names as tuple\n",
    "movies_ids = tuple(ys.keys())\n",
    "print(movies_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58fe1e41-addd-40d2-acdc-bd9c0c256542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n"
     ]
    }
   ],
   "source": [
    "# load movies\n",
    "data_folder = os.path.join(\"..\",\"data\",\"raw_data_and_processing\",\"original_movies\")\n",
    "xs = load_movies_ids(data_folder, movies_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfb76416-cf14-4ee5-a5e2-74ecb639d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics_tools import get_sparks_locations_from_mask, process_spark_prediction, correspondences_precision_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96714b41-17da-4fec-8177-f03d9eced87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to consider sum of puffs and sparks preds inside of puffs\n",
    "sum_sparks_puffs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c886c53-d186-4156-b02c-f27bed98531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if white_background, add colored sparks to white background\n",
    "# if not white_bacjground, add colored sparks to original movies\n",
    "white_background = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f065c58d-e182-45ab-8e64-3871cf294d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video 05 ...\n",
      "Processing video 10 ...\n",
      "Processing video 15 ...\n",
      "Processing video 20 ...\n",
      "Processing video 25 ...\n",
      "Processing video 32 ...\n",
      "Processing video 34 ...\n",
      "Processing video 40 ...\n",
      "Processing video 45 ...\n"
     ]
    }
   ],
   "source": [
    "for movie_id in movies_ids:\n",
    "    print(\"Processing video\", movie_id, \"...\")\n",
    "    \n",
    "    # normalize sample movie\n",
    "    sample_video = xs[movie_id]\n",
    "    \n",
    "    if white_background:\n",
    "        sample_video.fill(1) # the movie will be white\n",
    "    sample_video = 255*(sample_video/sample_video.max())\n",
    "    \n",
    "    # get annotated sparks locations\n",
    "    coords_true = get_sparks_locations_from_mask(ys[movie_id],\n",
    "                                                 min_dist_xy=MIN_DIST_XY,\n",
    "                                                 min_dist_t=MIN_DIST_T,)\n",
    "    \n",
    "    # get predicted sparks locations\n",
    "    spark_preds = sparks[movie_id]\n",
    "    if sum_sparks_puffs:\n",
    "        # set puff boundarier\n",
    "        t_puffs_lower = 0.3\n",
    "        t_puffs_upper = 0.65 # = t detection puffs\n",
    "        puff_preds = puffs[movie_id]\n",
    "        # compute region where 0.3 <= puffs <= 0.65\n",
    "        binary_puffs_sparks = np.logical_and(puff_preds <= t_puffs_upper,\n",
    "                                             puff_preds >= t_puffs_lower)\n",
    "        # sum value of sparks and puffs in this region\n",
    "        spark_preds = spark_preds + binary_puffs_sparks * puff_preds\n",
    "\n",
    "    coords_preds = process_spark_prediction(pred=spark_preds, \n",
    "                                            t_detection=t_detection_sparks,\n",
    "                                            min_dist_xy=MIN_DIST_XY,\n",
    "                                            min_dist_t=MIN_DIST_T,\n",
    "                                            min_radius=min_radius_sparks,\n",
    "                                           )\n",
    "    \n",
    "    # Compute correspondences between annotations and predictions\n",
    "    ''' OLD\n",
    "    distances = spatial.distance_matrix(coords_true, coords_preds)\n",
    "    distances[distances > match_distance] = 9999999\n",
    "    row_ind, col_ind = optimize.linear_sum_assignment(distances)\n",
    "\n",
    "    paired_true = [coords_true[i].tolist() for i,j in zip(row_ind,col_ind) if distances[i,j]<=match_distance]\n",
    "    paired_preds = [coords_preds[j].tolist() for i,j in zip(row_ind,col_ind) if distances[i,j]<=match_distance]\n",
    "    '''\n",
    "    \n",
    "    paired_true, paired_preds, false_positives, false_negatives = correspondences_precision_recall(coords_true,\n",
    "                                                                                                   coords_preds,\n",
    "                                                                                                   MIN_DIST_T,\n",
    "                                                                                                   MIN_DIST_XY,\n",
    "                                                                                                   return_pairs_coords=True\n",
    "                                                                                                  )\n",
    "\n",
    "    # Add colored annotations to video\n",
    "\n",
    "    rgb_video = [Image.fromarray(frame).convert('RGB') for frame in sample_video]\n",
    "\n",
    "    annotated_video = add_colored_annotations_to_video(paired_true, rgb_video, [0,255,0], 0.8*transparency)\n",
    "    annotated_video = add_colored_annotations_to_video(paired_preds, annotated_video, [0,255,200], 0.8*transparency)\n",
    "    annotated_video = add_colored_annotations_to_video(false_positives, annotated_video, [255,255,0], transparency)\n",
    "    annotated_video = add_colored_annotations_to_video(false_negatives, annotated_video, [255,0,0], transparency)\n",
    "\n",
    "    annotated_video = [np.array(frame) for frame in annotated_video]\n",
    "    \n",
    "    # set saved movies filenames\n",
    "    white_background_fn = \"_white_backgroud\" if white_background else \"\"\n",
    "    sum_sparks_fn = \"_sum_puffs\" if sum_sparks_puffs else \"\"\n",
    "    \n",
    "    imageio.volwrite(os.path.join(metrics_folder,\n",
    "                                  training_name+\"_\"+str(epoch)+\"_\"+movie_id+\"_colored_sparks\"+sum_sparks_fn+white_background_fn+\".tif\"),\n",
    "                     annotated_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ba4e30-838a-48ba-ae9f-53c5c1ff97e3",
   "metadata": {},
   "source": [
    "### Write all script parameters to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58a07762-446b-42e6-9d0c-c18244f2e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba57f0a2-7d4b-4e39-a850-c8c9e7095f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(metrics_folder,\"parameters.txt\")\n",
    "\n",
    "with open(file_path, 'w') as f:\n",
    "    f.write(f\"{datetime.datetime.now()}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Phyisiological parameters\\n\")\n",
    "    f.write(f\"Pixel size: {PIXEL_SIZE} um\\n\")\n",
    "    f.write(f\"Min distance (x,y): {MIN_DIST_XY} pixels\\n\")\n",
    "    f.write(f\"Time frame: {TIME_FRAME} ms\\n\")\n",
    "    f.write(f\"Min distance t: {MIN_DIST_T} pixels\\n\\n\")\n",
    "    \n",
    "    f.write(\"Training parameters\\n\")\n",
    "    f.write(f\"Training name: {training_name}\\n\")\n",
    "    f.write(f\"Loaded epoch: {epoch}\\n\")\n",
    "    f.write(f\"Dataset folder: {data_folder}\\n\")\n",
    "    f.write(f\"Movies analysed for coloured sparks: {movies_ids}\\n\")\n",
    "    f.write(f\"Num frames ignored by loss function: {ignore_frames}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Sparks detection parameters\\n\")\n",
    "    f.write(f\"Min threshold for sparks detection: {t_detection_sparks}\\n\")\n",
    "    f.write(f\"Min radius of valid spark predictions: {min_radius_sparks}\\n\")\n",
    "    f.write(f\"Using puffs values for sparks detection: {sum_sparks_puffs}\\n\")\n",
    "    if sum_sparks_puffs:\n",
    "        f.write(f\"Min puffs' threshold for sparks over puffs detection: {t_puffs_lower}\\n\")\n",
    "        f.write(f\"Max puffs' threshold for sparks over puffs detection: {t_puffs_upper}\\n\\n\")\n",
    "        \n",
    "    f.write(\"Coloured sparks parameters\\n\")\n",
    "    f.write(f\"Saved coloured sparks path: {metrics_folder}\\n\")\n",
    "    f.write(f\"Coloured sparks' transparency: {transparency}\\n\")\n",
    "    f.write(f\"Using white background instead of original movies: {white_background}\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb35be4f-48ea-4e40-9731-a4f165e84dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
