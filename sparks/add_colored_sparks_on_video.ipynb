{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628be7f8-32a6-4b00-8d25-b0113b344742",
   "metadata": {},
   "source": [
    "### Add colored sparks on video\n",
    "\n",
    "Add colored annotations and predictions (with transparency) on top of sample video\n",
    "\n",
    "Created the 29th November 2019\n",
    "\n",
    "For a given sample video needs:\n",
    "- sample video\n",
    "- mask with sparks annotations\n",
    "- mask with network predicitions for sparks\n",
    "\n",
    "UPDATES:\n",
    "- 28/03/2022 Now using most recent annotations used for training (independently from training name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea84405-7f9b-417b-b8b1-a57fe34f6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662b9d29-20c1-46fd-ae81-30c1fff4ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import imageio\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "from scipy import spatial, optimize\n",
    "\n",
    "from metrics_tools import correspondences_precision_recall, get_sparks_locations_from_mask, process_spark_prediction\n",
    "from dataset_tools import load_movies_ids, load_predictions, load_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b681a06-fcde-475a-a2f3-8e6f17c6d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paste_annotations_on_video(video, colored_mask):\n",
    "    # video is a RGB video, list of PIL images\n",
    "    # colored_mask is a RGBA video, list of PIL images\n",
    "    for frame,ann in zip(video, colored_mask):\n",
    "        frame.paste(ann, mask = ann.split()[3])\n",
    "\n",
    "def add_colored_annotations_to_video(annotations,video,color,transparency=50,radius=4):\n",
    "    # annotations is a list of t,x,y coordinates\n",
    "    # video is a RGB video, list of PIL images\n",
    "    # color is a list of 3 RGB elements\n",
    "    mask_shape = (len(video), video[0].size[1], video[0].size[0], 4)\n",
    "    colored_mask = np.zeros(mask_shape, dtype=np.uint8)\n",
    "    for pt in annotations:\n",
    "        colored_mask = color_ball(colored_mask,pt,radius,color,transparency)\n",
    "    colored_mask = [Image.fromarray(frame).convert('RGBA') for frame in colored_mask]\n",
    "\n",
    "    paste_annotations_on_video(video, colored_mask)\n",
    "    return video\n",
    "\n",
    "def l2_dist(p1,p2):\n",
    "    # p1 = (t1,y1,x1)\n",
    "    # p2 = (t2,y2,x2)\n",
    "    t1,y1,x1 = p1\n",
    "    t2,y2,x2 = p2\n",
    "    return math.sqrt(math.pow((t1-t2),2)+math.pow((y1-y2),2)+math.pow((x1-x2),2))\n",
    "\n",
    "def ball(c,r):\n",
    "    # r scalar\n",
    "    # c = (t,y,x)\n",
    "    # returns coordinates c' around c st dist(c,c') <= r\n",
    "    t,y,x = c\n",
    "    t_vect = np.linspace(t-r,t+r, 2*r+1, dtype = int)\n",
    "    y_vect = np.linspace(y-r,y+r, 2*r+1, dtype = int)\n",
    "    x_vect = np.linspace(x-r,x+r, 2*r+1, dtype = int)\n",
    "\n",
    "    cube_idxs = list(itertools.product(t_vect,y_vect,x_vect))\n",
    "    ball_idxs = [pt for pt in cube_idxs if l2_dist(c, pt) <= r]\n",
    "\n",
    "    return ball_idxs\n",
    "\n",
    "def color_ball(mask,c,r,color,transparency=50):\n",
    "    color_idx = ball(c,r)\n",
    "    # mask boundaries\n",
    "    duration, height, width, _ = np.shape(mask)\n",
    "\n",
    "    for t,y,x in color_idx:\n",
    "        if 0 <= t and t < duration and 0 <= y and y < height and 0 <= x and x < width:\n",
    "            mask[t,y,x] = [*color, transparency]\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b52ff-59e0-4cd6-a817-0ea04caf4a9f",
   "metadata": {},
   "source": [
    "### Load movies, annotations & training predictions (only sparks needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c72c85ed-ee82-4a65-93ae-3596b5094508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_tools import load_annotations_ids,get_new_mask\n",
    "from metrics_tools import correspondences_precision_recall\n",
    "\n",
    "# select if using saved annotations or locations on raw annotations generated on-the-fly\n",
    "saved_annotations = False\n",
    "\n",
    "# physiological params\n",
    "PIXEL_SIZE = 0.2 # 1 pixel = 0.2 um x 0.2 um\n",
    "global MIN_DIST_XY\n",
    "MIN_DIST_XY = round(1.8 / PIXEL_SIZE) # min distance in space between sparks\n",
    "TIME_FRAME = 6.8 # 1 frame = 6.8 ms\n",
    "global MIN_DIST_T\n",
    "MIN_DIST_T = round(20 / TIME_FRAME) # min distance in time between sparks\n",
    "\n",
    "# parameters\n",
    "ignore_frames = 6\n",
    "\n",
    "t_detection_sparks = 0.65\n",
    "min_radius_sparks = 1\n",
    "\n",
    "transparency = 45\n",
    "\n",
    "sigma = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08a0d298-d6e4-414c-8ac4-c0cecfa79192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('05', '10', '15', '20', '25', '32', '34', '40', '45')\n"
     ]
    }
   ],
   "source": [
    "# sample training name\n",
    "#training_name = \"peak_sparks_lovasz_physio\"\n",
    "#epoch = 150000\n",
    "training_name = \"focal_loss_gamma_5_ubelix\"\n",
    "epoch = 100000\n",
    "data_folder = os.path.join(\"trainings_validation\", training_name)\n",
    "output_folder = os.path.join(\"trainings_validation\",training_name,\"colored_sparks\")\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# load annotations and sparks\n",
    "_, sparks, puffs, _ = load_predictions(training_name, epoch, data_folder)\n",
    "\n",
    "# get movies names as tuple\n",
    "movies_ids = tuple(sparks.keys())\n",
    "print(movies_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d72b62f-ee77-49c8-ac87-a7d886449cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n"
     ]
    }
   ],
   "source": [
    "# load movies\n",
    "data_folder = os.path.join(\"..\",\"data\",\"raw_data_and_processing\",\"original_movies\")\n",
    "xs = load_movies_ids(data_folder, movies_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab9aa2e6-014c-4410-aeeb-d4095673da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#movies_ids = ('05',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f9dea21-ca88-432c-8e97-f96859244dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to consider sum of puffs and sparks preds inside of puffs\n",
    "sum_sparks_puffs = False\n",
    "\n",
    "# if white_background, add colored sparks to white background\n",
    "# if not white_bacjground, add colored sparks to original movies\n",
    "white_background = False\n",
    "\n",
    "# events paramenters\n",
    "radius_event = 3\n",
    "radius_ignore = 1\n",
    "#radius_ignore = 3\n",
    "ignore_index = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc0f3112-9894-4022-8385-8c4cb4002fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using peaks generated on the fly from raw annotations\n",
      "Closest coordinates: \n",
      "[[355.  45. 163.]\n",
      " [352.  52. 156.]]\n",
      "\t\tNum of sparks: 75\n",
      "Closest coordinates: \n",
      "[[ 38.  10. 368.]\n",
      " [ 34.  10. 368.]]\n",
      "\t\tNum of sparks: 22\n",
      "Closest coordinates: \n",
      "[[391.  41. 355.]\n",
      " [387.  41. 355.]]\n",
      "\t\tNum of sparks: 49\n",
      "Closest coordinates: \n",
      "[[206.   8. 433.]\n",
      " [204.  17. 438.]]\n",
      "\t\tNum of sparks: 71\n",
      "Closest coordinates: \n",
      "[[363.  42. 130.]\n",
      " [357.  42. 117.]]\n",
      "\t\tNum of sparks: 32\n",
      "Closest coordinates: \n",
      "[[796.  39. 243.]\n",
      " [786.  41. 244.]]\n",
      "\t\tNum of sparks: 8\n",
      "Closest coordinates: \n",
      "[[895.  42. 330.]\n",
      " [893.  44. 340.]]\n",
      "\t\tNum of sparks: 21\n",
      "Closest coordinates: \n",
      "[[474.  37. 294.]\n",
      " [473.   5. 318.]]\n",
      "\t\tNum of sparks: 6\n"
     ]
    }
   ],
   "source": [
    "if saved_annotations:\n",
    "    # load most recent annotation masks\n",
    "    print(\"Loading most recent annotations used for training\")\n",
    "    recent_annotations_path = os.path.join(\"..\", \"data\", \"sparks_dataset\", \"videos_test\")\n",
    "    ys = load_annotations(data_folder=recent_annotations_path)\n",
    "else:\n",
    "    print(\"Using peaks generated on the fly from raw annotations\")\n",
    "    raw_annotations_path = os.path.join(\"..\",\"data\",\"raw_data_and_processing\",\"original_masks\")\n",
    "    raw_ys = load_annotations_ids(data_folder=raw_annotations_path, ids=movies_ids, mask_names=\"mask\")\n",
    "    \n",
    "    # detect peaks using most current version of nonmaxima_suppression\n",
    "    ys = {movie_id: get_new_mask(video=xs[movie_id],\n",
    "                                 mask=raw_ys[movie_id],\n",
    "                                 min_dist_xy=MIN_DIST_XY, min_dist_t=MIN_DIST_T,\n",
    "                                 radius_event=radius_event, radius_ignore=radius_ignore,\n",
    "                                 ignore_index=ignore_index, sigma=sigma\n",
    "                                ) for movie_id in movies_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f065c58d-e182-45ab-8e64-3871cf294d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video 05 ...\n",
      "Get annotated sparks locations\n",
      "Closest coordinates: \n",
      "[[355.  45. 163.]\n",
      " [352.  52. 156.]]\n",
      "Get predicted sparks locations\n",
      "Closest coordinates: \n",
      "[[  5.  44. 314.]\n",
      " [  4.  33. 318.]]\n",
      "Processing video 10 ...\n",
      "Get annotated sparks locations\n",
      "Closest coordinates: \n",
      "[[ 38.  10. 368.]\n",
      " [ 34.  10. 368.]]\n",
      "Get predicted sparks locations\n",
      "Closest coordinates: \n",
      "[[  3.  49. 413.]\n",
      " [  3.  49. 405.]]\n",
      "Processing video 15 ...\n",
      "Get annotated sparks locations\n",
      "Closest coordinates: \n",
      "[[391.  41. 355.]\n",
      " [387.  41. 355.]]\n",
      "Get predicted sparks locations\n",
      "Closest coordinates: \n",
      "[[  3.  44. 206.]\n",
      " [  3.  36. 214.]]\n",
      "Processing video 20 ...\n",
      "Get annotated sparks locations\n",
      "Closest coordinates: \n",
      "[[206.   8. 433.]\n",
      " [204.  17. 438.]]\n",
      "Get predicted sparks locations\n",
      "Closest coordinates: \n",
      "[[276.  48. 391.]\n",
      " [273.  48. 391.]]\n",
      "Processing video 25 ...\n",
      "Get annotated sparks locations\n",
      "Closest coordinates: \n",
      "[[363.  42. 130.]\n",
      " [357.  42. 117.]]\n",
      "Get predicted sparks locations\n",
      "Closest coordinates: \n",
      "[[628.  40. 247.]\n",
      " [626.  46. 248.]]\n",
      "Processing video 32 ...\n",
      "Get annotated sparks locations\n",
      "Closest coordinates: \n",
      "[[796.  39. 243.]\n",
      " [786.  41. 244.]]\n",
      "Get predicted sparks locations\n",
      "Closest coordinates: \n",
      "[[595.  20. 359.]\n",
      " [591.  14. 370.]]\n",
      "Processing video 34 ...\n",
      "Get annotated sparks locations\n",
      "Closest coordinates: \n",
      "[[895.  42. 330.]\n",
      " [893.  44. 340.]]\n",
      "Get predicted sparks locations\n",
      "Closest coordinates: \n",
      "[[477.  28. 244.]\n",
      " [476.  42. 241.]]\n",
      "Processing video 40 ...\n",
      "Get annotated sparks locations\n",
      "Closest coordinates: \n",
      "[[474.  37. 294.]\n",
      " [473.   5. 318.]]\n",
      "Get predicted sparks locations\n",
      "Closest coordinates: \n",
      "[[914.  13. 128.]\n",
      " [914.   2. 131.]]\n",
      "Processing video 45 ...\n",
      "Get annotated sparks locations\n",
      "Get predicted sparks locations\n",
      "Closest coordinates: \n",
      "[[567.  51. 262.]\n",
      " [565.  42. 263.]]\n"
     ]
    }
   ],
   "source": [
    "from metrics_tools import correspondences_precision_recall\n",
    "from dataset_tools import get_new_mask\n",
    "\n",
    "for movie_id in movies_ids:\n",
    "    print(\"Processing video\", movie_id, \"...\")\n",
    "    \n",
    "    # normalize sample movie\n",
    "    sample_video = xs[movie_id]\n",
    "    sample_video = 255*(sample_video/sample_video.max())\n",
    "    \n",
    "    # get movie duration\n",
    "    movie_duration = sample_video.shape[0]\n",
    "    \n",
    "    # get annotated sparks locations\n",
    "    print(\"Get annotated sparks locations\")\n",
    "    spark_true = ys[movie_id]\n",
    "    coords_true = get_sparks_locations_from_mask(mask=spark_true,\n",
    "                                                 min_dist_xy=MIN_DIST_XY,\n",
    "                                                 min_dist_t=MIN_DIST_T,\n",
    "                                                 ignore_frames=ignore_frames\n",
    "                                                )\n",
    "        \n",
    "    # get predicted sparks locations\n",
    "    print(\"Get predicted sparks locations\")\n",
    "    spark_preds = sparks[movie_id]\n",
    "    if sum_sparks_puffs:\n",
    "        # set puff boundarier\n",
    "        t_puffs_lower = 0.3\n",
    "        t_puffs_upper = 0.65 # = t detection puffs\n",
    "        puff_preds = puffs[movie_id]\n",
    "        # compute region where 0.3 <= puffs <= 0.65\n",
    "        binary_puffs_sparks = np.logical_and(puff_preds <= t_puffs_upper,\n",
    "                                             puff_preds >= t_puffs_lower)\n",
    "        # sum value of sparks and puffs in this region\n",
    "        spark_preds = spark_preds + binary_puffs_sparks * puff_preds\n",
    "\n",
    "    coords_preds = process_spark_prediction(pred=spark_preds, \n",
    "                                            movie=sample_video,\n",
    "                                            t_detection=t_detection_sparks,\n",
    "                                            min_dist_xy=MIN_DIST_XY,\n",
    "                                            min_dist_t=MIN_DIST_T,\n",
    "                                            min_radius=min_radius_sparks,\n",
    "                                            ignore_frames=ignore_frames,\n",
    "                                            sigma=sigma\n",
    "                                           )\n",
    "\n",
    "    # Compute correspondences between annotations and predictions\n",
    "    ''' OLD\n",
    "    distances = spatial.distance_matrix(coords_true, coords_preds)\n",
    "    distances[distances > match_distance] = 9999999\n",
    "    row_ind, col_ind = optimize.linear_sum_assignment(distances)\n",
    "\n",
    "    paired_true = [coords_true[i].tolist() for i,j in zip(row_ind,col_ind) if distances[i,j]<=match_distance]\n",
    "    paired_preds = [coords_preds[j].tolist() for i,j in zip(row_ind,col_ind) if distances[i,j]<=match_distance]\n",
    "    '''\n",
    "    \n",
    "    paired_true, paired_preds, false_positives, false_negatives = correspondences_precision_recall(coords_true,\n",
    "                                                                                                   coords_preds,\n",
    "                                                                                                   MIN_DIST_T,\n",
    "                                                                                                   MIN_DIST_XY,\n",
    "                                                                                                   return_pairs_coords=True\n",
    "                                                                                                  )\n",
    "    \n",
    "    # Write sparks locations to file\n",
    "    file_path = os.path.join(output_folder,f\"{movie_id}_sparks_location.txt\")\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(f\"{datetime.datetime.now()}\\n\\n\")\n",
    "        f.write(f\"Paired annotations and preds:\\n\")\n",
    "        for p_true, p_preds in zip(paired_true, paired_preds):\n",
    "            f.write(f\"{list(map(int, p_true))} {list(map(int, p_preds))}\\n\")\n",
    "        f.write(f\"\\n\")\n",
    "        f.write(f\"Unpaired preds (false positives):\\n\")\n",
    "        for f_p in false_positives:\n",
    "            f.write(f\"{list(map(int, f_p))}\\n\")\n",
    "        f.write(f\"\\n\")\n",
    "        f.write(f\"Unpaired annotations (false negatives):\\n\")\n",
    "        for f_n in false_negatives:\n",
    "            f.write(f\"{list(map(int, f_n))}\\n\")\n",
    "\n",
    "    # Add colored annotations to video\n",
    "    \n",
    "    if white_background:\n",
    "        sample_video.fill(255) # the movie will be white\n",
    "\n",
    "    rgb_video = [Image.fromarray(frame).convert('RGB') for frame in sample_video]\n",
    "\n",
    "    annotated_video = add_colored_annotations_to_video(paired_true, rgb_video, [0,255,0], 0.8*transparency)\n",
    "    annotated_video = add_colored_annotations_to_video(paired_preds, annotated_video, [0,255,200], 0.8*transparency)\n",
    "    annotated_video = add_colored_annotations_to_video(false_positives, annotated_video, [255,255,0], transparency)\n",
    "    annotated_video = add_colored_annotations_to_video(false_negatives, annotated_video, [255,0,0], transparency)\n",
    "\n",
    "    annotated_video = [np.array(frame) for frame in annotated_video]\n",
    "    \n",
    "    # set saved movies filenames\n",
    "    white_background_fn = \"_white_backgroud\" if white_background else \"\"\n",
    "    sum_sparks_fn = \"_sum_puffs\" if sum_sparks_puffs else \"\"\n",
    "    \n",
    "    imageio.volwrite(os.path.join(output_folder,\n",
    "                                  training_name+\"_\"+str(epoch)+\"_\"+movie_id+\"_colored_sparks\"+sum_sparks_fn+white_background_fn+\".tif\"),\n",
    "                     annotated_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ba4e30-838a-48ba-ae9f-53c5c1ff97e3",
   "metadata": {},
   "source": [
    "### Write all script parameters to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57f0a2-7d4b-4e39-a850-c8c9e7095f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(output_folder,f\"parameters{white_background_fn}.txt\")\n",
    "\n",
    "with open(file_path, 'w') as f:\n",
    "    f.write(f\"{datetime.datetime.now()}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Phyisiological parameters\\n\")\n",
    "    f.write(f\"Pixel size: {PIXEL_SIZE} um\\n\")\n",
    "    f.write(f\"Min distance (x,y): {MIN_DIST_XY} pixels\\n\")\n",
    "    f.write(f\"Time frame: {TIME_FRAME} ms\\n\")\n",
    "    f.write(f\"Min distance t: {MIN_DIST_T} pixels\\n\\n\")\n",
    "    \n",
    "    f.write(\"Training parameters\\n\")\n",
    "    f.write(f\"Training name: {training_name}\\n\")\n",
    "    f.write(f\"Loaded epoch: {epoch}\\n\")\n",
    "    f.write(f\"Dataset folder: {data_folder}\\n\")\n",
    "    f.write(f\"Annotations folder: {recent_annotations_path}\\n\")\n",
    "    f.write(f\"Movies analysed for coloured sparks: {movies_ids}\\n\")\n",
    "    f.write(f\"Num frames ignored by loss function: {ignore_frames}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Sparks detection parameters\\n\")\n",
    "    f.write(f\"Min threshold for sparks detection: {t_detection_sparks}\\n\")\n",
    "    f.write(f\"Min radius of valid spark predictions: {min_radius_sparks}\\n\")\n",
    "    f.write(f\"Using puffs values for sparks detection: {sum_sparks_puffs}\\n\")\n",
    "    if sum_sparks_puffs:\n",
    "        f.write(f\"Min puffs' threshold for sparks over puffs detection: {t_puffs_lower}\\n\")\n",
    "        f.write(f\"Max puffs' threshold for sparks over puffs detection: {t_puffs_upper}\\n\\n\")\n",
    "        \n",
    "    f.write(\"Coloured sparks parameters\\n\")\n",
    "    f.write(f\"Saved coloured sparks path: {output_folder}\\n\")\n",
    "    f.write(f\"Coloured sparks' transparency: {transparency}\\n\")\n",
    "    f.write(f\"Using white background instead of original movies: {white_background}\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3dd910-6ee0-4e36-8f4c-6933cb318572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f708b25-47c2-4dc2-972d-c3e04a0fbb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
