{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b70ded7-5a6f-45af-bd11-e22e9d8449a1",
   "metadata": {},
   "source": [
    "# Generate segmentation masks from predictions\n",
    "\n",
    "7.02.2022\n",
    "\n",
    "Uso questo script per trasformare i raw outputs della unet in un unico video che contiene la segmentazione dell'input (cioè con valori compresi in {0,1,2,3}).\n",
    "\n",
    "L'idea è poi di automatizzare la generazione di questi output \"binari\" per tutti i video di un certo modello (training) in uno script .py e di eliminare questo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d1a639c-ec6f-44bd-86eb-119f1edb2ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6495095-5423-400f-97a8-581e97f6763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import imageio\n",
    "\n",
    "from scipy.ndimage.morphology import binary_dilation, binary_erosion\n",
    "from sklearn.metrics import jaccard_score, f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import unet\n",
    "from metrics_tools import (correspondences_precision_recall, \n",
    "                           Metrics, \n",
    "                           reduce_metrics, \n",
    "                           empty_marginal_frames,\n",
    "                           process_spark_prediction,\n",
    "                           process_puff_prediction,\n",
    "                           process_wave_prediction,\n",
    "                           jaccard_score_exclusion_zone,\n",
    "                           write_videos_on_disk,\n",
    "                           get_sparks_locations_from_mask,\n",
    "                           compute_prec_rec,\n",
    "                           reduce_metrics_thresholds\n",
    "                          )\n",
    "from dataset_tools import final_mask\n",
    "#from generate_unet_annotations import final_mask # TODO: uncomment and fix generate_unet_annotations with main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "598cadb5-068f-4b57-855b-388035d7e074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dotti\\\\sparks_project\\\\sparks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASEDIR = os.path.abspath('')\n",
    "BASEDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc53bf9-d8d0-4717-a3fb-0d44fe636ab3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select predictions to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c9494c8-3a8e-46ad-9d42-20b373608c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_names = [#\"temporal_reduction_ubelix\",\n",
    "                  #\"256_long_chunks_ubelix\",\n",
    "                  #\"focal_loss_ubelix\",\n",
    "                  #\"pretrained_only_sparks_ubelix\",\n",
    "                  #\"only_sparks_ubelix\",\n",
    "                  \"focal_loss_gamma_5_ubelix\"\n",
    "                  ]\n",
    "\n",
    "epoch = 100000\n",
    "\n",
    "# if a training has a different load epoch\n",
    "epoch_2 = 200000 \n",
    "epoch_2_training_name = \"pretrained_only_sparks_ubelix\"\n",
    "\n",
    "# if loading a training that uses temporal reduction, predictions have to be adapted\n",
    "temp_red_training_name = \"temporal_reduction_ubelix\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65ff227-e593-44a5-b615-1367b1f14650",
   "metadata": {},
   "source": [
    "### Configure input/output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6219e8e8-a840-4e55-9806-147ebe7748f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_folder = \"trainings_validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339af7ac-aff3-4b96-aeee-9f7945767a31",
   "metadata": {},
   "source": [
    "### Load annotations\n",
    "open and process annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d026c21d-023d-4bda-b509-2c1788db9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_all_trainings = {} # indexed by video number\n",
    "\n",
    "ys_filenames = sorted(glob.glob(os.path.join(validation_folder, \"[0-9]*_video_mask.tif\")))\n",
    "\n",
    "for f in ys_filenames:\n",
    "    video_id = os.path.split(f)[1][:2]\n",
    "    ys_all_trainings[video_id] = np.asarray(imageio.volread(f)).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8cda152-c0c7-4158-b58b-82628622771b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['05', '10', '15', '20', '25', '32', '34', '40', '45'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_all_trainings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3113139a-8f3f-41c2-b2e2-fca8e3d31e99",
   "metadata": {},
   "source": [
    "### Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cac816e-a119-4b11-beff-eb070cd9d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = {} # contains annotations for each training: num trainings (dict) x num videos (dict) x video shape\n",
    "sparks = {} # contains sparks for each training: num trainings x num videos x video shape\n",
    "puffs = {} # contains puffs for each training: num trainings x num videos x video shape\n",
    "waves = {} # contains waves for each training: num trainings x num videos x video shape\n",
    "\n",
    "for training_name in training_names:\n",
    "    # Import .tif files as numpy array\n",
    "    load_epoch = epoch_2 if training_name == epoch_2_training_name else epoch\n",
    "    base_name = os.path.join(validation_folder,training_name+\"_\"+str(load_epoch)+\"_\")\n",
    "    \n",
    "    if training_name == temp_red_training_name: # need to use annotations from another training\n",
    "        if training_names[-1] != temp_red_training_name:\n",
    "            base_name_ys = os.path.join(validation_folder,training_names[-1]+\"_\"+str(load_epoch)+\"_\")\n",
    "            ys_filenames = sorted(glob.glob(base_name_ys+\"[0-9][0-9]_video_ys.tif\"))\n",
    "        else:\n",
    "            print(\"SELECT TRAINING NAME DIFFERENT FROM temporal_reduction\")\n",
    "            break\n",
    "    else:    \n",
    "        ys_filenames = sorted(glob.glob(base_name+\"[0-9][0-9]_video_ys.tif\"))\n",
    "        \n",
    "    sparks_filenames = sorted(glob.glob(base_name+\"[0-9][0-9]_video_sparks.tif\"))\n",
    "    puffs_filenames = sorted(glob.glob(base_name+\"[0-9][0-9]_video_puffs.tif\"))\n",
    "    waves_filenames = sorted(glob.glob(base_name+\"[0-9][0-9]_video_waves.tif\"))\n",
    "        \n",
    "    training_ys = {}\n",
    "    training_sparks = {}\n",
    "    training_puffs = {}\n",
    "    training_waves = {}\n",
    "    \n",
    "    for idx,y,s,p,w in zip(ys_all_trainings.keys(),ys_filenames,sparks_filenames,puffs_filenames,waves_filenames):\n",
    "        ys_loaded = np.asarray(imageio.volread(y)).astype('int')\n",
    "        training_ys[idx] = ys_loaded\n",
    "        \n",
    "        if training_name == temp_red_training_name: # repeat each frame 4 times\n",
    "            s_preds = np.asarray(imageio.volread(s))\n",
    "            p_preds = np.asarray(imageio.volread(p))\n",
    "            w_preds = np.asarray(imageio.volread(w))\n",
    "                        \n",
    "            # repeat predicted frames x4\n",
    "            s_preds = np.repeat(s_preds,4,0)\n",
    "            p_preds = np.repeat(p_preds,4,0)\n",
    "            w_preds = np.repeat(w_preds,4,0)\n",
    "            \n",
    "            # if original length %4 != 0, crop preds\n",
    "            if ys_loaded.shape != s_preds.shape:\n",
    "                duration = ys_loaded.shape[0]\n",
    "                s_preds = s_preds[:duration]\n",
    "                p_preds = p_preds[:duration]\n",
    "                w_preds = w_preds[:duration]\n",
    "            \n",
    "            assert ys_loaded.shape == s_preds.shape\n",
    "            assert ys_loaded.shape == p_preds.shape\n",
    "            assert ys_loaded.shape == w_preds.shape\n",
    "            \n",
    "            training_sparks[idx] = s_preds\n",
    "            training_puffs[idx] = p_preds\n",
    "            training_waves[idx] = w_preds\n",
    "        else:\n",
    "            training_sparks[idx] = np.asarray(imageio.volread(s))\n",
    "            training_puffs[idx] = np.asarray(imageio.volread(p))\n",
    "            training_waves[idx] = np.asarray(imageio.volread(w))\n",
    "\n",
    "    ys[training_name] = training_ys\n",
    "    sparks[training_name] = training_sparks\n",
    "    puffs[training_name] = training_puffs\n",
    "    waves[training_name] = training_waves        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda0405-7923-4eeb-8282-2ab5943ae9c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Select sample training and sample video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4a3c56e-9e62-4957-ad60-28252eabff48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05 10 15 20 25 32 34 40 45\n"
     ]
    }
   ],
   "source": [
    "# all video indices:\n",
    "print(*list(ys_all_trainings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49b48106-e783-4f6f-a6cf-ac3f16656414",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_name = training_names[0]\n",
    "video_id = '34'\n",
    "\n",
    "sample_ys = ys[training_name][video_id]\n",
    "sample_sparks = sparks[training_name][video_id]\n",
    "sample_puffs = puffs[training_name][video_id]\n",
    "sample_waves = waves[training_name][video_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848ce6e-e6e6-4417-997e-b4a8bf89f61e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Generate sample binary prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff3fa81-86b2-41c0-8064-1cfd3a527331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general params\n",
    "ignore_frames_loss = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43750de-6999-47d3-ba78-a1f9aa983300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# physiological params\n",
    "PIXEL_SIZE = 0.2 # 1 pixel = 0.2 um x 0.2 um\n",
    "global MIN_DIST_XY\n",
    "MIN_DIST_XY = round(1.8 / PIXEL_SIZE) # min distance in space between sparks\n",
    "TIME_FRAME = 6.8 # 1 frame = 6.8 ms\n",
    "global MIN_DIST_T\n",
    "MIN_DIST_T = round(20 / TIME_FRAME) # min distance in time between sparks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d11d1531-6eb7-4e7a-b1db-f63123cfdc38",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_sparks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10540/3784638497.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mt_sparks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmin_radius_sparks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m binary_sparks = process_spark_prediction(pred=sample_sparks,\n\u001b[0m\u001b[0;32m      7\u001b[0m                                          \u001b[0mt_detection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt_sparks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                          \u001b[0mmin_dist_xy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMIN_DIST_XY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_sparks' is not defined"
     ]
    }
   ],
   "source": [
    "# process sparks preds \n",
    "\n",
    "# get sparks centres\n",
    "t_sparks = 0.7\n",
    "min_radius_sparks = 2\n",
    "binary_sparks = process_spark_prediction(pred=sample_sparks,\n",
    "                                         t_detection=t_sparks,\n",
    "                                         min_dist_xy = MIN_DIST_XY,\n",
    "                                         min_dist_t = MIN_DIST_T,\n",
    "                                         min_radius=min_radius_sparks,\n",
    "                                         return_mask=True,\n",
    "                                         ignore_frames=ignore_frames_loss\n",
    "                                        )[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "148492d7-c7be-457b-b931-808a7ef16169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase sparks dimension\n",
    "radius_event = 3\n",
    "radius_ignore = 1\n",
    "ignore_index = 4\n",
    "binary_sparks = final_mask(mask=binary_sparks,\n",
    "                           radius1=radius_event,\n",
    "                           radius2=radius_event+radius_ignore,\n",
    "                           ignore_ind=ignore_index\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0ab495b-bf5f-4436-bd5d-a2fbcd8e3993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((904, 64, 512), array([0, 1, 4], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_sparks.shape, np.unique(binary_sparks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b67f41f3-b66f-49bd-bca4-92b42a9b3c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process puffs and waves preds to get binary masks\n",
    "t_puffs = 0.5\n",
    "t_waves = 0.5\n",
    "min_radius_puffs = 4\n",
    "min_radius_waves = 6\n",
    "\n",
    "binary_puffs = process_puff_prediction(pred=sample_puffs, \n",
    "                                       t_detection=t_puffs, \n",
    "                                       min_radius=min_radius_puffs, \n",
    "                                       ignore_frames=ignore_frames_loss\n",
    "                                      )\n",
    "binary_waves = process_wave_prediction(pred=sample_waves,\n",
    "                                       t_detection=t_waves, \n",
    "                                       min_radius=min_radius_waves, \n",
    "                                       ignore_frames=ignore_frames_loss\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2e080e6-2409-46d7-a31d-391b80fbbeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all classes into a unique prediction\n",
    "binary_puffs_waves = 3*binary_puffs + 2*binary_waves\n",
    "assert np.all(binary_puffs_waves) < 5, \"ROI in binary mask true for both puffs and waves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "004d729e-5aaf-4626-a9cc-42062012cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_mask = np.where(np.isin(binary_sparks, [1,4]), binary_sparks, binary_puffs_waves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb06e9b5-63c1-4670-88c2-340f518be232",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.volwrite(os.path.join(validation_folder, \"TEST.tif\"), np.uint8(binary_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93cf63d-b6b3-46f5-9bc4-e4ecbd1d7975",
   "metadata": {},
   "source": [
    "### Generate binary annotations for all training and all videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94026ffe-1f87-4b60-b5d9-7d833850d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# physiological params\n",
    "PIXEL_SIZE = 0.2 # 1 pixel = 0.2 um x 0.2 um\n",
    "global MIN_DIST_XY\n",
    "MIN_DIST_XY = round(1.8 / PIXEL_SIZE) # min distance in space between sparks\n",
    "TIME_FRAME = 6.8 # 1 frame = 6.8 ms\n",
    "global MIN_DIST_T\n",
    "MIN_DIST_T = round(20 / TIME_FRAME) # min distance in time between sparks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ef8b9d2-2f35-4761-b8d9-6712b0299065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general params\n",
    "ignore_frames_loss = 6\n",
    "\n",
    "'''# sparks params\n",
    "t_sparks = 0.9\n",
    "min_radius_sparks = 2\n",
    "radius_event = 3\n",
    "radius_ignore = 1\n",
    "ignore_index = 4\n",
    "\n",
    "# puffs and waves params\n",
    "t_puffs = 0.5\n",
    "t_waves = 0.5\n",
    "min_radius_puffs = 8\n",
    "min_radius_waves = 12'''\n",
    "\n",
    "# for focal_loss_gamma_5_ubelix\n",
    "# sparks params\n",
    "t_sparks = 0.7\n",
    "min_radius_sparks = 2\n",
    "radius_event = 3\n",
    "radius_ignore = 1\n",
    "ignore_index = 4\n",
    "\n",
    "# puffs and waves params\n",
    "t_puffs = 0.65\n",
    "t_waves = 0.65\n",
    "min_radius_puffs = 6\n",
    "min_radius_waves = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8d6c139-6a52-4d1a-8615-6253585c0a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing training focal_loss_gamma_5_ubelix...\n",
      "\tprocessing sample 05...\n",
      "ellipsoid shape (5, 11, 11)\n",
      "smoothed video shape (500, 64, 512)\n",
      "\tprocessing sample 10...\n",
      "ellipsoid shape (5, 11, 11)\n",
      "smoothed video shape (500, 64, 512)\n",
      "\tprocessing sample 15...\n",
      "ellipsoid shape (5, 11, 11)\n",
      "smoothed video shape (500, 64, 512)\n",
      "\tprocessing sample 20...\n",
      "ellipsoid shape (5, 11, 11)\n",
      "smoothed video shape (500, 64, 512)\n",
      "\tprocessing sample 25...\n",
      "ellipsoid shape (5, 11, 11)\n",
      "smoothed video shape (1000, 64, 512)\n",
      "\tprocessing sample 32...\n",
      "ellipsoid shape (5, 11, 11)\n",
      "smoothed video shape (1000, 64, 512)\n",
      "\tprocessing sample 34...\n",
      "ellipsoid shape (5, 11, 11)\n",
      "smoothed video shape (904, 64, 512)\n",
      "\tprocessing sample 40...\n",
      "ellipsoid shape (5, 11, 11)\n",
      "smoothed video shape (1000, 64, 512)\n",
      "\tprocessing sample 45...\n",
      "ellipsoid shape (5, 11, 11)\n",
      "smoothed video shape (1000, 64, 512)\n"
     ]
    }
   ],
   "source": [
    "from metrics_tools import process_spark_prediction\n",
    "for training_name in training_names:\n",
    "    print(f\"processing training {training_name}...\")\n",
    "    for video_id in ys_all_trainings.keys():\n",
    "        print(f\"\\tprocessing sample {video_id}...\")\n",
    "        sample_ys = ys[training_name][video_id]\n",
    "        sample_sparks = sparks[training_name][video_id]\n",
    "        sample_puffs = puffs[training_name][video_id]\n",
    "        sample_waves = waves[training_name][video_id]\n",
    "        \n",
    "        # process sparks preds \n",
    "        binary_sparks = process_spark_prediction(pred=sample_sparks,\n",
    "                                                 t_detection=t_sparks,\n",
    "                                                 min_dist_xy = MIN_DIST_XY,\n",
    "                                                 min_dist_t = MIN_DIST_T,\n",
    "                                                 min_radius=min_radius_sparks,\n",
    "                                                 return_mask=True,\n",
    "                                                 ignore_frames=ignore_frames_loss\n",
    "                                                )[1]\n",
    "        # increase sparks dimension\n",
    "        binary_sparks = final_mask(mask=binary_sparks,\n",
    "                                   radius1=radius_event,\n",
    "                                   radius2=radius_event+radius_ignore,\n",
    "                                   ignore_ind=ignore_index\n",
    "                                  )\n",
    "        \n",
    "        # process puffs and waves preds to get binary masks\n",
    "        binary_puffs = process_puff_prediction(pred=sample_puffs, \n",
    "                                               t_detection=t_puffs, \n",
    "                                               min_radius=min_radius_puffs, \n",
    "                                               ignore_frames=ignore_frames_loss\n",
    "                                              )\n",
    "        binary_waves = process_wave_prediction(pred=sample_waves,\n",
    "                                               t_detection=t_waves, \n",
    "                                               min_radius=min_radius_waves, \n",
    "                                               ignore_frames=ignore_frames_loss\n",
    "                                              )\n",
    "        \n",
    "        # combine all classes into a unique prediction\n",
    "        binary_puffs_waves = 3*binary_puffs + 2*binary_waves\n",
    "        assert np.all(binary_puffs_waves) < 5, \"ROI in binary mask true for both puffs and waves\"\n",
    "        binary_mask = np.where(np.isin(binary_sparks, [1,4]), binary_sparks, binary_puffs_waves)\n",
    "        \n",
    "        # write binary preds to disk\n",
    "        load_epoch = epoch_2 if  training_name == epoch_2_training_name else epoch \n",
    "        filename = training_name + \"_\" + str(load_epoch) + \"_\" + video_id + \"_pred_segmentation.tif\"\n",
    "        imageio.volwrite(os.path.join(validation_folder, filename), np.uint8(binary_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c299f-679b-49d2-b0cc-b0858564db5f",
   "metadata": {},
   "source": [
    "### Generate binary masks where sparks are unprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6b45421-4be7-488b-ae77-56aa93714496",
   "metadata": {},
   "outputs": [],
   "source": [
    "for training_name in training_names:\n",
    "    for video_id in ys_all_trainings.keys():\n",
    "        sample_ys = ys[training_name][video_id]\n",
    "        sample_sparks = sparks[training_name][video_id]\n",
    "        sample_puffs = puffs[training_name][video_id]\n",
    "        sample_waves = waves[training_name][video_id]\n",
    "        \n",
    "        # process sparks preds \n",
    "        binary_sparks = process_spark_prediction(pred=sample_sparks,\n",
    "                                                 t_detection=t_sparks,\n",
    "                                                 min_radius=min_radius_sparks,\n",
    "                                                 return_clean_pred=True,\n",
    "                                                 ignore_frames=ignore_frames_loss\n",
    "                                                ) > t_sparks\n",
    "    \n",
    "        # increase sparks dimension\n",
    "        '''binary_sparks = final_mask(mask=binary_sparks,\n",
    "                                   radius1=radius_event,\n",
    "                                   radius2=radius_event+radius_ignore,\n",
    "                                   ignore_ind=ignore_index\n",
    "                                  )'''\n",
    "        \n",
    "        # process puffs and waves preds to get binary masks\n",
    "        binary_puffs = process_puff_prediction(pred=sample_puffs, \n",
    "                                               t_detection=t_puffs, \n",
    "                                               min_radius=min_radius_puffs, \n",
    "                                               ignore_frames=ignore_frames_loss\n",
    "                                              )\n",
    "        binary_waves = process_wave_prediction(pred=sample_waves,\n",
    "                                               t_detection=t_waves, \n",
    "                                               min_radius=min_radius_waves, \n",
    "                                               ignore_frames=ignore_frames_loss\n",
    "                                              )\n",
    "        \n",
    "        # combine all classes into a unique prediction\n",
    "        binary_mask = 3*binary_puffs + 2*binary_waves + 1*binary_sparks\n",
    "        assert np.all(binary_mask) < 4, \"ROI in binary mask true for both puffs and waves\"\n",
    "        \n",
    "        # write binary preds to disk\n",
    "        load_epoch = epoch_2 if  training_name == epoch_2_training_name else epoch \n",
    "        filename = training_name + \"_\" + str(load_epoch) + \"_\" + video_id + \"_pred_segmentation_raw_sparks.tif\"\n",
    "        imageio.volwrite(os.path.join(validation_folder, filename), np.uint8(binary_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88b9ce-54bc-43f0-aab3-3aef1b83b046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
