; train using denoising/adding noise as data augmentation
[general]
verbosity = 3
wandb_enable = yes
wandb_project_name = sparks
; if training and testing == 0 only run validation once
training = yes
testing = yes
;batch_size = 2
batch_size = 1
run_name = noise_data_augmentation_physio

[state]
load_epoch = 0

[data]
size = full
;size = minimal
relative_path = ../data/sparks_dataset
;remove_background = moving
;remove_background = average
remove_background = no
step = 32
chunks_duration = 256
ignore_frames_loss = 6
ignore_index = 4
norm_video = abs_max
smoothing = no
noise_data_augmentation = yes

[network]
initialize_weights = no
num_classes = 4
ndims = 3
border_mode = same
batch_normalization = no
output_relative_path = runs/
first_layer_channels = 8
step = 6
;dilation = 1
;temporal_reduction = no
;num_channels = 1

[training]
num_workers = 0
;criterion = nll_loss
criterion = focal_loss
; only for focal loss:
gamma = 5
epochs = 150000
save_every = 10000
test_every = 5000
plot_every = 1000

[testing]
t_sparks = 0.8
t_puffs = 0.65
t_waves = 0.6
sparks_min_radius = 0
puffs_min_radius = 5
waves_min_radius = 0
; use this params to validate the unet
batch_size = 1
dataset_size = full
;dataset_size = minimal
load_epoch = 100000
