{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.11.2022\n",
    "\n",
    "# Test Grad-CAM on UNet model\n",
    "\n",
    "Provo a usare una Grad-CAM su un modello salvato della UNet e il movie 34 (dove la fine della wave viene detettata come puff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from architectures import TempRedUNet\n",
    "from datasets import SparkDataset\n",
    "from in_out_tools import write_videos_on_disk\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from training_inference_tools import run_samples_in_model, sampler\n",
    "from visualization_tools import get_discrete_cmap\n",
    "\n",
    "from medcam import medcam\n",
    "import unet\n",
    "import napari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training 'TEMP_new_annotated_peaks_physio'...\n"
     ]
    }
   ],
   "source": [
    "training_name = 'TEMP_new_annotated_peaks_physio'\n",
    "config_file = 'config_temp_new_annotated_peaks_physio.ini'\n",
    "\n",
    "print(f\"Processing training '{training_name}'...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files will be saved on 'trainings_validation\\TEMP_new_annotated_peaks_physio\\gradCAM'\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"trainings_validation\" # same folder for train and test preds\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# subdirectory of output_folder where predictions are saved\n",
    "# change this to save results for same model with different inference approaches\n",
    "output_name = training_name\n",
    "\n",
    "save_folder = os.path.join(output_folder, output_name, \"gradCAM\")\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "print(f\"Output files will be saved on '{save_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect GPU, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device 'cuda' with 1 GPUs\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"Using device '{device}' with {n_gpus} GPUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config_files\\config_temp_new_annotated_peaks_physio.ini\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['config_files\\\\config_temp_new_annotated_peaks_physio.ini']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_folder = \"config_files\"\n",
    "CONFIG_FILE = os.path.join(config_folder, config_file)\n",
    "c = configparser.ConfigParser()\n",
    "\n",
    "print(f\"Loading {CONFIG_FILE}\")\n",
    "c.read(CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Params ###\n",
    "load_epoch = c.getint(\"testing\", \"load_epoch\")\n",
    "\n",
    "batch_size = c.getint(\"testing\", \"batch_size\", fallback=\"1\")\n",
    "ignore_frames = c.getint(\"training\", \"ignore_frames_loss\")\n",
    "\n",
    "temporal_reduction = c.getboolean(\"network\", \"temporal_reduction\", fallback=False)\n",
    "num_channels = c.getint(\"network\", \"num_channels\", fallback=1) if temporal_reduction else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configure UNet ###\n",
    "\n",
    "batch_norm = {'batch': True, 'none': False}\n",
    "\n",
    "unet_config = unet.UNetConfig(\n",
    "    steps=c.getint(\"network\", \"unet_steps\"),\n",
    "    first_layer_channels=c.getint(\"network\", \"first_layer_channels\"),\n",
    "    num_classes=4,\n",
    "    ndims=3,\n",
    "    dilation=c.getint(\"network\", \"dilation\", fallback=1),\n",
    "    border_mode=c.get(\"network\", \"border_mode\"),\n",
    "    batch_normalization=batch_norm[c.get(\"network\", \"batch_normalization\")],\n",
    "    num_input_channels=num_channels,\n",
    ")\n",
    "if not temporal_reduction:\n",
    "    network = unet.UNetClassifier(unet_config)\n",
    "else:\n",
    "    assert c.getint(\"dataset\", \"data_duration\") % num_channels == 0, \\\n",
    "    \"using temporal reduction chunks_duration must be a multiple of num_channels\"\n",
    "    network = TempRedUNet(unet_config)\n",
    "\n",
    "network = nn.DataParallel(network).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model 'TEMP_new_annotated_peaks_physio' at epoch 100000...\n"
     ]
    }
   ],
   "source": [
    "### Load UNet model ###\n",
    "models_relative_path = 'runs/'\n",
    "model_path = os.path.join(models_relative_path, training_name)\n",
    "#logger.info(f\"Saved model path: {model_path}\")\n",
    "summary_writer = SummaryWriter(os.path.join(model_path, \"summary\"),\n",
    "                               purge_step=0)\n",
    "\n",
    "trainer = unet.TrainingManager(\n",
    "        # training items\n",
    "        training_step = None,\n",
    "        save_path=model_path,\n",
    "        managed_objects=unet.managed_objects({'network': network}),\n",
    "        summary_writer=summary_writer\n",
    "    )\n",
    "\n",
    "print(f\"Loading trained model '{training_name}' at epoch {load_epoch}...\")\n",
    "trainer.load(load_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print summary of network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─UNetClassifier: 1-1                    [-1, 4, 256, 64, 512]     --\n",
      "|    └─Sequential: 2                     []                        --\n",
      "|    |    └─UNetLayer: 3-1               [-1, 8, 256, 64, 512]     1,960\n",
      "|    └─MaxPool3d: 2-1                    [-1, 8, 128, 32, 256]     --\n",
      "|    └─Sequential: 2                     []                        --\n",
      "|    |    └─UNetLayer: 3-2               [-1, 16, 128, 32, 256]    10,400\n",
      "|    └─MaxPool3d: 2-2                    [-1, 16, 64, 16, 128]     --\n",
      "|    └─Sequential: 2                     []                        --\n",
      "|    |    └─UNetLayer: 3-3               [-1, 32, 64, 16, 128]     41,536\n",
      "|    └─MaxPool3d: 2-3                    [-1, 32, 32, 8, 64]       --\n",
      "|    └─Sequential: 2                     []                        --\n",
      "|    |    └─UNetLayer: 3-4               [-1, 64, 32, 8, 64]       166,016\n",
      "|    └─MaxPool3d: 2-4                    [-1, 64, 16, 4, 32]       --\n",
      "|    └─Sequential: 2                     []                        --\n",
      "|    |    └─UNetLayer: 3-5               [-1, 128, 16, 4, 32]      663,808\n",
      "|    └─MaxPool3d: 2-5                    [-1, 128, 8, 2, 16]       --\n",
      "|    └─Sequential: 2                     []                        --\n",
      "|    |    └─UNetLayer: 3-6               [-1, 256, 8, 2, 16]       2,654,720\n",
      "|    └─MaxPool3d: 2-6                    [-1, 256, 4, 1, 8]        --\n",
      "|    └─Sequential: 2                     []                        --\n",
      "|    |    └─UNetLayer: 3-7               [-1, 512, 4, 1, 8]        10,617,856\n",
      "|    └─Sequential: 2                     []                        --\n",
      "|    |    └─ConvTranspose3d: 3-8         [-1, 256, 8, 2, 16]       1,048,832\n",
      "|    |    └─UNetLayer: 3-9               [-1, 256, 8, 2, 16]       5,308,928\n",
      "|    |    └─ConvTranspose3d: 3-10        [-1, 128, 16, 4, 32]      262,272\n",
      "|    |    └─UNetLayer: 3-11              [-1, 128, 16, 4, 32]      1,327,360\n",
      "|    |    └─ConvTranspose3d: 3-12        [-1, 64, 32, 8, 64]       65,600\n",
      "|    |    └─UNetLayer: 3-13              [-1, 64, 32, 8, 64]       331,904\n",
      "|    |    └─ConvTranspose3d: 3-14        [-1, 32, 64, 16, 128]     16,416\n",
      "|    |    └─UNetLayer: 3-15              [-1, 32, 64, 16, 128]     83,008\n",
      "|    |    └─ConvTranspose3d: 3-16        [-1, 16, 128, 32, 256]    4,112\n",
      "|    |    └─UNetLayer: 3-17              [-1, 16, 128, 32, 256]    20,768\n",
      "|    |    └─ConvTranspose3d: 3-18        [-1, 8, 256, 64, 512]     1,032\n",
      "|    |    └─UNetLayer: 3-19              [-1, 8, 256, 64, 512]     5,200\n",
      "|    └─Conv3d: 2-7                       [-1, 4, 256, 64, 512]     36\n",
      "|    └─LogSoftmax: 2-8                   [-1, 4, 256, 64, 512]     --\n",
      "==========================================================================================\n",
      "Total params: 22,631,764\n",
      "Trainable params: 22,631,764\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 17.24\n",
      "==========================================================================================\n",
      "Input size (MB): 32.00\n",
      "Forward/backward pass size (MB): 938.50\n",
      "Params size (MB): 86.33\n",
      "Estimated Total Size (MB): 1056.83\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─UNetClassifier: 1-1                    [-1, 4, 256, 64, 512]     --\n",
       "|    └─Sequential: 2                     []                        --\n",
       "|    |    └─UNetLayer: 3-1               [-1, 8, 256, 64, 512]     1,960\n",
       "|    └─MaxPool3d: 2-1                    [-1, 8, 128, 32, 256]     --\n",
       "|    └─Sequential: 2                     []                        --\n",
       "|    |    └─UNetLayer: 3-2               [-1, 16, 128, 32, 256]    10,400\n",
       "|    └─MaxPool3d: 2-2                    [-1, 16, 64, 16, 128]     --\n",
       "|    └─Sequential: 2                     []                        --\n",
       "|    |    └─UNetLayer: 3-3               [-1, 32, 64, 16, 128]     41,536\n",
       "|    └─MaxPool3d: 2-3                    [-1, 32, 32, 8, 64]       --\n",
       "|    └─Sequential: 2                     []                        --\n",
       "|    |    └─UNetLayer: 3-4               [-1, 64, 32, 8, 64]       166,016\n",
       "|    └─MaxPool3d: 2-4                    [-1, 64, 16, 4, 32]       --\n",
       "|    └─Sequential: 2                     []                        --\n",
       "|    |    └─UNetLayer: 3-5               [-1, 128, 16, 4, 32]      663,808\n",
       "|    └─MaxPool3d: 2-5                    [-1, 128, 8, 2, 16]       --\n",
       "|    └─Sequential: 2                     []                        --\n",
       "|    |    └─UNetLayer: 3-6               [-1, 256, 8, 2, 16]       2,654,720\n",
       "|    └─MaxPool3d: 2-6                    [-1, 256, 4, 1, 8]        --\n",
       "|    └─Sequential: 2                     []                        --\n",
       "|    |    └─UNetLayer: 3-7               [-1, 512, 4, 1, 8]        10,617,856\n",
       "|    └─Sequential: 2                     []                        --\n",
       "|    |    └─ConvTranspose3d: 3-8         [-1, 256, 8, 2, 16]       1,048,832\n",
       "|    |    └─UNetLayer: 3-9               [-1, 256, 8, 2, 16]       5,308,928\n",
       "|    |    └─ConvTranspose3d: 3-10        [-1, 128, 16, 4, 32]      262,272\n",
       "|    |    └─UNetLayer: 3-11              [-1, 128, 16, 4, 32]      1,327,360\n",
       "|    |    └─ConvTranspose3d: 3-12        [-1, 64, 32, 8, 64]       65,600\n",
       "|    |    └─UNetLayer: 3-13              [-1, 64, 32, 8, 64]       331,904\n",
       "|    |    └─ConvTranspose3d: 3-14        [-1, 32, 64, 16, 128]     16,416\n",
       "|    |    └─UNetLayer: 3-15              [-1, 32, 64, 16, 128]     83,008\n",
       "|    |    └─ConvTranspose3d: 3-16        [-1, 16, 128, 32, 256]    4,112\n",
       "|    |    └─UNetLayer: 3-17              [-1, 16, 128, 32, 256]    20,768\n",
       "|    |    └─ConvTranspose3d: 3-18        [-1, 8, 256, 64, 512]     1,032\n",
       "|    |    └─UNetLayer: 3-19              [-1, 8, 256, 64, 512]     5,200\n",
       "|    └─Conv3d: 2-7                       [-1, 4, 256, 64, 512]     36\n",
       "|    └─LogSoftmax: 2-8                   [-1, 4, 256, 64, 512]     --\n",
       "==========================================================================================\n",
       "Total params: 22,631,764\n",
       "Trainable params: 22,631,764\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 17.24\n",
       "==========================================================================================\n",
       "Input size (MB): 32.00\n",
       "Forward/backward pass size (MB): 938.50\n",
       "Params size (MB): 86.33\n",
       "Estimated Total Size (MB): 1056.83\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(network, (1,256,64,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.down_path.0.layers.0 Conv3d(1, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.down_path.0.layers.3 Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.down_path.1.layers.0 Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.down_path.1.layers.3 Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.down_path.2.layers.0 Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.down_path.2.layers.3 Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.down_path.3.layers.0 Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.down_path.3.layers.3 Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.down_path.4.layers.0 Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.down_path.4.layers.3 Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.down_path.5.layers.0 Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.down_path.5.layers.3 Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.down_path.6.layers.0 Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.down_path.6.layers.3 Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.up_path.1.layers.0 Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.up_path.1.layers.3 Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.up_path.3.layers.0 Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.up_path.3.layers.3 Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.up_path.5.layers.0 Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.up_path.5.layers.3 Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.up_path.7.layers.0 Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.up_path.7.layers.3 Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.up_path.9.layers.0 Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.up_path.9.layers.3 Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.up_path.11.layers.0 Conv3d(16, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.up_path.11.layers.3 Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "module.final_layer Conv3d(8, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n"
     ]
    }
   ],
   "source": [
    "for name, layer in network.named_modules():\n",
    "    if isinstance(layer, torch.nn.Conv3d):\n",
    "        print(name, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load input sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using C:\\Users\\dotti\\sparks_project\\data\\sparks_dataset as dataset root path\n"
     ]
    }
   ],
   "source": [
    "### Configure sample input ### \n",
    "\n",
    "sample_ids = [\"34\"]\n",
    "\n",
    "dataset_path = os.path.realpath(c.get(\"dataset\", \"relative_path\"))\n",
    "assert os.path.isdir(dataset_path), f\"\\\"{dataset_path}\\\" is not a directory\"\n",
    "print(f\"Using {dataset_path} as dataset root path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configure inference method and parameters ###\n",
    "\n",
    "data_step = c.getint(\"testing\", \"data_step\")\n",
    "data_duration = c.getint(\"testing\", \"data_duration\")\n",
    "inference = c.get(\"testing\", \"inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTesting dataset of movie 34 \tcontains 22 samples.\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = SparkDataset(\n",
    "    base_path=dataset_path,\n",
    "    sample_ids=sample_ids,\n",
    "    testing=False, # we just do inference, without metrics computation\n",
    "    smoothing=c.get(\"dataset\", \"data_smoothing\"),\n",
    "    step=data_step,\n",
    "    duration=data_duration,\n",
    "    remove_background=c.get(\"dataset\", \"remove_background\"),\n",
    "    temporal_reduction=c.getboolean(\"network\", \"temporal_reduction\", fallback=False),\n",
    "    num_channels=num_channels,\n",
    "    normalize_video=c.get(\"dataset\", \"norm_video\"),\n",
    "    only_sparks=c.getboolean(\"dataset\", \"only_sparks\", fallback=False),\n",
    "    sparks_type=c.get(\"dataset\", \"sparks_type\"),\n",
    "    ignore_frames=c.get(\"training\", \"ignore_frames_loss\"),\n",
    "    ignore_index=4,\n",
    "    gt_available=True,\n",
    "    inference=inference)\n",
    "\n",
    "print(f\"\\tTesting dataset of movie {testing_dataset.video_name} \"\\\n",
    "      f\"\\tcontains {len(testing_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(testing_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_network = medcam.inject(network,\n",
    "                        label=3,\n",
    "                        replace=True,\n",
    "                        #backend=\"gcam\",\n",
    "                        layer='module.final_layer',\n",
    "                        output_dir=save_folder,\n",
    "                        save_maps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sample's chunks in network and re-assemble UNet's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chunks = len(testing_dataset)\n",
    "half_overlap = (data_duration-data_step)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_concat = []\n",
    "x_concat = []\n",
    "for i, (x, _) in enumerate(data_loader):\n",
    "    # define start and end of used frames in chunks\n",
    "    start = 0 if i==0 else half_overlap\n",
    "    end = None if i+1==n_chunks else -half_overlap\n",
    "\n",
    "    x_concat.append(x[0,start:end])\n",
    "\n",
    "    x = x.to(device)\n",
    "    out = cam_network(x[None,:])[0,0]\n",
    "    out_concat.append(out[start:end].cpu())\n",
    "x_concat = torch.cat(x_concat, dim=0).numpy()\n",
    "out_concat = torch.cat(out_concat, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(928, 64, 512)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_concat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise result with Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Napari cmap\n",
    "cmap = get_discrete_cmap(name='gray', lut=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'network output' at 0x14486fb6130>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "\n",
    "viewer.add_image(x_concat, \n",
    "                 name='input movie', \n",
    "                 colormap=('colors',cmap)\n",
    "                )\n",
    "\n",
    "viewer.add_image(out_concat, \n",
    "                 name='network output', \n",
    "                 colormap=('colors',cmap)\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('sparks')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d33eb8e81965b779f2871c6ab1ae98a760df4ff814358c9a5efa0a44482010f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
