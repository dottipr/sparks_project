{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06/10/2023\n",
    "\n",
    "Uso questo script per ricreare i datasets cercando di strutturarli meglio\n",
    "- dataset che prende movies e labels come inputs,\n",
    "- dataset che prende dataset_path e movie ids come inputs,\n",
    "- dataset che gestisce l'inference con o senza ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from data.datasets import SparkDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, Union, Tuple, List, Optional\n",
    "\n",
    "from config import config, TrainingConfig\n",
    "from data.data_processing_tools import (\n",
    "    masks_to_instances_dict,\n",
    "    preds_dict_to_mask,\n",
    "    process_raw_predictions,\n",
    ")\n",
    "from evaluation.metrics_tools import compute_iou, get_matches_summary, get_metrics_from_summary, get_score_matrix\n",
    "from utils.custom_losses import MySoftDiceLoss\n",
    "from utils.in_out_tools import write_videos_on_disk\n",
    "from utils.training_inference_tools import training_step, sampler\n",
    "from utils.training_script_utils import init_model, init_dataset, init_criterion, get_sample_ids\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "config.verbosity = 3  # To get debug messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:29:34] [  INFO  ] [   config   ] <291 > -- Loading C:\\Users\\dotti\\sparks_project\\config_files\\config_final_model.ini\n",
      "[12:29:34] [  INFO  ] [utils.training_script_utils] <137 > -- Samples in training dataset: 9\n"
     ]
    }
   ],
   "source": [
    "# Create a TrainingConfig object\n",
    "# params = TrainingConfig()\n",
    "config_filename = os.path.join(\"config_files\", \"config_final_model.ini\")\n",
    "params = TrainingConfig(training_config_file=config_filename)\n",
    "\n",
    "# Adapt parameters for debugging\n",
    "# params.inference_dataset_size = \"minimal\"\n",
    "# params.inference_batch_size = 2\n",
    "# params.data_duration = 64\n",
    "# params.set_device(device=\"cpu\")\n",
    "\n",
    "# Select samples for training and testing based on dataset size\n",
    "train_sample_ids = get_sample_ids(\n",
    "    train_data=True,\n",
    "    dataset_size=params.dataset_size,\n",
    ")\n",
    "\n",
    "# Create a sparkdataset\n",
    "dataset = init_dataset(\n",
    "    params=params,\n",
    "    sample_ids=train_sample_ids,\n",
    "    apply_data_augmentation=True,\n",
    "    load_instances=False,\n",
    ")\n",
    "\n",
    "# Create a dataloader\n",
    "dataset_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=params.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=params.num_workers,\n",
    "    pin_memory=params.pin_memory,\n",
    ")\n",
    "\n",
    "# Create a U-Net\n",
    "network = init_model(params=params)\n",
    "# network = network.to(params.device, non_blocking=True)\n",
    "network = nn.DataParallel(network).to(params.device, non_blocking=True)\n",
    "# cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get item from dataloader\n",
    "batch = next(iter(dataset_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['movie_id', 'original_duration', 'data', 'labels', 'sample_id']),\n",
       " tensor([0, 0, 0, 0]),\n",
       " torch.Size([4, 256, 64, 512]),\n",
       " torch.Size([4, 256, 64, 512]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys(), batch[\"movie_id\"], batch[\"data\"].shape, batch[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:  RIORGANIZZARE QUESTE FUNZIONI\n",
    "\n",
    "Domanda: ha davvero senso utilizzare get_preds etc e non solo do_inference? Sembra un po' tanto lavoro inutile ricostruire i labels e i video originali dal dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:29:36] [  INFO  ] [  __main__  ] < 13 > -- Loading trained model 'final_model' at epoch 100000...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained model\n",
    "load_epoch = 100000\n",
    "model_filename = f\"network_{load_epoch:06d}.pth\"\n",
    "\n",
    "# Path to the saved model checkpoint\n",
    "models_relative_path = os.path.join(\n",
    "    \"models\", \"saved_models\", params.run_name, model_filename\n",
    ")\n",
    "model_dir = os.path.realpath(os.path.join(\n",
    "    config.basedir, models_relative_path))\n",
    "\n",
    "# Load the model state dictionary\n",
    "logger.info(\n",
    "    f\"Loading trained model '{params.run_name}' at epoch {load_epoch}...\")\n",
    "network.load_state_dict(torch.load(model_dir, map_location=params.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr=params.lr_start)\n",
    "criterion = nn.NLLLoss(\n",
    "    ignore_index=config.ignore_index,  # .to(\n",
    "    # params.device, non_blocking=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:29:40] [WARNING ] [utils.training_inference_tools] <136 > -- loss: 0.2515716254711151, loss is tensor: True\n",
      "[12:29:40] [WARNING ] [utils.training_inference_tools] <139 > -- gradients zeroed\n",
      "[12:29:42] [WARNING ] [utils.training_inference_tools] <142 > -- loss backward\n",
      "[12:29:43] [WARNING ] [utils.training_inference_tools] <145 > -- optimizer step done\n"
     ]
    }
   ],
   "source": [
    "loss = training_step(\n",
    "    dataset_loader=dataset_loader,\n",
    "    params=params,\n",
    "    sampler=sampler,\n",
    "    network=network,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=None,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.2515716254711151}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
