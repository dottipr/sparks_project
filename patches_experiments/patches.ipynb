{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on Patches of Confocal tyx-Image Series\n",
    "\n",
    "**Author**: Prisca Dotti\n",
    "\n",
    "**Last Edit**: 07.06.2024\n",
    "\n",
    "This notebook is used to run some experiments using the U-Net model on patches obtained from either real or fake confocal imaging data.\n",
    "\n",
    "The dataset used for training could be one of the following:\n",
    "- dataset of patches extracted in a meaningful way from confocal imaging recordings\n",
    "- dataset of recordings from which patches are extracted when processing them in the U-Net for memory management reasons and which are recombined as whole movies after inference\n",
    "- dataset of simulated confocal imaging patches --> this is similar to what I've been doing so far\n",
    "- a combination of real and fake patches of confocal imaging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload is used to reload modules automatically before entering the\n",
    "# execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# To import modules from parent directory in Jupyter Notebook\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "# from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "import wandb\n",
    "from config import TrainingConfig, config\n",
    "from data.datasets import PatchSparkDataset\n",
    "from models.UNet import unet\n",
    "from utils.training_inference_tools import (\n",
    "    MyTrainingManager,\n",
    "    sampler,\n",
    "    test_function,\n",
    "    training_step,\n",
    "    weights_init,\n",
    "    TransformedSparkDataset,\n",
    "    random_flip,\n",
    "    random_flip_noise,\n",
    ")\n",
    "from utils.training_script_utils import (\n",
    "    get_sample_ids,\n",
    "    init_criterion,\n",
    "    init_dataset,\n",
    "    init_model,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:48:37] [  INFO  ] [   config   ] <315 > -- Loading C:\\Users\\prisc\\Code\\sparks_project\\config_files\\config_patches.ini\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --     training_config_file: C:\\Users\\prisc\\Code\\sparks_project\\config_files\\config_patches.ini\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --              dataset_dir: C:\\Users\\prisc\\Code\\sparks_project\\data\\sparks_dataset\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --                        c: <configparser.ConfigParser object at 0x000001FEF0C8D820>\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --                 run_name: patches\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --            load_run_name: \n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --               load_epoch: 0\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --             train_epochs: 100000\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --                criterion: lovasz_softmax\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --                 lr_start: 0.0001\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --       ignore_frames_loss: 6\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --                     cuda: True\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --                scheduler: None\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --                optimizer: adam\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --             dataset_size: minimal\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --               batch_size: 32\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --              num_workers: 0\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --            data_duration: 64\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --              data_stride: 32\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --               patch_size: [64, 64, 64]\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --           data_smoothing: no\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --               norm_video: abs_max\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --        remove_background: no\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --       mask_cell_exterior: True\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --  noise_data_augmentation: False\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --              sparks_type: raw\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --                  new_fps: 0\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --             sin_channels: False\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --           n_sin_channels: [0, 0, 0]\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --  inference_data_duration: 256\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --    inference_data_stride: 32\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --                inference: patches\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --     inference_load_epoch: 100000\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --     inference_batch_size: 32\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --   inference_dataset_size: full\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --          nn_architecture: pablos_unet\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --               unet_steps: 6\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --     first_layer_channels: 8\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --             num_channels: 1\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --                 dilation: 1\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --              border_mode: same\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --      batch_normalization: none\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --       temporal_reduction: False\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --       initialize_weights: False\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --                wandb_log: False\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --                   device: cuda\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --               pin_memory: True\n",
      "[12:48:37] [  INFO  ] [   config   ] <567 > --                   n_gpus: 1\n"
     ]
    }
   ],
   "source": [
    "##################### Get training-specific parameters #####################\n",
    "\n",
    "# Initialize training-specific parameters\n",
    "# (get the configuration file path from ArgParse)\n",
    "config_filename = os.path.join(\"config_files\", \"config_patches.ini\")\n",
    "# config_filename = os.path.join(\"config_files\", \"config_final_model.ini\")\n",
    "\n",
    "params = TrainingConfig(training_config_file=config_filename)\n",
    "\n",
    "# Print parameters to console if needed\n",
    "params.print_params()\n",
    "\n",
    "######################### Initialize random seeds ##########################\n",
    "\n",
    "# We used these random seeds to ensure reproducibility of the results\n",
    "\n",
    "torch.manual_seed(0)  # <--------------------------------------------------!\n",
    "random.seed(0)  # <--------------------------------------------------------!\n",
    "np.random.seed(0)  # <-----------------------------------------------------!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:38:10] [  INFO  ] [   config   ] <562 > -- Using cuda\n"
     ]
    }
   ],
   "source": [
    "# params.set_device(\"cpu\")\n",
    "params.display_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:38:11] [  INFO  ] [  __main__  ] < 26 > -- Samples in dataset (patches sampled in an iteration): 10000\n",
      "[12:38:11] [WARNING ] [tifffile.tifffile] <16549> -- TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n",
      "[12:38:11] [WARNING ] [tifffile.tifffile] <16549> -- TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n",
      "[12:38:17] [  INFO  ] [utils.training_script_utils] <149 > -- Samples in dataset: 1\n"
     ]
    }
   ],
   "source": [
    "############################ Configure datasets ############################\n",
    "\n",
    "# Select samples for training and testing based on dataset size\n",
    "train_sample_ids = get_sample_ids(\n",
    "    train_data=True,\n",
    "    dataset_size=params.dataset_size,\n",
    ")\n",
    "test_sample_ids = get_sample_ids(\n",
    "    train_data=False,\n",
    "    dataset_size=params.dataset_size,\n",
    ")\n",
    "\n",
    "# Initialize training dataset: here it only samples random patches\n",
    "dataset = PatchSparkDataset(\n",
    "    params=params,\n",
    "    base_path=params.dataset_dir,\n",
    "    sample_ids=train_sample_ids,\n",
    "    load_instances=False,\n",
    "    inference=None,\n",
    ")\n",
    "# Apply transforms based on noise_data_augmentation setting\n",
    "# (transforms are applied when getting a sample from the dataset)\n",
    "transforms = random_flip_noise if params.noise_data_augmentation else random_flip\n",
    "dataset = TransformedSparkDataset(dataset, transforms)\n",
    "\n",
    "logger.info(f\"Samples in dataset (patches sampled in an iteration): {len(dataset)}\")\n",
    "\n",
    "# Initialize testing datasets\n",
    "testing_dataset = init_dataset(\n",
    "    params=params,\n",
    "    sample_ids=test_sample_ids,\n",
    "    apply_data_augmentation=False,\n",
    "    load_instances=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import napari\n",
    "\n",
    "# train_sample_id = 0\n",
    "# test_sample_id = 0\n",
    "\n",
    "# train_sample = dataset[train_sample_id]\n",
    "# test_sample = testing_dataset[test_sample_id]\n",
    "\n",
    "# train_data = train_sample[\"data\"].numpy()\n",
    "# train_labels = train_sample[\"labels\"].numpy()\n",
    "\n",
    "# test_data = test_sample[\"data\"].numpy()\n",
    "# test_labels = test_sample[\"labels\"].numpy()\n",
    "\n",
    "# viewer = napari.Viewer()\n",
    "# viewer.add_image(train_data)\n",
    "# viewer.add_labels(train_labels)\n",
    "# viewer.add_image(test_data)\n",
    "# viewer.add_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loaders\n",
    "dataset_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=params.batch_size,\n",
    "    num_workers=params.num_workers,\n",
    "    pin_memory=params.pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Configure UNet ##############################\n",
    "\n",
    "# Initialize the UNet model\n",
    "network = init_model(params=params)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "if params.device.type != \"cpu\":\n",
    "    network = nn.DataParallel(network).to(params.device, non_blocking=True)\n",
    "    # cudnn.benchmark = True\n",
    "\n",
    "# Watch the model with wandb for logging if enabled\n",
    "if params.wandb_log:\n",
    "    wandb.watch(network)\n",
    "\n",
    "# Initialize UNet weights if required\n",
    "if params.initialize_weights:\n",
    "    logger.info(\"Initializing UNet weights...\")\n",
    "    network.apply(weights_init)\n",
    "\n",
    "# The following line is commented as it does not work on Windows\n",
    "# torch.compile(network, mode=\"default\", backend=\"inductor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:46:15] [  INFO  ] [  __main__  ] < 26 > -- Output directory: C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\patches\n"
     ]
    }
   ],
   "source": [
    "########################### Initialize training ############################\n",
    "\n",
    "# Initialize the optimizer based on the specified type\n",
    "if params.optimizer == \"adam\":\n",
    "    optimizer = optim.Adam(network.parameters(), lr=params.lr_start)\n",
    "elif params.optimizer == \"adadelta\":\n",
    "    optimizer = optim.Adadelta(network.parameters(), lr=params.lr_start)\n",
    "elif params.optimizer == \"sgd\":\n",
    "    optimizer = optim.SGD(network.parameters(), lr=params.lr_start)\n",
    "else:\n",
    "    logger.error(f\"{params.optimizer} is not a valid optimizer.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the learning rate scheduler if specified\n",
    "if params.scheduler == \"step\":\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=params.scheduler_step_size,\n",
    "        gamma=params.scheduler_gamma,\n",
    "    )\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "# Define the output directory path\n",
    "output_path = os.path.join(config.output_dir, params.run_name)\n",
    "logger.info(f\"Output directory: {os.path.realpath(output_path)}\")\n",
    "\n",
    "# Initialize the summary writer for TensorBoard logging\n",
    "summary_writer = SummaryWriter(os.path.join(output_path, \"summary\"), purge_step=0)\n",
    "\n",
    "# Check if a pre-trained model should be loaded\n",
    "if params.load_run_name != \"\":\n",
    "    load_path = os.path.join(config.output_dir, params.load_run_name)\n",
    "    logger.info(f\"Model loaded from directory: {os.path.realpath(load_path)}\")\n",
    "else:\n",
    "    load_path = None\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = init_criterion(params=params, dataset=dataset)\n",
    "\n",
    "# Create a directory to save predicted class movies\n",
    "preds_output_dir = os.path.join(output_path, \"predictions\")\n",
    "os.makedirs(preds_output_dir, exist_ok=True)\n",
    "\n",
    "# Create a dictionary of managed objects\n",
    "managed_objects = {\"network\": network, \"optimizer\": optimizer}\n",
    "if scheduler is not None:\n",
    "    managed_objects[\"scheduler\"] = scheduler\n",
    "\n",
    "# Create a training manager with the specified training and testing functions\n",
    "trainer = MyTrainingManager(\n",
    "    # Training parameters\n",
    "    training_step=lambda _: training_step(\n",
    "        dataset_loader=dataset_loader,\n",
    "        params=params,\n",
    "        sampler=sampler,\n",
    "        network=network,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        scheduler=scheduler,\n",
    "        # scaler=GradScaler(),\n",
    "    ),\n",
    "    save_every=params.c.getint(\"training\", \"save_every\", fallback=5000),\n",
    "    load_path=load_path,\n",
    "    save_path=output_path,\n",
    "    managed_objects=unet.managed_objects(managed_objects),\n",
    "    # Testing parameters\n",
    "    test_function=lambda _: test_function(\n",
    "        network=network,\n",
    "        device=params.device,\n",
    "        criterion=criterion,\n",
    "        params=params,\n",
    "        testing_dataset=testing_dataset,\n",
    "        training_name=params.run_name,\n",
    "        output_dir=preds_output_dir,\n",
    "        training_mode=True,\n",
    "    ),\n",
    "    test_every=params.c.getint(\"training\", \"test_every\", fallback=1000),\n",
    "    plot_every=params.c.getint(\"training\", \"test_every\", fallback=1000),\n",
    "    summary_writer=summary_writer,\n",
    ")\n",
    "\n",
    "# Load the model if a specific epoch is provided\n",
    "if params.load_epoch != 0:\n",
    "    trainer.load(params.load_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the network before training if resuming from a checkpoint\n",
    "# if params.load_epoch > 0:\n",
    "#     logger.info(\"Validate network before training\")\n",
    "#     trainer.run_validation(wandb_log=params.wandb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:11:18] [  INFO  ] [  __main__  ] < 5  > -- Starting training\n",
      "[13:12:08] [  INFO  ] [utils.training_inference_tools] <1489> -- Iteration 0...\n",
      "[13:12:08] [  INFO  ] [utils.training_inference_tools] <1490> -- \tTraining loss: 0.7575\n",
      "[13:12:08] [  INFO  ] [utils.training_inference_tools] <1491> -- \tTime elapsed: 2.06s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model for the specified number of epochs\u001b[39;00m\n\u001b[0;32m      5\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprint_every\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwandb_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prisc\\Code\\sparks_project\\patches_experiments\\..\\utils\\training_inference_tools.py:1394\u001b[0m, in \u001b[0;36mMyTrainingManager.train\u001b[1;34m(self, num_iters, print_every, maxtime, wandb_log)\u001b[0m\n\u001b[0;32m   1391\u001b[0m loss_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# for wandb\u001b[39;00m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iters):\n\u001b[1;32m-> 1394\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wandb_log:\n\u001b[0;32m   1397\u001b[0m         loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m step_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[1;32mIn[13], line 53\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m     48\u001b[0m     managed_objects[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m scheduler\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Create a training manager with the specified training and testing functions\u001b[39;00m\n\u001b[0;32m     51\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MyTrainingManager(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Training parameters\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     training_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m _: \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGradScaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     63\u001b[0m     save_every\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mc\u001b[38;5;241m.\u001b[39mgetint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_every\u001b[39m\u001b[38;5;124m\"\u001b[39m, fallback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m),\n\u001b[0;32m     64\u001b[0m     load_path\u001b[38;5;241m=\u001b[39mload_path,\n\u001b[0;32m     65\u001b[0m     save_path\u001b[38;5;241m=\u001b[39moutput_path,\n\u001b[0;32m     66\u001b[0m     managed_objects\u001b[38;5;241m=\u001b[39munet\u001b[38;5;241m.\u001b[39mmanaged_objects(managed_objects),\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Testing parameters\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     test_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m _: test_function(\n\u001b[0;32m     69\u001b[0m         network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[0;32m     70\u001b[0m         device\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m     71\u001b[0m         criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m     72\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m     73\u001b[0m         testing_dataset\u001b[38;5;241m=\u001b[39mtesting_dataset,\n\u001b[0;32m     74\u001b[0m         training_name\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mrun_name,\n\u001b[0;32m     75\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39mpreds_output_dir,\n\u001b[0;32m     76\u001b[0m         training_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     77\u001b[0m     ),\n\u001b[0;32m     78\u001b[0m     test_every\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mc\u001b[38;5;241m.\u001b[39mgetint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_every\u001b[39m\u001b[38;5;124m\"\u001b[39m, fallback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m),\n\u001b[0;32m     79\u001b[0m     plot_every\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mc\u001b[38;5;241m.\u001b[39mgetint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_every\u001b[39m\u001b[38;5;124m\"\u001b[39m, fallback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m),\n\u001b[0;32m     80\u001b[0m     summary_writer\u001b[38;5;241m=\u001b[39msummary_writer,\n\u001b[0;32m     81\u001b[0m )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Load the model if a specific epoch is provided\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params\u001b[38;5;241m.\u001b[39mload_epoch \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\prisc\\Code\\sparks_project\\patches_experiments\\..\\utils\\training_inference_tools.py:115\u001b[0m, in \u001b[0;36mtraining_step\u001b[1;34m(dataset_loader, params, sampler, network, optimizer, criterion, scaler, scheduler)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:  \u001b[38;5;66;03m# [b, d, d, d] -> [b, 1, d, d, d]\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 115\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [b, 4, d, d, d] or [b, 4, d, d]\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Handle specific loss functions\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(criterion, MySoftDiceLoss):\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# Set regions in pred and y where the label is ignored to 0\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:169\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m    171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prisc\\Code\\sparks_project\\patches_experiments\\..\\models\\architectures.py:46\u001b[0m, in \u001b[0;36mUNetPadWrapper.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m     41\u001b[0m     x,\n\u001b[0;32m     42\u001b[0m     (w_pad \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, w_pad \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m w_pad \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m, h_pad \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, h_pad \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m h_pad \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Apply the forward pass:\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Remove the padding:\u001b[39;00m\n\u001b[0;32m     49\u001b[0m crop_h_start \u001b[38;5;241m=\u001b[39m h_pad \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\prisc\\Code\\sparks_project\\patches_experiments\\..\\models\\UNet\\unet\\network.py:329\u001b[0m, in \u001b[0;36mUNetClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 329\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogsoftmax(x)\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\prisc\\Code\\sparks_project\\patches_experiments\\..\\models\\UNet\\unet\\network.py:326\u001b[0m, in \u001b[0;36mUNetClassifier.linear_output\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlinear_output\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prisc\\Code\\sparks_project\\patches_experiments\\..\\models\\UNet\\unet\\network.py:289\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x, return_feature_maps)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m unet_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_path[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    288\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pool(x)\n\u001b[1;32m--> 289\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43munet_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m     down_outputs\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m    292\u001b[0m feature_maps\u001b[38;5;241m.\u001b[39mextend(down_outputs)\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prisc\\Code\\sparks_project\\patches_experiments\\..\\models\\UNet\\unet\\network.py:198\u001b[0m, in \u001b[0;36mUNetLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\nn\\modules\\conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\nn\\modules\\conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    598\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    599\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    607\u001b[0m     )\n\u001b[1;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the network in training mode\n",
    "network.train()\n",
    "\n",
    "# Train the model for the specified number of epochs\n",
    "logger.info(\"Starting training\")\n",
    "trainer.train(\n",
    "    params.train_epochs,\n",
    "    print_every=params.c.getint(\"training\", \"print_every\", fallback=100),\n",
    "    wandb_log=params.wandb_log,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting final validation\")\n",
    "# Run the final validation/testing procedure\n",
    "# trainer.run_validation(wandb_log=params.wandb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the summary writer\n",
    "summary_writer.close()\n",
    "\n",
    "# Close the wandb run\n",
    "if params.wandb_log:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
