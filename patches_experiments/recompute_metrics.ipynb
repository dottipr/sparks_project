{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-compute metrics of given training name at saved test iterations\n",
    "\n",
    "**Author**: Prisca Dotti\n",
    "\n",
    "**Last Edit**: 18.06.2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload is used to reload modules automatically before entering the\n",
    "# execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# To import modules from parent directory in Jupyter Notebook\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import TrainingConfig, config\n",
    "from data.datasets import PatchSparksDataset\n",
    "from utils.training_script_utils import (\n",
    "    init_model,\n",
    "    init_criterion,\n",
    ")\n",
    "from utils.training_inference_tools import test_function_patches\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get training-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:45:17] [  INFO  ] [   config   ] <318 > -- Loading C:\\Users\\prisc\\Code\\sparks_project\\config_files\\patches\\config_sparks_64x64x64.ini\n",
      "[16:45:17] [ ERROR  ] [wandb.jupyter] <224 > -- Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdottip\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\prisc\\Code\\sparks_project\\patches_experiments\\wandb\\run-20240625_164520-sparks_patches_64x64x64_no_ignore_frames</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dottip/patches/runs/sparks_patches_64x64x64_no_ignore_frames/workspace' target=\"_blank\">sparks_patches_64x64x64_no_ignore_frames</a></strong> to <a href='https://wandb.ai/dottip/patches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dottip/patches' target=\"_blank\">https://wandb.ai/dottip/patches</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dottip/patches/runs/sparks_patches_64x64x64_no_ignore_frames/workspace' target=\"_blank\">https://wandb.ai/dottip/patches/runs/sparks_patches_64x64x64_no_ignore_frames/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:45:30] [  INFO  ] [  __main__  ] < 33 > -- Processing training 'sparks_patches_64x64x64_no_ignore_frames'...\n",
      "[16:45:30] [  INFO  ] [  __main__  ] < 34 > -- Predicting outputs for samples ['05', '10', '15', '20', '25', '32', '34', '40', '45'].\n",
      "[16:45:30] [  INFO  ] [  __main__  ] < 35 > -- Using C:\\Users\\prisc\\Code\\sparks_project\\data\\sparks_dataset as dataset root path.\n",
      "[16:45:30] [  INFO  ] [   config   ] <566 > -- Using cuda\n"
     ]
    }
   ],
   "source": [
    "run_name = \"sparks_patches_64x64x64_no_ignore_frames\"\n",
    "config_filename = os.path.join(\"config_files\", \"patches\", \"config_sparks_64x64x64.ini\")\n",
    "\n",
    "use_train_data = False\n",
    "test_ids = [\n",
    "    \"05\",\n",
    "    \"10\",\n",
    "    \"15\",\n",
    "    \"20\",\n",
    "    \"25\",\n",
    "    \"32\",\n",
    "    \"34\",\n",
    "    \"40\",\n",
    "    \"45\",\n",
    "]\n",
    "\n",
    "params = TrainingConfig(training_config_file=config_filename)\n",
    "params.run_name = run_name\n",
    "\n",
    "# trained_epochs = params.train_epochs\n",
    "trained_epochs = 45000\n",
    "saved_every = params.c.getint(\"training\", \"save_every\", fallback=1000)\n",
    "\n",
    "models_dir = os.path.realpath(\n",
    "    os.path.join(\n",
    "        config.basedir,\n",
    "        \"models\",\n",
    "        \"saved_models\",\n",
    "        params.run_name,\n",
    "    )\n",
    ")\n",
    "\n",
    "logger.info(f\"Processing training '{params.run_name}'...\")\n",
    "logger.info(f\"Predicting outputs for samples {test_ids}.\")\n",
    "logger.info(f\"Using {params.dataset_dir} as dataset root path.\")\n",
    "\n",
    "params.set_device(device=\"auto\")  # can also be set to \"cpu\" or \"cuda\"\n",
    "params.display_device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:45:31] [WARNING ] [tifffile.tifffile] <16549> -- TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n",
      "[16:45:31] [WARNING ] [tifffile.tifffile] <16549> -- TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n",
      "[16:46:32] [  INFO  ] [  __main__  ] < 9  > -- Samples in dataset (patches): 126\n"
     ]
    }
   ],
   "source": [
    "# Initialize training dataset\n",
    "dataset = PatchSparksDataset(\n",
    "    params=params,\n",
    "    base_path=params.dataset_dir,\n",
    "    sample_ids=test_ids,\n",
    "    load_instances=True,  # this is needed to detect patches wrt spark peaks\n",
    "    inference=None,\n",
    ")\n",
    "logger.info(f\"Samples in dataset (patches): {len(dataset)}\")\n",
    "\n",
    "# Create a dataloader\n",
    "dataset_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=params.inference_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=params.num_workers,\n",
    "    pin_memory=params.pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the UNet model\n",
    "network = init_model(params=params)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "if params.device.type != \"cpu\":\n",
    "    network = nn.DataParallel(network).to(params.device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load UNet models and compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"DELETEME\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:46:33] [  INFO  ] [  __main__  ] < 28 > -- Computing metrics for epoch 0...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing metrics for epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m network\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 30\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_patches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_criterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtesting_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m, val \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     42\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prisc\\Code\\sparks_project\\patches_experiments\\..\\utils\\training_inference_tools.py:1304\u001b[0m, in \u001b[0;36mtest_function_patches\u001b[1;34m(network, device, criterion, params, testing_dataset, training_name, output_dir, training_mode)\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;66;03m# Get predicted segmentation and event instances\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m raw_pred_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1300\u001b[0m     event_type: raw_pred[config\u001b[38;5;241m.\u001b[39mclasses_dict[event_type]]\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event_type \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mevent_types\n\u001b[0;32m   1302\u001b[0m }\n\u001b[1;32m-> 1304\u001b[0m preds_instances, preds_segmentation, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_raw_predictions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_preds_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_pred_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_movie\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_instances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_instances_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime to process predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;66;03m############### Compute pairwise scores (based on IoMin) ###############\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prisc\\Code\\sparks_project\\patches_experiments\\..\\data\\data_processing_tools.py:1119\u001b[0m, in \u001b[0;36mprocess_raw_predictions\u001b[1;34m(raw_preds_dict, input_movie, training_mode, fill_holes, compute_instances)\u001b[0m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_instances:\n\u001b[0;32m   1117\u001b[0m     \u001b[38;5;66;03m# Separate events in predictions\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeparating events in predictions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1119\u001b[0m     preds_instances_dict, coords_events \u001b[38;5;241m=\u001b[39m \u001b[43mget_separated_events\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43margmax_preds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreds_classes_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmovie\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_movie\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;66;03m# If there are no predicted events don't do unnecessary processing\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(\u001b[38;5;28mlist\u001b[39m(preds_instances_dict\u001b[38;5;241m.\u001b[39mvalues())):\n",
      "File \u001b[1;32mc:\\Users\\prisc\\Code\\sparks_project\\patches_experiments\\..\\data\\data_processing_tools.py:728\u001b[0m, in \u001b[0;36mget_separated_events\u001b[1;34m(argmax_preds, movie, training_mode, watershed_classes)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# Compute watershed separation\u001b[39;00m\n\u001b[0;32m    727\u001b[0m markers \u001b[38;5;241m=\u001b[39m label(mask_loc)\n\u001b[1;32m--> 728\u001b[0m split_event_mask \u001b[38;5;241m=\u001b[39m \u001b[43mwatershed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43msmooth_xs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmarkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margmax_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mevent_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconnectivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompactness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m training_mode:\n\u001b[0;32m    737\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot in training mode, computing missing peaks...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\skimage\\segmentation\\_watershed.py:221\u001b[0m, in \u001b[0;36mwatershed\u001b[1;34m(image, markers, connectivity, offset, mask, compactness, watershed_line)\u001b[0m\n\u001b[0;32m    218\u001b[0m marker_locations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mflatnonzero(output)\n\u001b[0;32m    219\u001b[0m image_strides \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image\u001b[38;5;241m.\u001b[39mstrides, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m image\u001b[38;5;241m.\u001b[39mitemsize\n\u001b[1;32m--> 221\u001b[0m \u001b[43m_watershed_cy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwatershed_raveled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mmarker_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_neighborhood\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_strides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompactness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m                                \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mwatershed_line\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m output \u001b[38;5;241m=\u001b[39m crop(output, pad_width, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load_epochs = np.arange(0, trained_epochs, saved_every)  # all epochs must be saved\n",
    "for load_epoch in load_epochs:\n",
    "    if load_epoch != 0:\n",
    "        logger.info(f\"Loading trained model '{run_name}' at epoch {load_epoch}...\")\n",
    "\n",
    "        # Path to the saved model checkpoint\n",
    "        model_dir = os.path.join(models_dir, f\"network_{load_epoch:06d}.pth\")\n",
    "        try:\n",
    "            network.load_state_dict(torch.load(model_dir, map_location=params.device))\n",
    "        except RuntimeError as e:\n",
    "            if \"module\" in str(e):\n",
    "                # The error message contains \"module,\" so handle the DataParallel loading\n",
    "                logger.warning(\n",
    "                    \"Failed to load the model, as it was trained with DataParallel. Wrapping it in DataParallel and retrying...\"\n",
    "                )\n",
    "                # Get current device of the object (model)\n",
    "                temp_device = next(iter(network.parameters())).device\n",
    "\n",
    "                network = nn.DataParallel(network)\n",
    "                network.load_state_dict(\n",
    "                    torch.load(model_dir, map_location=params.device)\n",
    "                )\n",
    "\n",
    "                logger.info(\n",
    "                    \"Network should be on CPU, removing DataParallel wrapper...\"\n",
    "                )\n",
    "                network = network.module.to(temp_device)\n",
    "            else:\n",
    "                # Handle other exceptions or re-raise the exception if it's unrelated\n",
    "                raise\n",
    "\n",
    "    logger.info(f\"Computing metrics for epoch {load_epoch}...\")\n",
    "    network.eval()\n",
    "    res = test_function_patches(\n",
    "        network=network,\n",
    "        device=params.device,\n",
    "        criterion=init_criterion(params=params, dataset=dataset),\n",
    "        params=params,\n",
    "        testing_dataset=dataset,\n",
    "        training_name=params.run_name,\n",
    "        output_dir=output_dir,\n",
    "        training_mode=True,\n",
    "    )\n",
    "\n",
    "    for m, val in res.items():\n",
    "        logger.info(f\"{m}: {val}\")\n",
    "        if \"confusion_matrix\" not in m:\n",
    "            wandb.log({m: val}, step=saved_every, commit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8111942cb0344619d80f8ad91a96f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average/correctly_classified</td><td>▁</td></tr><tr><td>average/detected</td><td>▁</td></tr><tr><td>average/f1-score</td><td>▁</td></tr><tr><td>average/labeled</td><td>▁</td></tr><tr><td>average/precision</td><td>▁</td></tr><tr><td>average/recall</td><td>▁</td></tr><tr><td>segmentation/average_IoU</td><td>▁</td></tr><tr><td>segmentation/sparks_IoU</td><td>▁</td></tr><tr><td>sparks/correctly_classified</td><td>▁</td></tr><tr><td>sparks/detected</td><td>▁</td></tr><tr><td>sparks/f1-score</td><td>▁</td></tr><tr><td>sparks/labeled</td><td>▁</td></tr><tr><td>sparks/precision</td><td>▁</td></tr><tr><td>sparks/recall</td><td>▁</td></tr><tr><td>total/correctly_classified</td><td>▁</td></tr><tr><td>total/detected</td><td>▁</td></tr><tr><td>total/f1-score</td><td>▁</td></tr><tr><td>total/labeled</td><td>▁</td></tr><tr><td>total/precision</td><td>▁</td></tr><tr><td>total/recall</td><td>▁</td></tr><tr><td>validation_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average/correctly_classified</td><td>1.0</td></tr><tr><td>average/detected</td><td>0.32685</td></tr><tr><td>average/f1-score</td><td>0.35462</td></tr><tr><td>average/labeled</td><td>0.38756</td></tr><tr><td>average/precision</td><td>0.38756</td></tr><tr><td>average/recall</td><td>0.32685</td></tr><tr><td>segmentation/average_IoU</td><td>0.17023</td></tr><tr><td>segmentation/sparks_IoU</td><td>0.17023</td></tr><tr><td>sparks/correctly_classified</td><td>1.0</td></tr><tr><td>sparks/detected</td><td>0.32685</td></tr><tr><td>sparks/f1-score</td><td>0.35462</td></tr><tr><td>sparks/labeled</td><td>0.38756</td></tr><tr><td>sparks/precision</td><td>0.38756</td></tr><tr><td>sparks/recall</td><td>0.32685</td></tr><tr><td>total/correctly_classified</td><td>1.0</td></tr><tr><td>total/detected</td><td>0.32685</td></tr><tr><td>total/f1-score</td><td>0.35462</td></tr><tr><td>total/labeled</td><td>0.38756</td></tr><tr><td>total/precision</td><td>0.38756</td></tr><tr><td>total/recall</td><td>0.32685</td></tr><tr><td>validation_loss</td><td>0.44997</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sparks_patches_64x64x64_no_ignore_frames</strong> at: <a href='https://wandb.ai/dottip/patches/runs/sparks_patches_64x64x64_no_ignore_frames/workspace' target=\"_blank\">https://wandb.ai/dottip/patches/runs/sparks_patches_64x64x64_no_ignore_frames/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240625_130336-sparks_patches_64x64x64_no_ignore_frames\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Close the wandb run\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mparams\u001b[49m\u001b[38;5;241m.\u001b[39mwandb_log:\n\u001b[0;32m      3\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "# Close the wandb run\n",
    "if params.wandb_log:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
