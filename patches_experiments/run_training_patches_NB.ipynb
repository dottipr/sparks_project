{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on Patches of Confocal tyx-Image Series\n",
    "\n",
    "**Author**: Prisca Dotti\n",
    "\n",
    "**Last Edit**: 18.06.2024\n",
    "\n",
    "This notebook is used to run some experiments using the U-Net model on patches obtained from either real or fake confocal imaging data.\n",
    "\n",
    "The dataset used for training could be one of the following:\n",
    "- dataset of patches extracted in a meaningful way from confocal imaging recordings\n",
    "- dataset of recordings from which patches are extracted when processing them in the U-Net for memory management reasons and which are recombined as whole movies after inference\n",
    "- dataset of simulated confocal imaging patches --> this is similar to what I've been doing so far\n",
    "- a combination of real and fake patches of confocal imaging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload is used to reload modules automatically before entering the\n",
    "# execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# To import modules from parent directory in Jupyter Notebook\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn, optim\n",
    "\n",
    "# from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from config import TrainingConfig, config\n",
    "from data.datasets import PatchCaEventsDataset, PatchSparksDataset\n",
    "from models.UNet import unet\n",
    "from utils.training_inference_tools import (\n",
    "    MyTrainingManager,\n",
    "    TransformedSparkDataset,\n",
    "    random_flip,\n",
    "    random_flip_noise,\n",
    "    sampler,\n",
    "    test_function_patches,\n",
    "    training_step,\n",
    "    weights_init,\n",
    ")\n",
    "from utils.training_script_utils import (\n",
    "    get_sample_ids,\n",
    "    init_config_file_path,\n",
    "    init_criterion,\n",
    "    init_dataset,\n",
    "    init_model,\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:08:20] [  INFO  ] [   config   ] <318 > -- Loading C:\\Users\\prisc\\Code\\sparks_project\\config_files\\config_sparks_patches.ini\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --     training_config_file: C:\\Users\\prisc\\Code\\sparks_project\\config_files\\config_sparks_patches.ini\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --              dataset_dir: C:\\Users\\prisc\\Code\\sparks_project\\data\\sparks_dataset\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --                        c: <configparser.ConfigParser object at 0x000001F5986204C0>\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --                 run_name: sparks_patches\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --            load_run_name: \n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --               load_epoch: 0\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --             train_epochs: 100000\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --                criterion: lovasz_softmax\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --                 lr_start: 0.0001\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --       ignore_frames_loss: 6\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --                     cuda: True\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --                scheduler: None\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --                optimizer: adam\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --             dataset_size: minimal\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --               batch_size: 64\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --              num_workers: 0\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --            data_duration: 64\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --              data_stride: 32\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --               patch_size: [32, 32, 32]\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --           data_smoothing: no\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --               norm_video: abs_max\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --        remove_background: no\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --       mask_cell_exterior: False\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --  noise_data_augmentation: False\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --              sparks_type: raw\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --                  new_fps: 0\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --             sin_channels: False\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --           n_sin_channels: [0, 0, 0]\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --  inference_data_duration: 256\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --    inference_data_stride: 32\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --                inference: patches\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --     inference_load_epoch: 100000\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --     inference_batch_size: 32\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --   inference_dataset_size: full\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --          nn_architecture: pablos_unet\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --               unet_steps: 4\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --     first_layer_channels: 8\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --             num_channels: 1\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --                 dilation: 1\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --              border_mode: same\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --      batch_normalization: none\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --       temporal_reduction: False\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --       initialize_weights: False\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --                wandb_log: False\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --                   device: cuda\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --               pin_memory: True\n",
      "[12:08:21] [  INFO  ] [   config   ] <570 > --                   n_gpus: 1\n"
     ]
    }
   ],
   "source": [
    "##################### Get training-specific parameters #####################\n",
    "\n",
    "# Initialize training-specific parameters\n",
    "# (get the configuration file path from ArgParse)\n",
    "config_filename = os.path.join(\"config_files\", \"config_sparks_patches.ini\")\n",
    "# config_filename = os.path.join(\"config_files\", \"config_final_model.ini\")\n",
    "\n",
    "params = TrainingConfig(training_config_file=config_filename)\n",
    "\n",
    "# Print parameters to console if needed\n",
    "params.print_params()\n",
    "\n",
    "######################### Initialize random seeds ##########################\n",
    "\n",
    "# We used these random seeds to ensure reproducibility of the results\n",
    "\n",
    "torch.manual_seed(0)  # <--------------------------------------------------!\n",
    "random.seed(0)  # <--------------------------------------------------------!\n",
    "np.random.seed(0)  # <-----------------------------------------------------!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:08:21] [  INFO  ] [   config   ] <566 > -- Using cuda\n"
     ]
    }
   ],
   "source": [
    "# params.set_device(\"cpu\")\n",
    "params.display_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:08:25] [  INFO  ] [  __main__  ] < 27 > -- Samples in dataset (patches): 12\n",
      "[12:08:25] [WARNING ] [tifffile.tifffile] <16549> -- TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n",
      "[12:08:25] [WARNING ] [tifffile.tifffile] <16549> -- TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n",
      "[12:08:31] [  INFO  ] [  __main__  ] < 38 > -- Samples in test dataset (patches): 12\n"
     ]
    }
   ],
   "source": [
    "############################ Configure datasets ############################\n",
    "\n",
    "# Select samples for training and testing based on dataset size\n",
    "train_sample_ids = get_sample_ids(\n",
    "    train_data=True,\n",
    "    dataset_size=params.dataset_size,\n",
    ")\n",
    "test_sample_ids = get_sample_ids(\n",
    "    train_data=False,\n",
    "    dataset_size=params.dataset_size,\n",
    ")\n",
    "\n",
    "# Initialize training dataset\n",
    "dataset = PatchSparksDataset(\n",
    "    params=params,\n",
    "    base_path=params.dataset_dir,\n",
    "    sample_ids=train_sample_ids,\n",
    "    load_instances=True,  # this is needed to detect patches wrt spark peaks\n",
    "    inference=None,\n",
    ")\n",
    "\n",
    "# Apply transforms based on noise_data_augmentation setting\n",
    "# (transforms are applied when getting a sample from the dataset)\n",
    "transforms = random_flip_noise if params.noise_data_augmentation else random_flip\n",
    "dataset = TransformedSparkDataset(dataset, transforms)\n",
    "\n",
    "logger.info(f\"Samples in dataset (patches): {len(dataset)}\")\n",
    "\n",
    "# Initialize testing datasets\n",
    "testing_dataset = PatchSparksDataset(\n",
    "    params=params,\n",
    "    base_path=params.dataset_dir,\n",
    "    sample_ids=test_sample_ids,\n",
    "    load_instances=True,  # this is needed to detect patches wrt spark peaks\n",
    "    inference=None,\n",
    ")\n",
    "\n",
    "logger.info(f\"Samples in test dataset (patches): {len(testing_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import napari\n",
    "\n",
    "# viewer = napari.Viewer()\n",
    "\n",
    "# train_sample_id = 9\n",
    "# train_sample = dataset[train_sample_id]\n",
    "# train_data = train_sample[\"data\"].numpy()\n",
    "# train_labels = train_sample[\"labels\"].numpy()\n",
    "# viewer.add_image(train_data)\n",
    "# viewer.add_labels(train_labels)\n",
    "\n",
    "# test_sample_id = 0\n",
    "# test_sample = testing_dataset[test_sample_id]\n",
    "# test_data = test_sample[\"data\"].numpy()\n",
    "# test_labels = test_sample[\"labels\"].numpy()\n",
    "# viewer.add_image(test_data)\n",
    "# viewer.add_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loaders\n",
    "dataset_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=params.batch_size,\n",
    "    num_workers=params.num_workers,\n",
    "    pin_memory=params.pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Configure UNet ##############################\n",
    "\n",
    "# Initialize the UNet model\n",
    "network = init_model(params=params)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "if params.device.type != \"cpu\":\n",
    "    network = nn.DataParallel(network).to(params.device, non_blocking=True)\n",
    "    # cudnn.benchmark = True\n",
    "\n",
    "# Watch the model with wandb for logging if enabled\n",
    "if params.wandb_log:\n",
    "    wandb.watch(network)\n",
    "\n",
    "# Initialize UNet weights if required\n",
    "if params.initialize_weights:\n",
    "    logger.info(\"Initializing UNet weights...\")\n",
    "    network.apply(weights_init)\n",
    "\n",
    "# The following line is commented as it does not work on Windows\n",
    "# torch.compile(network, mode=\"default\", backend=\"inductor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:08:32] [  INFO  ] [  __main__  ] < 26 > -- Output directory: C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\sparks_patches\n"
     ]
    }
   ],
   "source": [
    "########################### Initialize training ############################\n",
    "\n",
    "# Initialize the optimizer based on the specified type\n",
    "if params.optimizer == \"adam\":\n",
    "    optimizer = optim.Adam(network.parameters(), lr=params.lr_start)\n",
    "elif params.optimizer == \"adadelta\":\n",
    "    optimizer = optim.Adadelta(network.parameters(), lr=params.lr_start)\n",
    "elif params.optimizer == \"sgd\":\n",
    "    optimizer = optim.SGD(network.parameters(), lr=params.lr_start)\n",
    "else:\n",
    "    logger.error(f\"{params.optimizer} is not a valid optimizer.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the learning rate scheduler if specified\n",
    "if params.scheduler == \"step\":\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=params.scheduler_step_size,\n",
    "        gamma=params.scheduler_gamma,\n",
    "    )\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "# Define the output directory path\n",
    "output_path = os.path.join(config.output_dir, params.run_name)\n",
    "logger.info(f\"Output directory: {os.path.realpath(output_path)}\")\n",
    "\n",
    "# Initialize the summary writer for TensorBoard logging\n",
    "summary_writer = SummaryWriter(os.path.join(output_path, \"summary\"), purge_step=0)\n",
    "\n",
    "# Check if a pre-trained model should be loaded\n",
    "if params.load_run_name != \"\":\n",
    "    load_path = os.path.join(config.output_dir, params.load_run_name)\n",
    "    logger.info(f\"Model loaded from directory: {os.path.realpath(load_path)}\")\n",
    "else:\n",
    "    load_path = None\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = init_criterion(params=params, dataset=dataset)\n",
    "\n",
    "# Create a directory to save predicted class movies\n",
    "preds_output_dir = os.path.join(output_path, \"predictions\")\n",
    "os.makedirs(preds_output_dir, exist_ok=True)\n",
    "\n",
    "# Create a dictionary of managed objects\n",
    "managed_objects = {\"network\": network, \"optimizer\": optimizer}\n",
    "if scheduler is not None:\n",
    "    managed_objects[\"scheduler\"] = scheduler\n",
    "\n",
    "# Create a training manager with the specified training and testing functions\n",
    "trainer = MyTrainingManager(\n",
    "    # Training parameters\n",
    "    training_step=lambda _: training_step(\n",
    "        dataset_loader=dataset_loader,\n",
    "        params=params,\n",
    "        sampler=sampler,\n",
    "        network=network,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        scheduler=scheduler,\n",
    "        # scaler=GradScaler(),\n",
    "    ),\n",
    "    save_every=params.c.getint(\"training\", \"save_every\", fallback=5000),\n",
    "    load_path=load_path,\n",
    "    save_path=output_path,\n",
    "    managed_objects=unet.managed_objects(managed_objects),\n",
    "    # Testing parameters\n",
    "    test_function=lambda _: test_function_patches(\n",
    "        network=network,\n",
    "        device=params.device,\n",
    "        criterion=criterion,\n",
    "        params=params,\n",
    "        testing_dataset=testing_dataset,\n",
    "        training_name=params.run_name,\n",
    "        output_dir=preds_output_dir,\n",
    "        training_mode=True,\n",
    "    ),\n",
    "    test_every=params.c.getint(\"training\", \"test_every\", fallback=1000),\n",
    "    plot_every=params.c.getint(\"training\", \"test_every\", fallback=1000),\n",
    "    summary_writer=summary_writer,\n",
    ")\n",
    "\n",
    "# Load the model if a specific epoch is provided\n",
    "if params.load_epoch != 0:\n",
    "    trainer.load(params.load_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the network before training if resuming from a checkpoint\n",
    "# if params.load_epoch > 0:\n",
    "#     logger.info(\"Validate network before training\")\n",
    "#     trainer.run_validation(wandb_log=params.wandb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the network in training mode\n",
    "network.train()\n",
    "\n",
    "# Train the model for the specified number of epochs\n",
    "logger.info(\"Starting training\")\n",
    "trainer.train(\n",
    "    params.train_epochs,\n",
    "    print_every=params.c.getint(\"training\", \"print_every\", fallback=100),\n",
    "    wandb_log=params.wandb_log,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:16:01] [  INFO  ] [  __main__  ] < 1  > -- Starting final validation\n",
      "[12:16:01] [  INFO  ] [utils.training_inference_tools] <1495> -- Validating network at iteration 0...\n",
      "[12:16:01] [ DEBUG  ] [utils.training_inference_tools] <1205> -- Test function: running samples in UNet\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1233> -- Time to run testing dataset in UNet: 0.93 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1235> -- Test function: computing loss\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1264> -- Processing patch 0/12\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1273> -- Test function: saving raw predictions on disk (C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\sparks_patches\\predictions)\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1288> -- Test function: re-organising annotations\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1298> -- Time to re-organise annotations: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1303> -- Test function: getting processed output (segmentation and instances)\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1320> -- Time to process predictions: 0.10 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1340> -- Test function: computing pairwise scores between 1 annotated events and 4 predicted events\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1353> -- Time to compute pairwise scores: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1360> -- Test function: getting matches summary\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1382> -- Time to get matches summary: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1264> -- Processing patch 1/12\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1273> -- Test function: saving raw predictions on disk (C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\sparks_patches\\predictions)\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1288> -- Test function: re-organising annotations\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1298> -- Time to re-organise annotations: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1303> -- Test function: getting processed output (segmentation and instances)\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1320> -- Time to process predictions: 0.10 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1340> -- Test function: computing pairwise scores between 2 annotated events and 3 predicted events\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1353> -- Time to compute pairwise scores: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1360> -- Test function: getting matches summary\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1382> -- Time to get matches summary: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1264> -- Processing patch 2/12\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1273> -- Test function: saving raw predictions on disk (C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\sparks_patches\\predictions)\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1288> -- Test function: re-organising annotations\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1298> -- Time to re-organise annotations: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1303> -- Test function: getting processed output (segmentation and instances)\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1320> -- Time to process predictions: 0.12 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1340> -- Test function: computing pairwise scores between 1 annotated events and 2 predicted events\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1353> -- Time to compute pairwise scores: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1360> -- Test function: getting matches summary\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1382> -- Time to get matches summary: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1264> -- Processing patch 3/12\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1273> -- Test function: saving raw predictions on disk (C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\sparks_patches\\predictions)\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1288> -- Test function: re-organising annotations\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1298> -- Time to re-organise annotations: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1303> -- Test function: getting processed output (segmentation and instances)\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1320> -- Time to process predictions: 0.10 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1340> -- Test function: computing pairwise scores between 1 annotated events and 4 predicted events\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1353> -- Time to compute pairwise scores: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1360> -- Test function: getting matches summary\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1382> -- Time to get matches summary: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1264> -- Processing patch 4/12\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1273> -- Test function: saving raw predictions on disk (C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\sparks_patches\\predictions)\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1288> -- Test function: re-organising annotations\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1298> -- Time to re-organise annotations: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1303> -- Test function: getting processed output (segmentation and instances)\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1320> -- Time to process predictions: 0.09 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1340> -- Test function: computing pairwise scores between 2 annotated events and 3 predicted events\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1353> -- Time to compute pairwise scores: 0.00 s\n",
      "[12:16:02] [ DEBUG  ] [utils.training_inference_tools] <1360> -- Test function: getting matches summary\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1382> -- Time to get matches summary: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1264> -- Processing patch 5/12\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1273> -- Test function: saving raw predictions on disk (C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\sparks_patches\\predictions)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1288> -- Test function: re-organising annotations\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1298> -- Time to re-organise annotations: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1303> -- Test function: getting processed output (segmentation and instances)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1320> -- Time to process predictions: 0.10 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1340> -- Test function: computing pairwise scores between 1 annotated events and 3 predicted events\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1353> -- Time to compute pairwise scores: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1360> -- Test function: getting matches summary\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1382> -- Time to get matches summary: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1264> -- Processing patch 6/12\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1273> -- Test function: saving raw predictions on disk (C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\sparks_patches\\predictions)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1288> -- Test function: re-organising annotations\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1298> -- Time to re-organise annotations: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1303> -- Test function: getting processed output (segmentation and instances)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1320> -- Time to process predictions: 0.16 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1340> -- Test function: computing pairwise scores between 1 annotated events and 2 predicted events\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1353> -- Time to compute pairwise scores: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1360> -- Test function: getting matches summary\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1382> -- Time to get matches summary: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1264> -- Processing patch 7/12\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1273> -- Test function: saving raw predictions on disk (C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\sparks_patches\\predictions)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1288> -- Test function: re-organising annotations\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1298> -- Time to re-organise annotations: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1303> -- Test function: getting processed output (segmentation and instances)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1320> -- Time to process predictions: 0.11 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1340> -- Test function: computing pairwise scores between 1 annotated events and 2 predicted events\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1353> -- Time to compute pairwise scores: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1360> -- Test function: getting matches summary\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1382> -- Time to get matches summary: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1264> -- Processing patch 8/12\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1273> -- Test function: saving raw predictions on disk (C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\sparks_patches\\predictions)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1288> -- Test function: re-organising annotations\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1298> -- Time to re-organise annotations: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1303> -- Test function: getting processed output (segmentation and instances)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1320> -- Time to process predictions: 0.11 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1340> -- Test function: computing pairwise scores between 1 annotated events and 7 predicted events\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1353> -- Time to compute pairwise scores: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1360> -- Test function: getting matches summary\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1382> -- Time to get matches summary: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1264> -- Processing patch 9/12\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1273> -- Test function: saving raw predictions on disk (C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\sparks_patches\\predictions)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1288> -- Test function: re-organising annotations\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1298> -- Time to re-organise annotations: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1303> -- Test function: getting processed output (segmentation and instances)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1320> -- Time to process predictions: 0.11 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1340> -- Test function: computing pairwise scores between 1 annotated events and 4 predicted events\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1353> -- Time to compute pairwise scores: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1360> -- Test function: getting matches summary\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1382> -- Time to get matches summary: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1264> -- Processing patch 10/12\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1273> -- Test function: saving raw predictions on disk (C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\sparks_patches\\predictions)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1288> -- Test function: re-organising annotations\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1298> -- Time to re-organise annotations: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1303> -- Test function: getting processed output (segmentation and instances)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1320> -- Time to process predictions: 0.10 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1340> -- Test function: computing pairwise scores between 1 annotated events and 5 predicted events\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1353> -- Time to compute pairwise scores: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1360> -- Test function: getting matches summary\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1382> -- Time to get matches summary: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1264> -- Processing patch 11/12\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1273> -- Test function: saving raw predictions on disk (C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\sparks_patches\\predictions)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1288> -- Test function: re-organising annotations\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1298> -- Time to re-organise annotations: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1303> -- Test function: getting processed output (segmentation and instances)\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1320> -- Time to process predictions: 0.10 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1340> -- Test function: computing pairwise scores between 1 annotated events and 1 predicted events\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1353> -- Time to compute pairwise scores: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1360> -- Test function: getting matches summary\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1382> -- Time to get matches summary: 0.00 s\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1395> -- Test function: reducing metrics\n",
      "[12:16:03] [ DEBUG  ] [utils.training_inference_tools] <1471> -- Time to reduce metrics: 0.02 s\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1598> -- Metrics:\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \tvalidation_loss: 0.5012\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \tsparks/precision: 0.35\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \tsparks/recall: 1\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \tsparks/correctly_classified: 1\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \tsparks/detected: 1\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \tsparks/f1-score: 0.5185\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \tsparks/labeled: 0.35\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \ttotal/precision: 0.35\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \ttotal/recall: 1\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \ttotal/correctly_classified: 1\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \ttotal/detected: 1\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \ttotal/f1-score: 0.5185\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \ttotal/labeled: 0.35\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \taverage/precision: 0.35\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \taverage/recall: 1\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \taverage/correctly_classified: 1\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \taverage/detected: 1\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \taverage/f1-score: 0.5185\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \taverage/labeled: 0.35\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \tsegmentation/sparks_IoU: 0.03527\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1604> -- \tsegmentation/average_IoU: 0.03527\n",
      "[12:16:03] [  INFO  ] [utils.training_inference_tools] <1602> -- \tsegmentation_confusion_matrix:\n",
      "[[ 36652 343773]\n",
      " [     0  12568]]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting final validation\")\n",
    "# Run the final validation/testing procedure\n",
    "trainer.run_validation(wandb_log=params.wandb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the summary writer\n",
    "summary_writer.close()\n",
    "\n",
    "# Close the wandb run\n",
    "if params.wandb_log:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
