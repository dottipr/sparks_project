{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06/10/2023\n",
    "\n",
    "Uso questo script per ricreare i datasets cercando di strutturarli meglio\n",
    "- dataset che prende movies e labels come inputs,\n",
    "- dataset che prende dataset_path e movie ids come inputs,\n",
    "- dataset che gestisce l'inference con o senza ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "import math\n",
    "import ntpath\n",
    "import os\n",
    "\n",
    "from typing import List, Dict, Union, Tuple, Any\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import convolve\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import GaussianBlur\n",
    "\n",
    "from config import config, TrainingConfig\n",
    "from data.data_processing_tools import detect_spark_peaks\n",
    "from utils.in_out_tools import load_annotations_ids, load_movies_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for spark detection.\n",
    "\n",
    "    Args:\n",
    "        params (TrainingConfig): A configuration object containing the\n",
    "            dataset parameters.\n",
    "        **kwargs: Additional keyword arguments to customize the dataset.\n",
    "\n",
    "    Keyword Args:\n",
    "        base_path (str): The base path to the dataset files on disk.\n",
    "        sample_ids (List[str]): A list of sample IDs to load from disk.\n",
    "        load_instances (bool): Whether to load instance data from disk.\n",
    "        movies (List[np.ndarray]): A list of numpy arrays containing the movie\n",
    "            data.\n",
    "        labels (List[np.ndarray]): A list of numpy arrays containing the ground\n",
    "            truth labels.\n",
    "        instances (List[np.ndarray]): A list of numpy arrays containing the\n",
    "            instance data.\n",
    "        stride (int): The stride to use when generating samples from the movie\n",
    "            data.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If neither `movies` nor `base_path` and `sample_ids` are\n",
    "        provided.\n",
    "\n",
    "    Attributes:\n",
    "        params (TrainingConfig): The configuration object containing the dataset\n",
    "            parameters.\n",
    "        window_size (int): The duration of each sample in frames.\n",
    "        stride (int): The stride to use when generating samples from the movie\n",
    "            data.\n",
    "        movies (List[torch.Tensor]): A list of PyTorch tensors containing the\n",
    "            movie data.\n",
    "        labels (List[torch.Tensor]): A list of PyTorch tensors containing the\n",
    "            ground truth labels.\n",
    "        instances (List[torch.Tensor]): A list of PyTorch tensors containing the\n",
    "            instance data.\n",
    "        gt_available (bool): Whether ground truth labels are available for the\n",
    "            dataset.\n",
    "        spark_peaks (List[Tuple[int, int]]): A list of tuples containing the\n",
    "            (t, y, x) coordinates of the spark peaks in each movie.\n",
    "        original_durations (List[int]): A list of the original durations of each\n",
    "            movie before padding.\n",
    "\n",
    "    Methods:\n",
    "        __len__(): Returns the total number of samples in the dataset.\n",
    "        __getitem__(idx: int) -> Dict[str, Any]: Returns a dictionary containing\n",
    "            the data, labels, and metadata for a given sample.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: TrainingConfig, **kwargs) -> None:\n",
    "        base_path: str = kwargs.get(\"base_path\", \"\")\n",
    "        sample_ids: List[str] = kwargs.get(\"sample_ids\", [])\n",
    "\n",
    "        movies: List[np.ndarray] = kwargs.get(\"movies\", [])\n",
    "\n",
    "        if base_path and sample_ids:\n",
    "            # Load data from disk if base_path and sample_ids are provided\n",
    "            self.base_path = base_path\n",
    "            self.sample_ids = sample_ids\n",
    "            load_instances: bool = kwargs.get(\"load_instances\", False)\n",
    "\n",
    "            ### Get video samples and ground truth ###\n",
    "            movies = self._get_movies()  # dict of numpy arrays\n",
    "            labels = self._get_labels()  # list of numpy arrays\n",
    "            instances = (\n",
    "                self._get_instances() if load_instances else []\n",
    "            )  # list of numpy arrays\n",
    "\n",
    "        elif movies:\n",
    "            # Otherwise, data is provided directly\n",
    "            labels: List[np.ndarray] = kwargs.get(\"labels\", [])\n",
    "            instances: List[np.ndarray] = kwargs.get(\"instances\", [])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Either movies or base_path and sample_ids must be provided.\"\n",
    "            )\n",
    "\n",
    "        # Store the dataset parameters\n",
    "        self.params = params\n",
    "        self.window_size = params.data_duration\n",
    "        self.stride: int = kwargs.get(\"stride\", 0) or params.data_stride\n",
    "\n",
    "        # Store the movies, labels and instances\n",
    "        self.movies = [torch.from_numpy(\n",
    "            movie.astype(np.int32)) for movie in movies]\n",
    "        self.labels = [torch.from_numpy(label.astype(np.int8))\n",
    "                       for label in labels]\n",
    "        self.instances = [\n",
    "            torch.from_numpy(instance.astype(np.int8)) for instance in instances\n",
    "        ]\n",
    "        self.gt_available = True if len(labels) == len(movies) else False\n",
    "\n",
    "        # If instances are available, get the locations of the spark peaks\n",
    "        if len(self.instances) > 0:\n",
    "            self.spark_peaks = self._detect_spark_peaks()\n",
    "        else:\n",
    "            self.spark_peaks = []\n",
    "\n",
    "        # Preprocess videos if necessary\n",
    "        self._preprocess_videos()\n",
    "\n",
    "        # Store original duration of all movies before padding\n",
    "        self.original_durations = [movie.shape[0] for movie in self.movies]\n",
    "\n",
    "        # Adjust videos shape so that it is suitable for the model\n",
    "        self._adjust_videos_shape()\n",
    "\n",
    "    ############################## Class methods ###############################\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        total_samples = 0\n",
    "        for movie in self.movies:\n",
    "            frames = movie.shape[0]\n",
    "            # Calculate the number of samples for each movie\n",
    "            samples_per_movie = (frames - self.window_size) // self.stride + 1\n",
    "            total_samples += samples_per_movie\n",
    "        return total_samples\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        if idx < 0:\n",
    "            idx = self.__len__() + idx\n",
    "\n",
    "        sample_dict = {}\n",
    "\n",
    "        # Get the movie index and chunk index for the given idx\n",
    "        movie_idx, chunk_idx = self._get_movie_and_chunk_indices(idx)\n",
    "        sample_dict[\"movie_id\"] = movie_idx\n",
    "\n",
    "        # Store the original duration of the movie\n",
    "        sample_dict[\"original_duration\"] = self.original_durations[movie_idx]\n",
    "\n",
    "        # Calculate the starting frame within the movie\n",
    "        start_frame = chunk_idx * self.stride\n",
    "        end_frame = start_frame + self.window_size\n",
    "\n",
    "        # Extract the windowed data and labels\n",
    "        sample_dict[\"data\"] = self.movies[movie_idx][start_frame:end_frame]\n",
    "\n",
    "        if self.gt_available:\n",
    "            sample_dict[\"labels\"] = self.labels[movie_idx][start_frame:end_frame]\n",
    "\n",
    "        # Add the sample ID (string) to the item dictionary, if available\n",
    "        if self.sample_ids:\n",
    "            sample_dict[\"sample_id\"] = self.sample_ids[movie_idx]\n",
    "\n",
    "        return sample_dict\n",
    "\n",
    "    ############################## Private methods #############################\n",
    "\n",
    "    def _get_movies(self) -> List[np.ndarray]:\n",
    "        # Load movie data for each sample ID\n",
    "        movies = load_movies_ids(\n",
    "            data_folder=self.base_path,\n",
    "            ids=self.sample_ids,\n",
    "            names_available=True,\n",
    "            movie_names=\"video\",\n",
    "        )\n",
    "\n",
    "        # Extract and return the movie values as a list\n",
    "        movies = list(movies.values())\n",
    "\n",
    "        return movies\n",
    "\n",
    "    def _get_labels(self) -> List[np.ndarray]:\n",
    "        # preprocess annotations if necessary\n",
    "        if self.params.sparks_type == \"raw\":\n",
    "            mask_names = \"class_label\"\n",
    "        elif self.params.sparks_type == \"peaks\":\n",
    "            mask_names = \"class_label_peaks\"\n",
    "        elif self.params.sparks_type == \"small\":\n",
    "            mask_names = \"class_label_small_peaks\"\n",
    "        elif self.params.sparks_type == \"dilated\":\n",
    "            mask_names = \"class_label_dilated\"\n",
    "        else:\n",
    "            raise NotImplementedError(\"Annotation type not supported yet.\")\n",
    "\n",
    "        labels = load_annotations_ids(\n",
    "            data_folder=self.base_path, ids=self.sample_ids, mask_names=mask_names\n",
    "        )\n",
    "\n",
    "        if labels:\n",
    "            # Extract and return the mask values as a list\n",
    "            labels = list(labels.values())\n",
    "        else:\n",
    "            labels = []\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def _get_instances(self) -> List[np.ndarray]:\n",
    "        # Load single event instances for each sample ID\n",
    "        instances = load_annotations_ids(\n",
    "            data_folder=self.base_path,\n",
    "            ids=self.sample_ids,\n",
    "            mask_names=\"event_label\",\n",
    "        )\n",
    "\n",
    "        if instances:\n",
    "            # Extract and return the mask values as a list\n",
    "            instances = list(instances.values())\n",
    "        else:\n",
    "            raise ValueError(\"Instances not available for this dataset.\")\n",
    "\n",
    "        return instances\n",
    "\n",
    "    def _get_movie_and_chunk_indices(self, idx: int) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Given an index, returns the movie index and chunk index for the\n",
    "        corresponding chunk in the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the chunk in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the movie index and chunk index for the\n",
    "            corresponding chunk.\n",
    "        \"\"\"\n",
    "        current_idx = 0  # Number of samples seen so far\n",
    "        for movie_idx, movie in enumerate(self.movies):\n",
    "            frames, _, _ = movie.shape\n",
    "            samples_per_movie = (frames - self.window_size) // self.stride + 1\n",
    "            if idx < current_idx + samples_per_movie:\n",
    "                # If idx is smaller than the number of samples seen so\n",
    "                # far plus the number of samples in the current movie,\n",
    "                # then the sample we're looking for is in the current\n",
    "                # movie.\n",
    "                chunk_idx = idx - current_idx  # chunk idx in the movie\n",
    "                return movie_idx, chunk_idx\n",
    "\n",
    "            current_idx += samples_per_movie\n",
    "\n",
    "        # If the index is out of range, raise an error\n",
    "        raise IndexError(\n",
    "            f\"Index {idx} is out of range for dataset of length {len(self)}\"\n",
    "        )\n",
    "\n",
    "    def _detect_spark_peaks(self, class_name: str = \"sparks\") -> List[np.ndarray]:\n",
    "        # Detect the spark peaks in the instance mask of each movie\n",
    "        # Remark: can be used for other classes as well\n",
    "        spark_peaks = []\n",
    "        for movie, labels, instances in zip(self.movies, self.labels, self.instances):\n",
    "            spark_mask = np.where(\n",
    "                labels == config.classes_dict[class_name], instances, 0\n",
    "            )\n",
    "            self.coords_true = detect_spark_peaks(\n",
    "                movie=movie,\n",
    "                instances_mask=spark_mask,\n",
    "                sigma=config.sparks_sigma_dataset,\n",
    "                max_filter_size=10,\n",
    "            )\n",
    "            spark_peaks.append(self.coords_true)\n",
    "        return spark_peaks\n",
    "\n",
    "    def _preprocess_videos(self) -> None:\n",
    "        \"\"\"\n",
    "        Preprocesses the videos in the dataset.\n",
    "        \"\"\"\n",
    "        if self.params.remove_background == \"average\":\n",
    "            self.movies = [self._remove_avg_background(\n",
    "                movie) for movie in self.movies]\n",
    "\n",
    "        if self.params.data_smoothing in [\"2d\", \"3d\"]:\n",
    "            n_dims = int(self.params.data_smoothing[0])\n",
    "            self.movies = [\n",
    "                self._blur_movie(movie, n_dims=n_dims) for movie in self.movies\n",
    "            ]\n",
    "\n",
    "        if self.params.norm_video in [\"movie\", \"abs_max\", \"std_dev\"]:\n",
    "            self.movies = [\n",
    "                self._normalize(movie, norm_type=self.params.norm_video)\n",
    "                for movie in self.movies\n",
    "            ]\n",
    "\n",
    "    def _remove_avg_background(self, movie: torch.Tensor) -> torch.Tensor:\n",
    "        # Remove the average background from the video frames.\n",
    "        avg = torch.mean(movie, dim=0)\n",
    "        return movie - avg\n",
    "\n",
    "    def _blur_movie(self, movie: torch.Tensor, n_dims: int) -> torch.Tensor:\n",
    "        # Define the kernel size and sigma based on the number of dimensions\n",
    "        kernel_size = (3,) * n_dims\n",
    "        sigma = 1.0\n",
    "\n",
    "        # Apply gaussian blur to the video\n",
    "        gaussian_blur = GaussianBlur(kernel_size=kernel_size, sigma=sigma)\n",
    "        return gaussian_blur(movie)\n",
    "\n",
    "    def _normalize(self, movie: torch.Tensor, norm_type: str) -> torch.Tensor:\n",
    "        # Normalize the video frames.\n",
    "        if norm_type == \"movie\":\n",
    "            # Normalize each movie separately using its own max and min\n",
    "            movie = (movie - torch.min(movie)) / \\\n",
    "                (torch.max(movie) - torch.min(movie))\n",
    "        elif norm_type == \"abs_max\":\n",
    "            # Normalize each movie separately using the absolute max of uint16\n",
    "            absolute_max = np.iinfo(np.uint16).max  # 65535\n",
    "            movie = (movie - torch.min(movie)) / \\\n",
    "                (absolute_max - torch.min(movie))\n",
    "        elif norm_type == \"std_dev\":\n",
    "            # Normalize each movie separately using its own standard deviation\n",
    "            movie = (movie - torch.mean(movie)) / torch.std(movie)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid norm type: {norm_type}\")\n",
    "        return movie\n",
    "\n",
    "    def _adjust_videos_shape(self) -> None:\n",
    "        # Pad videos shorter than chunk duration with zeros on both sides\n",
    "        self.movies = [self._pad_short_video(video) for video in self.movies]\n",
    "        if self.gt_available:\n",
    "            self.labels = [\n",
    "                self._pad_short_video(mask, padding_value=config.ignore_index)\n",
    "                for mask in self.labels\n",
    "            ]\n",
    "\n",
    "        # Pad videos whose length does not match with chunks_duration and\n",
    "        # stride params\n",
    "        self.movies = [self._pad_extremities_of_video(\n",
    "            video) for video in self.movies]\n",
    "        if self.gt_available:\n",
    "            self.labels = [\n",
    "                self._pad_extremities_of_video(\n",
    "                    mask, padding_value=config.ignore_index)\n",
    "                for mask in self.labels\n",
    "            ]\n",
    "\n",
    "    def _pad_short_video(\n",
    "        self, video: torch.Tensor, padding_value: int = 0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pads a video tensor with zeros on both sides if its length is shorter\n",
    "        than the specified chunk duration.\n",
    "\n",
    "        Args:\n",
    "            video (torch.Tensor): The video tensor to pad.\n",
    "            padding_value (int, optional): The value to use for padding.\n",
    "                Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The padded video tensor.\n",
    "        \"\"\"\n",
    "        padding_length = self.params.data_duration - video.shape[0]\n",
    "        if padding_length > 0:\n",
    "            video = F.pad(\n",
    "                video,\n",
    "                (\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    padding_length // 2,\n",
    "                    padding_length // 2 + padding_length % 2,\n",
    "                ),\n",
    "                \"constant\",\n",
    "                value=padding_value,\n",
    "            )\n",
    "            assert video.shape[0] == self.params.data_duration, \"Padding is wrong\"\n",
    "            # logger.debug(\"Added padding to short video\")\n",
    "        return video\n",
    "\n",
    "    def _pad_extremities_of_video(\n",
    "        self, video: torch.Tensor, padding_value: int = 0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pads videos whose length does not match with chunks_duration and step\n",
    "        params.\n",
    "\n",
    "        Args:\n",
    "        - video (torch.Tensor): The video to pad.\n",
    "        - padding_value (int): The value to use for padding. Default is 0.\n",
    "\n",
    "        Returns:\n",
    "        - The padded video.\n",
    "        \"\"\"\n",
    "\n",
    "        length = video.shape[0]\n",
    "        padding_length = self.params.data_stride * math.ceil(\n",
    "            (length - self.params.data_duration) / self.params.data_stride\n",
    "        ) - (length - self.params.data_duration)\n",
    "        if padding_length > 0:\n",
    "            video = F.pad(\n",
    "                video,\n",
    "                (\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    padding_length // 2,\n",
    "                    padding_length // 2 + padding_length % 2,\n",
    "                ),\n",
    "                \"constant\",\n",
    "                value=padding_value,\n",
    "            )\n",
    "            length = video.shape[0]\n",
    "            # if not mask:\n",
    "            #     logger.debug(\n",
    "            #         f\"Added padding of {padding_length} frames to video with unsuitable duration\"\n",
    "            #     )\n",
    "\n",
    "        assert (\n",
    "            (length - self.params.data_duration) / self.params.data_stride\n",
    "        ) % 1 == 0, \"padding at end of video is wrong\"\n",
    "\n",
    "        return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Any, Dict\n",
    "from config import TrainingConfig\n",
    "\n",
    "\n",
    "class SparkDatasetTemporalReduction(SparkDataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for spark detection with temporal reduction.\n",
    "\n",
    "    This class is a subclass of the `SparkDataset` class and is specifically\n",
    "    designed to work with deep learning models that use temporal reduction.\n",
    "    It shrinks the annotation masks and instances to match the reduced temporal\n",
    "    resolution of the model.\n",
    "\n",
    "    Args:\n",
    "        same as SparkDataset\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If neither `movies` nor `base_path` and `sample_ids` are\n",
    "        provided.\n",
    "        AssertionError: If temporal reduction is not enabled in the parameters.\n",
    "\n",
    "    Attributes:\n",
    "        same as SparkDataset\n",
    "\n",
    "    Methods:\n",
    "        same as SparkDataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: TrainingConfig, **kwargs: Any) -> None:\n",
    "        # check that the temporal reduction is enabled in the parameters\n",
    "        assert (\n",
    "            params.temporal_reduction\n",
    "        ), \"Temporal reduction is not enabled in the parameters.\"\n",
    "\n",
    "        # call the parent constructor\n",
    "        super().__init__(params, **kwargs)\n",
    "\n",
    "        # shrink the labels\n",
    "        self.labels = [self._shrink_mask(mask) for mask in self.labels]\n",
    "\n",
    "        # shrink the instances (not implemented yet!)\n",
    "        if self.instances:\n",
    "            # raise and error if instances are available\n",
    "            raise NotImplementedError(\n",
    "                \"Instances are not supported for temporal reduction yet.\"\n",
    "            )\n",
    "\n",
    "    ############################## Private methods #############################\n",
    "\n",
    "    def _shrink_mask(self, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Shrink an annotation mask based on the number of channels.\n",
    "\n",
    "        Args:\n",
    "            mask (numpy.ndarray): Input annotation mask.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Shrinked annotation mask.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            mask.shape[0] % self.params.num_channels == 0\n",
    "        ), \"Duration of the mask is not a multiple of num_channels.\"\n",
    "\n",
    "        # Get tensor of duration 'self.num_channels'\n",
    "        sub_masks = np.split(mask, mask.shape[0] // self.params.num_channels)\n",
    "        new_mask = []\n",
    "\n",
    "        # For each subtensor get a single frame\n",
    "        for sub_mask in sub_masks:\n",
    "            new_frame = np.array(\n",
    "                [\n",
    "                    [\n",
    "                        self._get_new_voxel_label(sub_mask[:, y, x])\n",
    "                        for x in range(sub_mask.shape[2])\n",
    "                    ]\n",
    "                    for y in range(sub_mask.shape[1])\n",
    "                ]\n",
    "            )\n",
    "            new_mask.append(new_frame)\n",
    "\n",
    "        new_mask = np.stack(new_mask)\n",
    "        return new_mask\n",
    "\n",
    "    def _get_new_voxel_label(self, voxel_seq: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Get the new voxel label based on the sequence of voxel values.\n",
    "\n",
    "        Args:\n",
    "            voxel_seq (numpy.ndarray): Sequence of voxel values\n",
    "                (num_channels elements).\n",
    "\n",
    "        Returns:\n",
    "            int: New voxel label.\n",
    "        \"\"\"\n",
    "        # voxel_seq is a vector of 'num_channels' elements\n",
    "        # {0} -> 0\n",
    "        # {0, i}, {i} -> i, i = 1,2,3\n",
    "        # {0, 1, i}, {1, i} -> 1, i = 2,3\n",
    "        # {0, 2 ,3}, {2, 3} -> 3\n",
    "        # print(voxel_seq)\n",
    "\n",
    "        if np.max(voxel_seq == 0):\n",
    "            return 0\n",
    "        elif 1 in voxel_seq:\n",
    "            return 1\n",
    "        elif 3 in voxel_seq:\n",
    "            return 3\n",
    "        else:\n",
    "            return np.max(voxel_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkDatasetResampled(SparkDataset):\n",
    "    \"\"\"\n",
    "    Dataset class for resampled SR-calcium releases segmented dataset.\n",
    "\n",
    "    This class extends the `SparkDataset` class and resamples the movies to a\n",
    "    given frame rate. The original frame rate of the movies is obtained from\n",
    "    their metadata. The resampled movies, labels, and instances are stored in\n",
    "    memory.\n",
    "\n",
    "    Args:\n",
    "    - params (TrainingConfig): The training configuration.\n",
    "    - movie_paths (List[str]): A list of paths to the movies (same order as the\n",
    "        movies in the dataset). This allows to obtain the original frame rate of\n",
    "        the movies from their metadata.\n",
    "    - new_fps (int): The frame rate to resample the movies to.\n",
    "    ... (same as SparkDataset)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `movie_paths` or `new_fps` are not provided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: TrainingConfig,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        # Verify that movie_paths and new_fps are provided\n",
    "        if \"movie_paths\" not in kwargs:\n",
    "            raise ValueError(\"movie_paths must be provided\")\n",
    "        if \"new_fps\" not in kwargs:\n",
    "            raise ValueError(\"new_fps must be provided\")\n",
    "\n",
    "        self.movie_paths: List[str] = kwargs[\"movie_paths\"]\n",
    "        self.new_fps: int = kwargs[\"new_fps\"]\n",
    "\n",
    "        # Initialize the SparksDataset class\n",
    "        super().__init__(params=params, **kwargs)\n",
    "\n",
    "    ############################## Class methods ###############################\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, int, float]]:\n",
    "        # Get item from the SparksDataset class and add the original frame rate\n",
    "        item_dict = super().__getitem__(idx)\n",
    "        item_dict[\"original_fps\"] = self.original_fps[int(item_dict[\"movie_id\"])]\n",
    "\n",
    "        return item_dict\n",
    "\n",
    "    ############################## Private methods #############################\n",
    "\n",
    "    def _preprocess_videos(self) -> None:\n",
    "        \"\"\"\n",
    "        Preprocesses the videos in the dataset.\n",
    "        \"\"\"\n",
    "        # apply the same preprocessing as in the SparksDataset class\n",
    "        super()._preprocess_videos()\n",
    "\n",
    "        # Get the original frame rate of the movies\n",
    "        self.original_fps = [\n",
    "            self._get_fps(movie_path) for movie_path in self.movie_paths\n",
    "        ]\n",
    "\n",
    "        # Resample the movies to the desired frame rate\n",
    "        self.movies = [\n",
    "            self._resample_video(movie, movie_path)\n",
    "            for movie, movie_path in zip(self.movies, self.movie_paths)\n",
    "        ]\n",
    "\n",
    "        # Resample the labels to the desired frame rate\n",
    "        if self.labels:\n",
    "            self.labels = [\n",
    "                self._resample_video(mask, movie_path)\n",
    "                for mask, movie_path in zip(self.labels, self.movie_paths)\n",
    "            ]\n",
    "\n",
    "        # Resample the instances to the desired frame rate\n",
    "        if self.instances:\n",
    "            self.instances = [\n",
    "                self._resample_video(instance, movie_path)\n",
    "                for instance, movie_path in zip(self.instances, self.movie_paths)\n",
    "            ]\n",
    "\n",
    "    ####################### Methods for video resampling #######################\n",
    "\n",
    "    def _resample_video(self, movie: torch.Tensor, movie_path: str) -> torch.Tensor:\n",
    "        # Resample the video to the desired frame rate\n",
    "        return self._video_spline_interpolation(\n",
    "            movie=movie, movie_path=movie_path, new_fps=self.new_fps\n",
    "        )\n",
    "\n",
    "    def _get_fps(self, movie_path: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute estimated video FPS value with respect to sampling time deltas.\n",
    "\n",
    "        Args:\n",
    "            movie_path (str): Path to the video.\n",
    "\n",
    "        Returns:\n",
    "            float: Estimated FPS value.\n",
    "        \"\"\"\n",
    "        times = self._get_times(movie_path)\n",
    "        deltas = np.diff(times)\n",
    "        return float(1.0 / np.mean(deltas))\n",
    "\n",
    "    def _get_times(self, movie_path: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get times at which video frames were sampled.\n",
    "\n",
    "        Args:\n",
    "            movie_path (str): Path to the video.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Array of frame times.\n",
    "        \"\"\"\n",
    "        with Image.open(movie_path) as img:\n",
    "            exif_data = img.getexif()\n",
    "        description = exif_data[270][0].split(\"\\r\\n\")\n",
    "        description = [line.split(\"\\t\") for line in description]\n",
    "        description = [\n",
    "            [int(i) if i.isdigit() else i for i in line] for line in description\n",
    "        ]\n",
    "        description = [d for d in description if isinstance(d[0], int)]\n",
    "        return np.array([float(line[1]) for line in description])\n",
    "\n",
    "    def _video_spline_interpolation(\n",
    "        self, movie: torch.Tensor, movie_path: str, new_fps: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Interpolate video frames based on new sampling times (FPS).\n",
    "\n",
    "        Args:\n",
    "            movie (numpy.ndarray): Input video frames.\n",
    "            movie_path (str): Path to the video.\n",
    "            new_fps (int): Desired FPS for the output video.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Interpolated video frames.\n",
    "        \"\"\"\n",
    "        frames_time = self._get_times(movie_path)\n",
    "        f = interp1d(frames_time, movie, kind=\"linear\", axis=0)\n",
    "        assert len(frames_time) == movie.shape[0], (\n",
    "            \"In video_spline_interpolation the duration of the video \"\n",
    "            \"is not equal to the number of frames\"\n",
    "        )\n",
    "        frames_new = np.linspace(\n",
    "            frames_time[0], frames_time[-1], int(frames_time[-1] * new_fps)\n",
    "        )\n",
    "\n",
    "        return f(frames_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union, List\n",
    "import torch\n",
    "\n",
    "\n",
    "class SparkDatasetLSTM(SparkDataset):\n",
    "    \"\"\"\n",
    "    SparkDataset class for UNet-convLSTM model.\n",
    "\n",
    "    The dataset is adapted in such a way that each chunk is a sequence of\n",
    "    frames centered around the frame to be predicted.\n",
    "    The label is the segmentation mask of the central frame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: TrainingConfig, **kwargs: Dict[str, Any]) -> None:\n",
    "        # step = 1 and ignore_frames = 0 because we need to have a prediction\n",
    "        # for each frame.\n",
    "        self.params.data_stride = 1\n",
    "        self.params.ignore_frames_loss = 0\n",
    "        super().__init__(params, **kwargs)\n",
    "\n",
    "    ############################## Class methods ###############################\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, int, float]]:\n",
    "        \"\"\"\n",
    "        As opposed to the SparkDataset class, here the label is just the\n",
    "        middle frame of the chunk.\n",
    "        \"\"\"\n",
    "        sample_dict = super().__getitem__(idx)\n",
    "\n",
    "        if self.gt_available:\n",
    "            # extract middle frame from label\n",
    "            sample_dict[\"labels\"] = sample_dict[\"labels\"][\n",
    "                self.params.data_duration // 2\n",
    "            ]\n",
    "\n",
    "        return sample_dict\n",
    "\n",
    "    ############################## Private methods #############################\n",
    "\n",
    "    def _pad_short_video(\n",
    "        self, video: torch.Tensor, padding_value: int = 0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Instead of padding the video with zeros, pad it with the first\n",
    "        and last frame of the video.\n",
    "        \"\"\"\n",
    "        padding_length = self.params.data_duration - video.shape[0]\n",
    "        if padding_length:\n",
    "            video = F.pad(\n",
    "                video,\n",
    "                (\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    padding_length // 2,\n",
    "                    padding_length // 2 + padding_length % 2,\n",
    "                ),\n",
    "                \"replicate\",\n",
    "            )\n",
    "\n",
    "            assert video.shape[0] == self.params.data_duration, \"Padding is wrong\"\n",
    "\n",
    "            # logger.debug(\"Added padding to short video\")\n",
    "\n",
    "        return video\n",
    "\n",
    "    def _pad_extremities_of_video(\n",
    "        self, video: torch.Tensor, padding_value: int = 0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pad duration/2 frames at the beginning and at the end of the video\n",
    "        with the first and last frame of the video.\n",
    "        \"\"\"\n",
    "        length = video.shape[0]\n",
    "\n",
    "        # check that duration is odd\n",
    "        assert self.params.data_duration % 2 == 1, \"duration must be odd\"\n",
    "\n",
    "        pad = self.params.data_duration - 1\n",
    "\n",
    "        # if video is int32, cast it to float32\n",
    "        cast_to_float = video.dtype == torch.int32\n",
    "        if cast_to_float:\n",
    "            video = video.float()  # cast annotations to float32\n",
    "\n",
    "        replicate = nn.ReplicationPad3d((0, 0, 0, 0, pad // 2, pad // 2))\n",
    "        video = replicate(video[None, :])[0]\n",
    "\n",
    "        if cast_to_float:\n",
    "            video = video.int()  # cast annotations back to int32\n",
    "\n",
    "        # check that duration of video is original duration + chunk duration - 1\n",
    "        assert (\n",
    "            video.shape[0] == length + self.params.data_duration - 1\n",
    "        ), \"padding at end of video is wrong\"\n",
    "\n",
    "        return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dotti\\sparks_project\\TEST new_dataset.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dotti/sparks_project/TEST%20new_dataset.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSparkDatasetInference\u001b[39;00m(SparkDataset):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dotti/sparks_project/TEST%20new_dataset.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dotti/sparks_project/TEST%20new_dataset.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    Create a dataset that contains only a single movie for inference.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dotti/sparks_project/TEST%20new_dataset.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m    It requires either a single movie or a movie path to be provided.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dotti/sparks_project/TEST%20new_dataset.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dotti/sparks_project/TEST%20new_dataset.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, params: TrainingConfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dotti/sparks_project/TEST%20new_dataset.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m# Check that the arguments are suitable\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkDataset' is not defined"
     ]
    }
   ],
   "source": [
    "class SparkDatasetInference(SparkDataset):\n",
    "    \"\"\"\n",
    "    Create a dataset that contains only a single movie for inference.\n",
    "    It requires either a single movie or a movie path to be provided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: TrainingConfig, **kwargs) -> None:\n",
    "        # Check that the arguments are suitable\n",
    "        if \"movie\" not in kwargs and \"movie_path\" not in kwargs:\n",
    "            raise ValueError(\"Either movie or movie_path must be provided.\")\n",
    "\n",
    "        # TODO: continuare da qui!\n",
    "        # Remarks: il dataset non avrà labels e instances, e alla parent class\n",
    "        # dovrà passare il movie come una lista di un solo elemento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea for new inference approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=params.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Run movie in the network and perform inference\n",
    "    preds, loss = do_inference(\n",
    "        network=model,\n",
    "        dataloader=dataloader,\n",
    "        device=params.device,\n",
    "        detect_nan=detect_nan,\n",
    "        compute_loss=True if criterion is not None else False,\n",
    "        inference_types=inference_types,\n",
    "    )\n",
    "\n",
    "    return preds, loss\n",
    "\n",
    "\n",
    "def do_inference(network, dataloader, device, detect_nan=False, compute_loss=False, inference_types=None):\n",
    "    \"\"\"\n",
    "    Given a trained network and a dataloader, run the data through the network\n",
    "    and perform inference.\n",
    "\n",
    "    Args:\n",
    "        network (torch.nn.Module): The trained neural network.\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader containing the\n",
    "            data to run through the network.\n",
    "        device (torch.device): The device to run the network on.\n",
    "        detect_nan (bool, optional): Whether to detect NaN values in input and\n",
    "            annotation tensors.\n",
    "        compute_loss (bool, optional): Whether to compute the loss.\n",
    "        inference_types (list of str, optional): List of inference types to use,\n",
    "            or None to use the default type.\n",
    "\n",
    "    Returns:\n",
    "        preds (numpy.ndarray or dict): The predictions, either as a numpy array of\n",
    "            shape (4 x movie duration x 64 x 512) if `inference_types` is None, or\n",
    "            as a dictionary with inference type as the key and predictions as the\n",
    "            value.\n",
    "        loss (float or None): The loss value if `compute_loss` is True, otherwise\n",
    "            None.\n",
    "    \"\"\"\n",
    "    if inference_types is None:\n",
    "        inference_types = [\"overlap\", \"average\", \"gaussian\", \"max\"]\n",
    "\n",
    "    # Move network to device\n",
    "    network.to(device)\n",
    "\n",
    "    # Set network to evaluation mode\n",
    "    network.eval()\n",
    "\n",
    "    # Initialize variables\n",
    "    preds = {}\n",
    "    loss = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            batch = [item.to(device) for item in batch]\n",
    "\n",
    "            # Run batch through network\n",
    "            output = network(*batch)\n",
    "\n",
    "            # Compute loss if requested\n",
    "            if compute_loss:\n",
    "                batch_loss = criterion(output, batch[-1])\n",
    "                if loss is None:\n",
    "                    loss = batch_loss.item()\n",
    "                else:\n",
    "                    loss += batch_loss.item()\n",
    "\n",
    "            # Perform inference\n",
    "            for inference_type in inference_types:\n",
    "                if inference_type == \"overlap\":\n",
    "                    pred = overlap_inference(output)\n",
    "                elif inference_type == \"average\":\n",
    "                    pred = average_inference(output)\n",
    "                elif inference_type == \"gaussian\":\n",
    "                    pred = gaussian_inference(output)\n",
    "                elif inference_type == \"max\":\n",
    "                    pred = max_inference(output)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported inference type: {inference_type}\")\n",
    "\n",
    "                if inference_type not in preds:\n",
    "                    preds[inference_type] = pred\n",
    "                else:\n",
    "                    preds[inference_type] = np.concatenate((preds[inference_type], pred), axis=0)\n",
    "\n",
    "            # Detect NaN values if requested\n",
    "            if detect_nan:\n",
    "                if torch.isnan(output).any():\n",
    "                    raise ValueError(\"NaN values detected in output\")\n",
    "\n",
    "    # Compute average loss if requested\n",
    "    if compute_loss:\n",
    "        loss /= len(dataloader)\n",
    "\n",
    "    # Return predictions and loss\n",
    "    if len(inference_types) == 1:\n",
    "        preds = preds[inference_types[0]]\n",
    "    return preds, loss\n",
    "\n",
    "\n",
    "def overlap_inference(output):\n",
    "    # TODO: Implement overlap inference\n",
    "    pass\n",
    "\n",
    "\n",
    "def average_inference(output):\n",
    "    # TODO: Implement average inference\n",
    "    pass\n",
    "\n",
    "\n",
    "def gaussian_inference(output):\n",
    "    # TODO: Implement Gaussian inference\n",
    "    pass\n",
    "\n",
    "\n",
    "def max_inference(output):\n",
    "    # TODO: Implement max inference\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
